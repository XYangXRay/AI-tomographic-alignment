{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1315c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential packages\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import tomography and imaging packages\n",
    "import tomopy\n",
    "from skimage.transform import rotate, AffineTransform\n",
    "from skimage import transform as tf\n",
    "from scipy.fft import fft2, fftshift\n",
    "from scipy.signal import correlate\n",
    "\n",
    "# Import neural net packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.profiler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff43d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Environment: pytorch\n",
      "Cuda Version: 11.8\n",
      "Cuda Availability: True\n",
      "/home/liam/Projects/Tomographic Alignment\r\n"
     ]
    }
   ],
   "source": [
    "# Checking to ensure environment and cuda are correct\n",
    "print(\"Working Environment: {}\".format(os.environ['CONDA_DEFAULT_ENV']))\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"Cuda Version: {}\".format(torch.version.cuda))\n",
    "print(\"Cuda Availability: {}\".format(torch.cuda.is_available()))\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a1f0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr_reprojection(data, entries):\n",
    "    \n",
    "    ang = tomopy.angles(data[0][0][0, 0].shape[0])\n",
    "    _rec = 1e-12 * np.ones((data[0][0][0, 0].shape[1], data[0][0][0, 0].shape[2], data[0][0][0, 0].shape[2]))\n",
    "    data_copy = data.copy()\n",
    "    out_data = np.zeros((entries, 2), dtype = object)\n",
    "    \n",
    "    for i in range (entries):\n",
    "    \n",
    "        out_data[i, 0] = np.zeros((1, 1, data[0][0][0, 0].shape[0], data[0][0][0, 0].shape[1] * 2 - 1,\n",
    "                                      data[0][0][0, 0].shape[2] * 2 - 1))\n",
    "        out_data[i, 1] = data_copy[i, 1]\n",
    "        \n",
    "        rec = tomopy.recon(data_copy[i][0][0, 0], ang, center = None, \n",
    "                            algorithm = 'mlem', init_recon = _rec)\n",
    "        reproj = tomopy.project(rec, ang, center = None, pad = False)\n",
    "        \n",
    "        for j in range (data[0][0][0, 0].shape[0]):\n",
    "            \n",
    "            out_data[i, 0][0, 0, j] = correlate(data_copy[i][0][0, 0, j], reproj[j], method = 'fft')\n",
    "        \n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3fc098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_projections(data, entries):\n",
    "    \n",
    "    projections = np.zeros((entries * data[0][0][0, 0].shape[0], 2), dtype = object)\n",
    "\n",
    "    for i in range (entries):\n",
    "\n",
    "        for j in range (data[0][0][0, 0].shape[0]):\n",
    "\n",
    "            projections[i * data[0][0][0, 0].shape[0] + j, 0] = data[i, 0][0, 0, j, :, :]\n",
    "            projections[i * data[0][0][0, 0].shape[0] + j, 1] = np.asarray([data[i, 1][0, 2 * j], \n",
    "                                                                            data[i, 1][0, 2 * j + 1]])\n",
    "            \n",
    "    return projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "987c91f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Projections: 45000\n"
     ]
    }
   ],
   "source": [
    "# Loading data, 25 entries of 128 resolution shepp3ds\n",
    "res = 128\n",
    "entries = 250\n",
    "data = []\n",
    "\n",
    "for i in range(entries):\n",
    "    data.append(np.load('./shepp{}-{}/shepp{}-{}_{}.npy'.format(res, entries, res, entries, i), \n",
    "                        allow_pickle = True))\n",
    "    \n",
    "data = np.asarray(data)\n",
    "angles = entries * data[0][0][0, 0].shape[0]\n",
    "print(\"Total Projections: {}\".format(angles))\n",
    "crosscorr_reproj = crosscorr_reprojection(data, entries)\n",
    "crosscorr_data = to_projections(crosscorr_reproj, entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bacc2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 2)\n",
      "(1, 1, 255, 367)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "for i in range (crosscorr_data.shape[0]):\n",
    "    \n",
    "    crosscorr_data[i, 0] = np.expand_dims(crosscorr_data[i, 0], axis = 0)\n",
    "    crosscorr_data[i, 0] = np.expand_dims(crosscorr_data[i, 0], axis = 0)\n",
    "    crosscorr_data[i, 1] = np.expand_dims(crosscorr_data[i, 1], axis = 0)\n",
    "    \n",
    "print(crosscorr_data.shape)\n",
    "print(crosscorr_data[0, 0].shape)\n",
    "print(crosscorr_data[0, 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e07c4e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Dataset: (36000, 2)\n",
      "Shape of Testing Dataset: (9000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of training and testing splits\n",
    "trainset, testset = np.split(crosscorr_data, [int(angles * 4 / 5)])\n",
    "print(\"Shape of Training Dataset: {}\".format(trainset.shape))\n",
    "print(\"Shape of Testing Dataset: {}\".format(testset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd2da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def norm(proj):\n",
    "    proj = (proj - torch.min(proj)) / (torch.max(proj) - torch.min(proj))\n",
    "    return proj\n",
    "\n",
    "# Get inplanes for resnet\n",
    "def get_inplanes():\n",
    "    return [64, 128, 256, 512]\n",
    "\n",
    "\n",
    "# Preset for a 3x3 kernel convolution\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "# Preset for a 1x1 kernel convolution\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=1,\n",
    "                     stride=stride,\n",
    "                     bias=False)\n",
    "\n",
    "# Basic block for resnet\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# Bottleneck block for resnet\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv1x1(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Resnet structure\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 block_inplanes,\n",
    "                 n_input_channels=1,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 shortcut_type='B',\n",
    "                 widen_factor=1.0,\n",
    "                 n_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
    "\n",
    "        self.in_planes = block_inplanes[0]\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_input_channels,\n",
    "                               self.in_planes,\n",
    "                               kernel_size=(conv1_t_size, 7),\n",
    "                               stride=(conv1_t_stride, 2),\n",
    "                               padding=(conv1_t_size // 2, 3),\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n",
    "                                       shortcut_type)\n",
    "        self.layer2 = self._make_layer(block,\n",
    "                                       block_inplanes[1],\n",
    "                                       layers[1],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer3 = self._make_layer(block,\n",
    "                                       block_inplanes[2],\n",
    "                                       layers[2],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer4 = self._make_layer(block,\n",
    "                                       block_inplanes[3],\n",
    "                                       layers[3],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _downsample_basic_block(self, x, planes, stride):\n",
    "        out = F.avg_pool2d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n",
    "                                out.size(3), out.size(4))\n",
    "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "            zero_pads = zero_pads.cuda()\n",
    "\n",
    "        out = torch.cat([out.data, zero_pads], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # make layer helper function\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            \n",
    "                downsample = nn.Sequential(\n",
    "                    conv1x1(self.in_planes, planes * block.expansion, stride),\n",
    "                    nn.BatchNorm2d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(in_planes=self.in_planes,\n",
    "                  planes=planes,\n",
    "                  stride=stride,\n",
    "                  downsample=downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = norm(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Generates form of resnet\n",
    "def generate_model(model_depth, **kwargs):\n",
    "    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
    "\n",
    "    if model_depth == 10:\n",
    "        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 18:\n",
    "        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 34:\n",
    "        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 50:\n",
    "        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 101:\n",
    "        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 152:\n",
    "        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 200:\n",
    "        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e62be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 2]                    --\n",
       "├─Conv2d: 1-1                            [1, 64, 255, 184]         3,136\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 255, 184]         128\n",
       "├─ReLU: 1-3                              [1, 64, 255, 184]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 128, 92]          --\n",
       "├─Sequential: 1-5                        [1, 64, 128, 92]          --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 128, 92]          36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 128, 92]          36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 128, 92]          --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 128, 92]          36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 128, 92]          36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 128, 92]          --\n",
       "├─Sequential: 1-6                        [1, 128, 64, 46]          --\n",
       "│    └─BasicBlock: 2-3                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 64, 46]          73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 64, 46]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 64, 46]          256\n",
       "│    │    └─Sequential: 3-18             [1, 128, 64, 46]          8,448\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 64, 46]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-20                 [1, 128, 64, 46]          147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-22                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 64, 46]          147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 64, 46]          --\n",
       "├─Sequential: 1-7                        [1, 256, 32, 23]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 32, 23]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 32, 23]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 256, 32, 23]          512\n",
       "│    │    └─Sequential: 3-31             [1, 256, 32, 23]          33,280\n",
       "│    │    └─ReLU: 3-32                   [1, 256, 32, 23]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-33                 [1, 256, 32, 23]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-35                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 32, 23]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-38                   [1, 256, 32, 23]          --\n",
       "├─Sequential: 1-8                        [1, 512, 16, 12]          --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-39                 [1, 512, 16, 12]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 512, 16, 12]          1,024\n",
       "│    │    └─ReLU: 3-41                   [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-42                 [1, 512, 16, 12]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 512, 16, 12]          1,024\n",
       "│    │    └─Sequential: 3-44             [1, 512, 16, 12]          132,096\n",
       "│    │    └─ReLU: 3-45                   [1, 512, 16, 12]          --\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-46                 [1, 512, 16, 12]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 512, 16, 12]          1,024\n",
       "│    │    └─ReLU: 3-48                   [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 16, 12]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 16, 12]          1,024\n",
       "│    │    └─ReLU: 3-51                   [1, 512, 16, 12]          --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [1, 2]                    1,026\n",
       "==========================================================================================\n",
       "Total params: 11,171,266\n",
       "Trainable params: 11,171,266\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 6.58\n",
       "==========================================================================================\n",
       "Input size (MB): 0.37\n",
       "Forward/backward pass size (MB): 149.36\n",
       "Params size (MB): 44.69\n",
       "Estimated Total Size (MB): 194.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = generate_model(18)\n",
    "summary(model, (1, 1, 255, 367))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a30649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared Cache\n",
      "Device: cuda:0\n",
      "Epoch: 0   Training Loss: 9.001903265839102 \n",
      "Epoch: 0   Validation Loss: 8.718365011807117 \n",
      "Epoch: 1   Training Loss: 8.977343308366933 \n",
      "Epoch: 1   Validation Loss: 8.718365464013484 \n",
      "Epoch: 2   Training Loss: 8.977343272391584 \n",
      "Epoch: 2   Validation Loss: 8.718365976444135 \n",
      "Epoch: 3   Training Loss: 8.977342571284499 \n",
      "Epoch: 3   Validation Loss: 8.718367257598167 \n",
      "Epoch: 4   Training Loss: 8.977342513580778 \n",
      "Epoch: 4   Validation Loss: 8.718368900542664 \n",
      "Epoch: 5   Training Loss: 8.977342381711388 \n",
      "Epoch: 5   Validation Loss: 8.718369437027102 \n",
      "Epoch: 6   Training Loss: 8.97734225643324 \n",
      "Epoch: 6   Validation Loss: 8.718369257967298 \n",
      "Epoch: 7   Training Loss: 8.977342362506914 \n",
      "Epoch: 7   Validation Loss: 8.718370199143887 \n",
      "Epoch: 8   Training Loss: 8.977343134102437 \n",
      "Epoch: 8   Validation Loss: 8.718371050827827 \n",
      "Epoch: 9   Training Loss: 8.977342989075142 \n",
      "Epoch: 9   Validation Loss: 8.718372612374525 \n",
      "Epoch: 10   Training Loss: 8.977341343335684 \n",
      "Epoch: 10   Validation Loss: 8.71836572574079 \n",
      "Epoch: 11   Training Loss: 8.977339565763975 \n",
      "Epoch: 11   Validation Loss: 8.718364108829448 \n",
      "Epoch: 12   Training Loss: 8.977341987447138 \n",
      "Epoch: 12   Validation Loss: 8.718364589144166 \n",
      "Epoch: 13   Training Loss: 8.977341470781814 \n",
      "Epoch: 13   Validation Loss: 8.718364487988046 \n",
      "Epoch: 14   Training Loss: 8.977341917300484 \n",
      "Epoch: 14   Validation Loss: 8.718364443508495 \n",
      "Epoch: 15   Training Loss: 8.97734181731477 \n",
      "Epoch: 15   Validation Loss: 8.718364080145955 \n",
      "Epoch: 16   Training Loss: 8.977341120132365 \n",
      "Epoch: 16   Validation Loss: 8.718364470262701 \n",
      "Epoch: 17   Training Loss: 8.97734153527154 \n",
      "Epoch: 17   Validation Loss: 8.71836449522132 \n",
      "Epoch: 18   Training Loss: 8.977341467378212 \n",
      "Epoch: 18   Validation Loss: 8.718364471669206 \n",
      "Epoch: 19   Training Loss: 8.977341133806746 \n",
      "Epoch: 19   Validation Loss: 8.718364259780488 \n",
      "Epoch: 20   Training Loss: 8.977340172938689 \n",
      "Epoch: 20   Validation Loss: 8.718363819798661 \n",
      "Epoch: 21   Training Loss: 8.977341357605852 \n",
      "Epoch: 21   Validation Loss: 8.71836465257075 \n",
      "Epoch: 22   Training Loss: 8.977340197672113 \n",
      "Epoch: 22   Validation Loss: 8.718363345490147 \n",
      "Epoch: 23   Training Loss: 8.977373168170487 \n",
      "Epoch: 23   Validation Loss: 8.718366213202476 \n",
      "Epoch: 24   Training Loss: 8.977340720690684 \n",
      "Epoch: 24   Validation Loss: 8.71836499165102 \n",
      "Epoch: 25   Training Loss: 8.977341974163272 \n",
      "Epoch: 25   Validation Loss: 8.718365411427493 \n",
      "Epoch: 26   Training Loss: 8.977341960699746 \n",
      "Epoch: 26   Validation Loss: 8.718365659303343 \n",
      "Epoch: 27   Training Loss: 8.977341687128883 \n",
      "Epoch: 27   Validation Loss: 8.718365909101648 \n",
      "Epoch: 28   Training Loss: 8.977341429008128 \n",
      "Epoch: 28   Validation Loss: 8.718365977609738 \n",
      "Epoch: 29   Training Loss: 8.977341535217992 \n",
      "Epoch: 29   Validation Loss: 8.718366220721768 \n",
      "Epoch: 30   Training Loss: 8.977341340605973 \n",
      "Epoch: 30   Validation Loss: 8.718366457635122 \n",
      "Epoch: 31   Training Loss: 8.977341704024491 \n",
      "Epoch: 31   Validation Loss: 8.71836644496479 \n",
      "Epoch: 32   Training Loss: 8.977340933360674 \n",
      "Epoch: 32   Validation Loss: 8.718366654857785 \n",
      "Epoch: 33   Training Loss: 8.977341255912952 \n",
      "Epoch: 33   Validation Loss: 8.7183668123494 \n",
      "Epoch: 34   Training Loss: 8.977340931512225 \n",
      "Epoch: 34   Validation Loss: 8.718367683212584 \n",
      "Epoch: 35   Training Loss: 8.977340780532694 \n",
      "Epoch: 35   Validation Loss: 8.718367929680479 \n",
      "Epoch: 36   Training Loss: 8.97734030974263 \n",
      "Epoch: 36   Validation Loss: 8.718369140914124 \n",
      "Epoch: 37   Training Loss: 8.977341194522541 \n",
      "Epoch: 37   Validation Loss: 8.71836968221205 \n",
      "Epoch: 38   Training Loss: 8.977340647179647 \n",
      "Epoch: 38   Validation Loss: 8.718368446431434 \n",
      "Epoch: 39   Training Loss: 8.977340562659867 \n",
      "Epoch: 39   Validation Loss: 8.718369231549817 \n",
      "Epoch: 40   Training Loss: 8.977340376543085 \n",
      "Epoch: 40   Validation Loss: 8.718372032890096 \n",
      "Epoch: 41   Training Loss: 8.977340531651029 \n",
      "Epoch: 41   Validation Loss: 8.718372028461967 \n",
      "Epoch: 42   Training Loss: 8.977339709400182 \n",
      "Epoch: 42   Validation Loss: 8.718371468673771 \n",
      "Epoch: 43   Training Loss: 8.977338721995903 \n",
      "Epoch: 43   Validation Loss: 8.718374566668023 \n",
      "Epoch: 44   Training Loss: 8.977339981691001 \n",
      "Epoch: 44   Validation Loss: 8.718375454984399 \n",
      "Epoch: 45   Training Loss: 8.977346001186747 \n",
      "Epoch: 45   Validation Loss: 8.718379960854021 \n",
      "Epoch: 46   Training Loss: 8.977337759657349 \n",
      "Epoch: 46   Validation Loss: 8.71838267257644 \n",
      "Epoch: 47   Training Loss: 8.977334182143618 \n",
      "Epoch: 47   Validation Loss: 8.718388618702482 \n",
      "Epoch: 48   Training Loss: 8.97733166027898 \n",
      "Epoch: 48   Validation Loss: 8.718389704726222 \n",
      "Epoch: 49   Training Loss: 8.977328703462515 \n",
      "Epoch: 49   Validation Loss: 8.718396383057659 \n",
      "Epoch: 50   Training Loss: 8.977334224324258 \n",
      "Epoch: 50   Validation Loss: 8.718404312505697 \n",
      "Epoch: 51   Training Loss: 8.977325340145413 \n",
      "Epoch: 51   Validation Loss: 8.718409990848041 \n",
      "Epoch: 52   Training Loss: 8.977328266298676 \n",
      "Epoch: 52   Validation Loss: 8.718418028955451 \n",
      "Epoch: 53   Training Loss: 8.977317072820876 \n",
      "Epoch: 53   Validation Loss: 8.718433303388663 \n",
      "Epoch: 54   Training Loss: 8.977321799201695 \n",
      "Epoch: 54   Validation Loss: 8.718435597424188 \n",
      "Epoch: 55   Training Loss: 8.977329072711806 \n",
      "Epoch: 55   Validation Loss: 8.718365596286539 \n",
      "Epoch: 56   Training Loss: 8.977341859285364 \n",
      "Epoch: 56   Validation Loss: 8.71836570366472 \n",
      "Epoch: 57   Training Loss: 8.977341898491728 \n",
      "Epoch: 57   Validation Loss: 8.718368889353549 \n",
      "Epoch: 58   Training Loss: 8.977341354732175 \n",
      "Epoch: 58   Validation Loss: 8.718370377668696 \n",
      "Epoch: 59   Training Loss: 8.977341210528964 \n",
      "Epoch: 59   Validation Loss: 8.718372024608156 \n",
      "Epoch: 60   Training Loss: 8.977340539590392 \n",
      "Epoch: 60   Validation Loss: 8.718377126933387 \n",
      "Epoch: 61   Training Loss: 8.977340584896838 \n",
      "Epoch: 61   Validation Loss: 8.718378428004268 \n",
      "Epoch: 62   Training Loss: 8.977341510074965 \n",
      "Epoch: 62   Validation Loss: 8.718377526142945 \n",
      "Epoch: 63   Training Loss: 8.977337525435347 \n",
      "Epoch: 63   Validation Loss: 8.718386741244338 \n",
      "Epoch: 64   Training Loss: 8.977334246366667 \n",
      "Epoch: 64   Validation Loss: 8.71840017046738 \n",
      "Epoch: 65   Training Loss: 8.977357401186456 \n",
      "Epoch: 65   Validation Loss: 8.718408715694004 \n",
      "Epoch: 66   Training Loss: 8.977325556319729 \n",
      "Epoch: 66   Validation Loss: 8.71841329490797 \n",
      "Epoch: 67   Training Loss: 8.977321308746033 \n",
      "Epoch: 67   Validation Loss: 8.718437353520446 \n",
      "Epoch: 68   Training Loss: 8.977316797557908 \n",
      "Epoch: 68   Validation Loss: 8.718487477724544 \n",
      "Epoch: 69   Training Loss: 8.977314728049034 \n",
      "Epoch: 69   Validation Loss: 8.718512015529392 \n",
      "Epoch: 70   Training Loss: 8.977306341951069 \n",
      "Epoch: 70   Validation Loss: 8.718504738363231 \n",
      "Epoch: 71   Training Loss: 8.977307987388773 \n",
      "Epoch: 71   Validation Loss: 8.718559087106026 \n",
      "Epoch: 72   Training Loss: 8.977354454488204 \n",
      "Epoch: 72   Validation Loss: 8.718423411171367 \n",
      "Epoch: 73   Training Loss: 8.97735577269839 \n",
      "Epoch: 73   Validation Loss: 8.718437933477572 \n",
      "Epoch: 74   Training Loss: 8.977327160971042 \n",
      "Epoch: 74   Validation Loss: 8.718472653631638 \n",
      "Epoch: 75   Training Loss: 8.977318459815734 \n",
      "Epoch: 75   Validation Loss: 8.718460571675768 \n",
      "Epoch: 76   Training Loss: 8.977327334898623 \n",
      "Epoch: 76   Validation Loss: 8.718469498037567 \n",
      "Epoch: 77   Training Loss: 8.977318275552928 \n",
      "Epoch: 77   Validation Loss: 8.718526004228016 \n",
      "Epoch: 78   Training Loss: 8.977307332942711 \n",
      "Epoch: 78   Validation Loss: 8.718494469746844 \n",
      "Epoch: 79   Training Loss: 8.977296955949893 \n",
      "Epoch: 79   Validation Loss: 8.718539476760249 \n",
      "Epoch: 80   Training Loss: 8.977284131984012 \n",
      "Epoch: 80   Validation Loss: 8.718616479766245 \n",
      "Epoch: 81   Training Loss: 8.977274920368188 \n",
      "Epoch: 81   Validation Loss: 8.71864175673067 \n",
      "Epoch: 82   Training Loss: 8.977276739218588 \n",
      "Epoch: 82   Validation Loss: 8.718734690303293 \n",
      "Epoch: 83   Training Loss: 8.977252109981096 \n",
      "Epoch: 83   Validation Loss: 8.71877394610395 \n",
      "Epoch: 84   Training Loss: 8.977263641659999 \n",
      "Epoch: 84   Validation Loss: 8.718781022900508 \n",
      "Epoch: 85   Training Loss: 8.977340015921747 \n",
      "Epoch: 85   Validation Loss: 8.718471307089553 \n",
      "Epoch: 86   Training Loss: 8.97735031862913 \n",
      "Epoch: 86   Validation Loss: 8.718473954623358 \n",
      "Epoch: 87   Training Loss: 8.977292664372664 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87   Validation Loss: 8.718643072041372 \n",
      "Epoch: 88   Training Loss: 8.977253435362378 \n",
      "Epoch: 88   Validation Loss: 8.71860249443673 \n",
      "Epoch: 89   Training Loss: 8.977270975116845 \n",
      "Epoch: 89   Validation Loss: 8.718621856703733 \n",
      "Epoch: 90   Training Loss: 8.97729793613489 \n",
      "Epoch: 90   Validation Loss: 8.718660972104201 \n",
      "Epoch: 91   Training Loss: 8.97746873829951 \n",
      "Epoch: 91   Validation Loss: 8.718699273117922 \n",
      "Epoch: 92   Training Loss: 8.977177918077167 \n",
      "Epoch: 92   Validation Loss: 8.719000683095409 \n",
      "Epoch: 93   Training Loss: 8.977211561076711 \n",
      "Epoch: 93   Validation Loss: 8.718743552230091 \n",
      "Epoch: 94   Training Loss: 8.977214862345614 \n",
      "Epoch: 94   Validation Loss: 8.718858965626504 \n",
      "Epoch: 95   Training Loss: 8.977155940180728 \n",
      "Epoch: 95   Validation Loss: 8.718931093799675 \n",
      "Epoch: 96   Training Loss: 8.97714275480105 \n",
      "Epoch: 96   Validation Loss: 8.718822850619857 \n",
      "Epoch: 97   Training Loss: 8.977147237874153 \n",
      "Epoch: 97   Validation Loss: 8.718960802335499 \n",
      "Epoch: 98   Training Loss: 8.977261868211935 \n",
      "Epoch: 98   Validation Loss: 8.719050289380984 \n",
      "Epoch: 99   Training Loss: 8.977180545790825 \n",
      "Epoch: 99   Validation Loss: 8.71920925616742 \n",
      "Epoch: 100   Training Loss: 8.977055726122343 \n",
      "Epoch: 100   Validation Loss: 8.719323528221395 \n",
      "Epoch: 101   Training Loss: 8.977040318487457 \n",
      "Epoch: 101   Validation Loss: 8.719101742092105 \n",
      "Epoch: 102   Training Loss: 8.977122142708945 \n",
      "Epoch: 102   Validation Loss: 8.719112439625173 \n",
      "Epoch: 103   Training Loss: 8.977133902739668 \n",
      "Epoch: 103   Validation Loss: 8.719031191338475 \n",
      "Epoch: 104   Training Loss: 8.97718601263947 \n",
      "Epoch: 104   Validation Loss: 8.718890105868173 \n",
      "Epoch: 105   Training Loss: 8.97710187373156 \n",
      "Epoch: 105   Validation Loss: 8.719079885971112 \n",
      "Epoch: 106   Training Loss: 8.977043485720701 \n",
      "Epoch: 106   Validation Loss: 8.719234599411694 \n",
      "Epoch: 107   Training Loss: 8.977065707584213 \n",
      "Epoch: 107   Validation Loss: 8.719200055145452 \n",
      "Epoch: 108   Training Loss: 8.977043982513818 \n",
      "Epoch: 108   Validation Loss: 8.719344941021978 \n",
      "Epoch: 109   Training Loss: 8.976944737171017 \n",
      "Epoch: 109   Validation Loss: 8.719651804668414 \n",
      "Epoch: 110   Training Loss: 8.976987141736206 \n",
      "Epoch: 110   Validation Loss: 8.719760585679879 \n",
      "Epoch: 111   Training Loss: 8.977056053427836 \n",
      "Epoch: 111   Validation Loss: 8.719630385241988 \n",
      "Epoch: 112   Training Loss: 8.976946135416975 \n",
      "Epoch: 112   Validation Loss: 8.719727020461216 \n",
      "Epoch: 113   Training Loss: 8.976884291775052 \n",
      "Epoch: 113   Validation Loss: 8.719841352679767 \n",
      "Epoch: 114   Training Loss: 8.977022902812115 \n",
      "Epoch: 114   Validation Loss: 8.719960527040477 \n",
      "Epoch: 115   Training Loss: 8.976908097732549 \n",
      "Epoch: 115   Validation Loss: 8.719854392868156 \n",
      "Epoch: 116   Training Loss: 8.976871720882924 \n",
      "Epoch: 116   Validation Loss: 8.720103801196338 \n",
      "Epoch: 117   Training Loss: 8.976831624912068 \n",
      "Epoch: 117   Validation Loss: 8.72003087949732 \n",
      "Epoch: 118   Training Loss: 8.976835751188831 \n",
      "Epoch: 118   Validation Loss: 8.719671248043474 \n",
      "Epoch: 119   Training Loss: 8.97688953933925 \n",
      "Epoch: 119   Validation Loss: 8.720062293822464 \n",
      "Epoch: 120   Training Loss: 8.976767830595541 \n",
      "Epoch: 120   Validation Loss: 8.720166308179497 \n",
      "Epoch: 121   Training Loss: 8.976864964305957 \n",
      "Epoch: 121   Validation Loss: 8.72017830083985 \n",
      "Epoch: 122   Training Loss: 8.97681808780803 \n",
      "Epoch: 122   Validation Loss: 8.720249143848196 \n",
      "Epoch: 123   Training Loss: 8.976792483778674 \n",
      "Epoch: 123   Validation Loss: 8.720356809399401 \n",
      "Epoch: 124   Training Loss: 8.976830715289015 \n",
      "Epoch: 124   Validation Loss: 8.72034356623857 \n",
      "Epoch: 125   Training Loss: 8.976796914094347 \n",
      "Epoch: 125   Validation Loss: 8.720371561170039 \n",
      "Epoch: 126   Training Loss: 8.976913418668355 \n",
      "Epoch: 126   Validation Loss: 8.720420446675892 \n",
      "Epoch: 127   Training Loss: 8.976788702023605 \n",
      "Epoch: 127   Validation Loss: 8.720382875289147 \n",
      "Epoch: 128   Training Loss: 8.97682177942463 \n",
      "Epoch: 128   Validation Loss: 8.720379389190528 \n",
      "Epoch: 129   Training Loss: 8.97671869620272 \n",
      "Epoch: 129   Validation Loss: 8.720169348369973 \n",
      "Epoch: 130   Training Loss: 8.976764756312516 \n",
      "Epoch: 130   Validation Loss: 8.720357525659931 \n",
      "Epoch: 131   Training Loss: 8.976764115408512 \n",
      "Epoch: 131   Validation Loss: 8.72024292511969 \n",
      "Epoch: 132   Training Loss: 8.976877006244136 \n",
      "Epoch: 132   Validation Loss: 8.720470507001505 \n",
      "Epoch: 133   Training Loss: 8.976717778075322 \n",
      "Epoch: 133   Validation Loss: 8.720206647355317 \n",
      "Epoch: 134   Training Loss: 8.976806715123939 \n",
      "Epoch: 134   Validation Loss: 8.720602766525415 \n",
      "Epoch: 135   Training Loss: 8.976757505549358 \n",
      "Epoch: 135   Validation Loss: 8.72050019249051 \n",
      "Epoch: 136   Training Loss: 8.976795028271697 \n",
      "Epoch: 136   Validation Loss: 8.72060514383432 \n",
      "Epoch: 137   Training Loss: 8.976676854660983 \n",
      "Epoch: 137   Validation Loss: 8.720766784426859 \n",
      "Epoch: 138   Training Loss: 8.97672370274785 \n",
      "Epoch: 138   Validation Loss: 8.720842044214718 \n",
      "Epoch: 139   Training Loss: 8.976757092214418 \n",
      "Epoch: 139   Validation Loss: 8.720896570381813 \n",
      "Epoch: 140   Training Loss: 8.976846988708084 \n",
      "Epoch: 140   Validation Loss: 8.72062824115126 \n",
      "Epoch: 141   Training Loss: 8.976867721025172 \n",
      "Epoch: 141   Validation Loss: 8.72059843355407 \n",
      "Epoch: 142   Training Loss: 8.976646758205588 \n",
      "Epoch: 142   Validation Loss: 8.720974844577515 \n",
      "Epoch: 143   Training Loss: 8.976935615733757 \n",
      "Epoch: 143   Validation Loss: 8.720305539917304 \n",
      "Epoch: 144   Training Loss: 8.976778181614284 \n",
      "Epoch: 144   Validation Loss: 8.720738924030526 \n",
      "Epoch: 145   Training Loss: 8.976746113637793 \n",
      "Epoch: 145   Validation Loss: 8.720577201278777 \n",
      "Epoch: 146   Training Loss: 8.976982237405277 \n",
      "Epoch: 146   Validation Loss: 8.718352730559289 \n",
      "Epoch: 147   Training Loss: 8.977356990471469 \n",
      "Epoch: 147   Validation Loss: 8.718363129403443 \n",
      "Epoch: 148   Training Loss: 8.977341784145539 \n",
      "Epoch: 148   Validation Loss: 8.718380494657698 \n",
      "Epoch: 149   Training Loss: 8.977305482013705 \n",
      "Epoch: 149   Validation Loss: 8.718386926754171 \n",
      "Epoch: 150   Training Loss: 8.977267991449692 \n",
      "Epoch: 150   Validation Loss: 8.71843010441545 \n",
      "Epoch: 151   Training Loss: 8.977247902334907 \n",
      "Epoch: 151   Validation Loss: 8.718407085306321 \n",
      "Epoch: 152   Training Loss: 8.977280786945899 \n",
      "Epoch: 152   Validation Loss: 8.718568241421755 \n",
      "Epoch: 153   Training Loss: 8.977266547397026 \n",
      "Epoch: 153   Validation Loss: 8.718810056202734 \n",
      "Epoch: 154   Training Loss: 8.977030799292283 \n",
      "Epoch: 154   Validation Loss: 8.720521311486348 \n",
      "Epoch: 155   Training Loss: 8.976727587454496 \n",
      "Epoch: 155   Validation Loss: 8.720829310223667 \n",
      "Epoch: 156   Training Loss: 8.976708746272113 \n",
      "Epoch: 156   Validation Loss: 8.720780506815865 \n",
      "Epoch: 157   Training Loss: 8.97695733475902 \n",
      "Epoch: 157   Validation Loss: 8.718338676068104 \n",
      "Epoch: 158   Training Loss: 8.977348355966345 \n",
      "Epoch: 158   Validation Loss: 8.718423496482169 \n",
      "Epoch: 159   Training Loss: 8.97726148259463 \n",
      "Epoch: 159   Validation Loss: 8.718476251477583 \n",
      "Epoch: 160   Training Loss: 8.977247262338603 \n",
      "Epoch: 160   Validation Loss: 8.718477871249327 \n",
      "Epoch: 161   Training Loss: 8.977217893049803 \n",
      "Epoch: 161   Validation Loss: 8.718875011608626 \n",
      "Epoch: 162   Training Loss: 8.97722857076648 \n",
      "Epoch: 162   Validation Loss: 8.719772399464725 \n",
      "Epoch: 163   Training Loss: 8.97694096171111 \n",
      "Epoch: 163   Validation Loss: 8.718404935597029 \n",
      "Epoch: 164   Training Loss: 8.977241221691248 \n",
      "Epoch: 164   Validation Loss: 8.71868748870823 \n",
      "Epoch: 165   Training Loss: 8.977202069871042 \n",
      "Epoch: 165   Validation Loss: 8.718894800575967 \n",
      "Epoch: 166   Training Loss: 8.977207291668607 \n",
      "Epoch: 166   Validation Loss: 8.718870510136915 \n",
      "Epoch: 167   Training Loss: 8.977203044190134 \n",
      "Epoch: 167   Validation Loss: 8.719352498101692 \n",
      "Epoch: 168   Training Loss: 8.977048327077277 \n",
      "Epoch: 168   Validation Loss: 8.72028343582319 \n",
      "Epoch: 169   Training Loss: 8.977068277829243 \n",
      "Epoch: 169   Validation Loss: 8.718485987360175 \n",
      "Epoch: 170   Training Loss: 8.977012801438121 \n",
      "Epoch: 170   Validation Loss: 8.720324564391644 \n",
      "Epoch: 171   Training Loss: 8.976988266460815 \n",
      "Epoch: 171   Validation Loss: 8.718412720594761 \n",
      "Epoch: 172   Training Loss: 8.97716958609133 \n",
      "Epoch: 172   Validation Loss: 8.71840943766468 \n",
      "Epoch: 173   Training Loss: 8.977224757858835 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 173   Validation Loss: 8.718699540746398 \n",
      "Epoch: 174   Training Loss: 8.97708535436568 \n",
      "Epoch: 174   Validation Loss: 8.72009408580698 \n",
      "Epoch: 175   Training Loss: 8.97711379629542 \n",
      "Epoch: 175   Validation Loss: 8.718429277019782 \n",
      "Epoch: 176   Training Loss: 8.977175784117703 \n",
      "Epoch: 176   Validation Loss: 8.720492665182592 \n",
      "Epoch: 177   Training Loss: 8.976805421115095 \n",
      "Epoch: 177   Validation Loss: 8.720462768891723 \n",
      "Epoch: 178   Training Loss: 8.976875721423745 \n",
      "Epoch: 178   Validation Loss: 8.718455802553747 \n",
      "Epoch: 179   Training Loss: 8.977180995204957 \n",
      "Epoch: 179   Validation Loss: 8.71845851636057 \n",
      "Epoch: 180   Training Loss: 8.977329874640409 \n",
      "Epoch: 180   Validation Loss: 8.718447177764132 \n",
      "Epoch: 181   Training Loss: 8.977119743026101 \n",
      "Epoch: 181   Validation Loss: 8.718411347275083 \n",
      "Epoch: 182   Training Loss: 8.977002310691049 \n",
      "Epoch: 182   Validation Loss: 8.720462442034648 \n",
      "Epoch: 183   Training Loss: 8.977110330473936 \n",
      "Epoch: 183   Validation Loss: 8.718422844519218 \n",
      "Epoch: 184   Training Loss: 8.977346693986439 \n",
      "Epoch: 184   Validation Loss: 8.718636506732656 \n",
      "Epoch: 185   Training Loss: 8.977324994983691 \n",
      "Epoch: 185   Validation Loss: 8.718788190037634 \n",
      "Epoch: 186   Training Loss: 8.977211258167964 \n",
      "Epoch: 186   Validation Loss: 8.71880995900246 \n",
      "Epoch: 187   Training Loss: 8.977276193430322 \n",
      "Epoch: 187   Validation Loss: 8.718955924058644 \n",
      "Epoch: 188   Training Loss: 8.977229166367545 \n",
      "Epoch: 188   Validation Loss: 8.719010494019836 \n",
      "Epoch: 189   Training Loss: 8.977181851476244 \n",
      "Epoch: 189   Validation Loss: 8.719078856888125 \n",
      "Epoch: 190   Training Loss: 8.977170004009983 \n",
      "Epoch: 190   Validation Loss: 8.719047514200831 \n",
      "Epoch: 191   Training Loss: 8.977160970321965 \n",
      "Epoch: 191   Validation Loss: 8.719115730316792 \n",
      "Epoch: 192   Training Loss: 8.977154743237275 \n",
      "Epoch: 192   Validation Loss: 8.719173372424828 \n",
      "Epoch: 193   Training Loss: 8.97711503309669 \n",
      "Epoch: 193   Validation Loss: 8.71917923782683 \n",
      "Epoch: 194   Training Loss: 8.977074648350294 \n",
      "Epoch: 194   Validation Loss: 8.71910597337296 \n",
      "Epoch: 195   Training Loss: 8.977085705822946 \n",
      "Epoch: 195   Validation Loss: 8.719228592784559 \n",
      "Epoch: 196   Training Loss: 8.977131684312397 \n",
      "Epoch: 196   Validation Loss: 8.719260054068123 \n",
      "Epoch: 197   Training Loss: 8.977216979096836 \n",
      "Epoch: 197   Validation Loss: 8.719543229905785 \n",
      "Epoch: 198   Training Loss: 8.976942485951275 \n",
      "Epoch: 198   Validation Loss: 8.718787023793181 \n",
      "Epoch: 199   Training Loss: 8.977229551446692 \n",
      "Epoch: 199   Validation Loss: 8.719214623037105 \n",
      "Epoch: 200   Training Loss: 8.97707765667061 \n",
      "Epoch: 200   Validation Loss: 8.71951151661937 \n",
      "Epoch: 201   Training Loss: 8.97699555385412 \n",
      "Epoch: 201   Validation Loss: 8.719629379296883 \n",
      "Epoch: 202   Training Loss: 8.976972886242745 \n",
      "Epoch: 202   Validation Loss: 8.719803258255434 \n",
      "Epoch: 203   Training Loss: 8.976892195862327 \n",
      "Epoch: 203   Validation Loss: 8.71988153676668 \n",
      "Epoch: 204   Training Loss: 8.976851314317566 \n",
      "Epoch: 204   Validation Loss: 8.719884171877471 \n",
      "Epoch: 205   Training Loss: 8.976811298672608 \n",
      "Epoch: 205   Validation Loss: 8.719956394795743 \n",
      "Epoch: 206   Training Loss: 8.976898909932531 \n",
      "Epoch: 206   Validation Loss: 8.719857811892819 \n",
      "Epoch: 207   Training Loss: 8.97679336879914 \n",
      "Epoch: 207   Validation Loss: 8.720067400967805 \n",
      "Epoch: 208   Training Loss: 8.976843220090219 \n",
      "Epoch: 208   Validation Loss: 8.72061279565634 \n",
      "Epoch: 209   Training Loss: 8.976853754511598 \n",
      "Epoch: 209   Validation Loss: 8.720111678616247 \n",
      "Epoch: 210   Training Loss: 8.976777845044957 \n",
      "Epoch: 210   Validation Loss: 8.720068958678075 \n",
      "Epoch: 211   Training Loss: 8.976755923736224 \n",
      "Epoch: 211   Validation Loss: 8.720148549688773 \n",
      "Epoch: 212   Training Loss: 8.976753129740793 \n",
      "Epoch: 212   Validation Loss: 8.720039822951476 \n",
      "Epoch: 213   Training Loss: 8.976972266464198 \n",
      "Epoch: 213   Validation Loss: 8.719725601486758 \n",
      "Epoch: 214   Training Loss: 8.976767080501764 \n",
      "Epoch: 214   Validation Loss: 8.71993280740248 \n",
      "Epoch: 215   Training Loss: 8.976811971507216 \n",
      "Epoch: 215   Validation Loss: 8.719827468288441 \n",
      "Epoch: 216   Training Loss: 8.976833629797033 \n",
      "Epoch: 216   Validation Loss: 8.719807002099335 \n",
      "Epoch: 217   Training Loss: 8.976925050111259 \n",
      "Epoch: 217   Validation Loss: 8.719321710899576 \n",
      "Epoch: 218   Training Loss: 8.97677693865994 \n",
      "Epoch: 218   Validation Loss: 8.719578343525528 \n",
      "Epoch: 219   Training Loss: 8.976569997530275 \n",
      "Epoch: 219   Validation Loss: 8.71995369141642 \n",
      "Epoch: 220   Training Loss: 8.97692198587904 \n",
      "Epoch: 220   Validation Loss: 8.719807476151942 \n",
      "Epoch: 221   Training Loss: 8.976531479828001 \n",
      "Epoch: 221   Validation Loss: 8.720068831221408 \n",
      "Epoch: 222   Training Loss: 8.976956580130475 \n",
      "Epoch: 222   Validation Loss: 8.719775419361476 \n",
      "Epoch: 223   Training Loss: 8.97664843577316 \n",
      "Epoch: 223   Validation Loss: 8.719907650124592 \n",
      "Epoch: 224   Training Loss: 8.976732252795847 \n",
      "Epoch: 224   Validation Loss: 8.719710157568876 \n",
      "Epoch: 225   Training Loss: 8.976799022004766 \n",
      "Epoch: 225   Validation Loss: 8.719801781138198 \n",
      "Epoch: 226   Training Loss: 8.976716003449193 \n",
      "Epoch: 226   Validation Loss: 8.719935318114237 \n",
      "Epoch: 227   Training Loss: 8.976793908446862 \n",
      "Epoch: 227   Validation Loss: 8.720114127762718 \n",
      "Epoch: 228   Training Loss: 8.97673114476917 \n",
      "Epoch: 228   Validation Loss: 8.719964043253826 \n",
      "Epoch: 229   Training Loss: 8.976773584046347 \n",
      "Epoch: 229   Validation Loss: 8.72010845677389 \n",
      "Epoch: 230   Training Loss: 8.977078420373447 \n",
      "Epoch: 230   Validation Loss: 8.718890706905889 \n",
      "Epoch: 231   Training Loss: 8.977075886397504 \n",
      "Epoch: 231   Validation Loss: 8.719289530959912 \n",
      "Epoch: 232   Training Loss: 8.976914050091501 \n",
      "Epoch: 232   Validation Loss: 8.71942628552299 \n",
      "Epoch: 233   Training Loss: 8.977068515455366 \n",
      "Epoch: 233   Validation Loss: 8.719198956540682 \n",
      "Epoch: 234   Training Loss: 8.976942344304582 \n",
      "Epoch: 234   Validation Loss: 8.71939781375519 \n",
      "Epoch: 235   Training Loss: 8.976959635149504 \n",
      "Epoch: 235   Validation Loss: 8.719718745599915 \n",
      "Epoch: 236   Training Loss: 8.977080046799076 \n",
      "Epoch: 236   Validation Loss: 8.719329167538012 \n",
      "Epoch: 237   Training Loss: 8.976982973060853 \n",
      "Epoch: 237   Validation Loss: 8.719439650374362 \n",
      "Epoch: 238   Training Loss: 8.97676558307502 \n",
      "Epoch: 238   Validation Loss: 8.71940958399874 \n",
      "Epoch: 239   Training Loss: 8.976778501790445 \n",
      "Epoch: 239   Validation Loss: 8.719510014173057 \n",
      "Epoch: 240   Training Loss: 8.976527372096918 \n",
      "Epoch: 240   Validation Loss: 8.720094390710402 \n",
      "Epoch: 241   Training Loss: 8.976434707170727 \n",
      "Epoch: 241   Validation Loss: 8.72023459104728 \n",
      "Epoch: 242   Training Loss: 8.976720716867277 \n",
      "Epoch: 242   Validation Loss: 8.718334164807573 \n",
      "Epoch: 243   Training Loss: 8.976860822284662 \n",
      "Epoch: 243   Validation Loss: 8.718423286889783 \n",
      "Epoch: 244   Training Loss: 8.977434002810481 \n",
      "Epoch: 244   Validation Loss: 8.718380334245351 \n",
      "Epoch: 245   Training Loss: 8.977337332778376 \n",
      "Epoch: 245   Validation Loss: 8.718395848286338 \n",
      "Epoch: 246   Training Loss: 8.977331735800972 \n",
      "Epoch: 246   Validation Loss: 8.718415197121187 \n",
      "Epoch: 247   Training Loss: 8.97731748146384 \n",
      "Epoch: 247   Validation Loss: 8.718441080943888 \n",
      "Epoch: 248   Training Loss: 8.977304577761696 \n",
      "Epoch: 248   Validation Loss: 8.718468441566245 \n",
      "Epoch: 249   Training Loss: 8.977304974634922 \n",
      "Epoch: 249   Validation Loss: 8.718523793314894 \n",
      "Epoch: 250   Training Loss: 8.97728460215594 \n",
      "Epoch: 250   Validation Loss: 8.718608408250846 \n",
      "Epoch: 251   Training Loss: 8.97725114230895 \n",
      "Epoch: 251   Validation Loss: 8.718716510188766 \n",
      "Epoch: 252   Training Loss: 8.977194917612257 \n",
      "Epoch: 252   Validation Loss: 8.71888275927895 \n",
      "Epoch: 253   Training Loss: 8.977165362578544 \n",
      "Epoch: 253   Validation Loss: 8.718921582775915 \n",
      "Epoch: 254   Training Loss: 8.977079286967971 \n",
      "Epoch: 254   Validation Loss: 8.719087163416463 \n",
      "Epoch: 255   Training Loss: 8.976997283193061 \n",
      "Epoch: 255   Validation Loss: 8.719231288143433 \n",
      "Epoch: 256   Training Loss: 8.976936084075518 \n",
      "Epoch: 256   Validation Loss: 8.719354233113117 \n",
      "Epoch: 257   Training Loss: 8.976862690704479 \n",
      "Epoch: 257   Validation Loss: 8.7196622818904 \n",
      "Epoch: 258   Training Loss: 8.977120173994205 \n",
      "Epoch: 258   Validation Loss: 8.719853160821419 \n",
      "Epoch: 259   Training Loss: 8.9769657896014 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 259   Validation Loss: 8.720177161041025 \n",
      "Epoch: 260   Training Loss: 8.977106036762212 \n",
      "Epoch: 260   Validation Loss: 8.718577266745891 \n",
      "Epoch: 261   Training Loss: 8.977192626370737 \n",
      "Epoch: 261   Validation Loss: 8.71919879960662 \n",
      "Epoch: 262   Training Loss: 8.977139465875155 \n",
      "Epoch: 262   Validation Loss: 8.718889824909676 \n",
      "Epoch: 263   Training Loss: 8.9770374406288 \n",
      "Epoch: 263   Validation Loss: 8.71949489524143 \n",
      "Epoch: 264   Training Loss: 8.976944022804528 \n",
      "Epoch: 264   Validation Loss: 8.71951127211139 \n",
      "Epoch: 265   Training Loss: 8.976845075036442 \n",
      "Epoch: 265   Validation Loss: 8.71989746026643 \n",
      "Epoch: 266   Training Loss: 8.976799015360536 \n",
      "Epoch: 266   Validation Loss: 8.719596117079982 \n",
      "Epoch: 267   Training Loss: 8.976903175176114 \n",
      "Epoch: 267   Validation Loss: 8.719474233020097 \n",
      "Epoch: 268   Training Loss: 8.976731713631034 \n",
      "Epoch: 268   Validation Loss: 8.719741627762922 \n",
      "Epoch: 269   Training Loss: 8.976732938412116 \n",
      "Epoch: 269   Validation Loss: 8.71914762566156 \n",
      "Epoch: 270   Training Loss: 8.976504383967123 \n",
      "Epoch: 270   Validation Loss: 8.720374948641906 \n",
      "Epoch: 271   Training Loss: 8.976596939505184 \n",
      "Epoch: 271   Validation Loss: 8.72054277133062 \n",
      "Epoch: 272   Training Loss: 8.97644171583973 \n",
      "Epoch: 272   Validation Loss: 8.720508289457403 \n",
      "Epoch: 273   Training Loss: 8.976359801361617 \n",
      "Epoch: 273   Validation Loss: 8.720660518689806 \n",
      "Epoch: 274   Training Loss: 8.976297944228964 \n",
      "Epoch: 274   Validation Loss: 8.7204465698992 \n",
      "Epoch: 275   Training Loss: 8.976163333998523 \n",
      "Epoch: 275   Validation Loss: 8.720722142786512 \n",
      "Epoch: 276   Training Loss: 8.975878311632938 \n",
      "Epoch: 276   Validation Loss: 8.720924640091964 \n",
      "Epoch: 277   Training Loss: 8.975725686787372 \n",
      "Epoch: 277   Validation Loss: 8.720775787770851 \n",
      "Epoch: 278   Training Loss: 8.975633292056873 \n",
      "Epoch: 278   Validation Loss: 8.720685882863899 \n",
      "Epoch: 279   Training Loss: 8.975524843955082 \n",
      "Epoch: 279   Validation Loss: 8.721287886331583 \n",
      "Epoch: 280   Training Loss: 8.975484071879675 \n",
      "Epoch: 280   Validation Loss: 8.720847351111058 \n",
      "Epoch: 281   Training Loss: 8.975384136902052 \n",
      "Epoch: 281   Validation Loss: 8.721344337857639 \n",
      "Epoch: 282   Training Loss: 8.976274797467314 \n",
      "Epoch: 282   Validation Loss: 8.721231560436802 \n",
      "Epoch: 283   Training Loss: 8.975859096400299 \n",
      "Epoch: 283   Validation Loss: 8.721528164981347 \n",
      "Epoch: 284   Training Loss: 8.975328407951327 \n",
      "Epoch: 284   Validation Loss: 8.720911815845511 \n",
      "Epoch: 285   Training Loss: 8.975322337368146 \n",
      "Epoch: 285   Validation Loss: 8.722714263208935 \n",
      "Epoch: 286   Training Loss: 8.97495453422369 \n",
      "Epoch: 286   Validation Loss: 8.721421763496886 \n",
      "Epoch: 287   Training Loss: 8.975122001653041 \n",
      "Epoch: 287   Validation Loss: 8.720151861849759 \n",
      "Epoch: 288   Training Loss: 8.976508968460795 \n",
      "Epoch: 288   Validation Loss: 8.718529312312086 \n",
      "Epoch: 289   Training Loss: 8.975811656448764 \n",
      "Epoch: 289   Validation Loss: 8.720715068976189 \n",
      "Epoch: 290   Training Loss: 8.976090775535306 \n",
      "Epoch: 290   Validation Loss: 8.719527136969795 \n",
      "Epoch: 291   Training Loss: 8.976705791953792 \n",
      "Epoch: 291   Validation Loss: 8.719562394881207 \n",
      "Epoch: 292   Training Loss: 8.976524548087266 \n",
      "Epoch: 292   Validation Loss: 8.720011527157803 \n",
      "Epoch: 293   Training Loss: 8.977344172940233 \n",
      "Epoch: 293   Validation Loss: 8.7179747987165 \n",
      "Epoch: 294   Training Loss: 8.976694172035886 \n",
      "Epoch: 294   Validation Loss: 8.720083582729307 \n",
      "Epoch: 295   Training Loss: 8.9766628297481 \n",
      "Epoch: 295   Validation Loss: 8.719122542668858 \n",
      "Epoch: 296   Training Loss: 8.976847223940633 \n",
      "Epoch: 296   Validation Loss: 8.719278499555982 \n",
      "Epoch: 297   Training Loss: 8.975910965935864 \n",
      "Epoch: 297   Validation Loss: 8.720043098526386 \n",
      "Epoch: 298   Training Loss: 8.97560629393898 \n",
      "Epoch: 298   Validation Loss: 8.720962603550197 \n",
      "Epoch: 299   Training Loss: 8.975982571290533 \n",
      "Epoch: 299   Validation Loss: 8.722086879689867 \n",
      "Epoch: 300   Training Loss: 8.97513898270965 \n",
      "Epoch: 300   Validation Loss: 8.723595175383302 \n",
      "Epoch: 301   Training Loss: 8.974919726519273 \n",
      "Epoch: 301   Validation Loss: 8.723171322370879 \n",
      "Epoch: 302   Training Loss: 8.974534919760133 \n",
      "Epoch: 302   Validation Loss: 8.724633309132937 \n",
      "Epoch: 303   Training Loss: 8.974941994648269 \n",
      "Epoch: 303   Validation Loss: 8.723581964141069 \n",
      "Epoch: 304   Training Loss: 8.976127126716989 \n",
      "Epoch: 304   Validation Loss: 8.71890780057934 \n",
      "Epoch: 305   Training Loss: 8.97724122089103 \n",
      "Epoch: 305   Validation Loss: 8.718810616452247 \n",
      "Epoch: 306   Training Loss: 8.97715830438023 \n",
      "Epoch: 306   Validation Loss: 8.719335120342569 \n",
      "Epoch: 307   Training Loss: 8.976760790004981 \n",
      "Epoch: 307   Validation Loss: 8.719283501952473 \n",
      "Epoch: 308   Training Loss: 8.975094791887745 \n",
      "Epoch: 308   Validation Loss: 8.722313145113384 \n",
      "Epoch: 309   Training Loss: 8.974559231773924 \n",
      "Epoch: 309   Validation Loss: 8.722940009390728 \n",
      "Epoch: 310   Training Loss: 8.977377642807367 \n",
      "Epoch: 310   Validation Loss: 8.719129453009202 \n",
      "Epoch: 311   Training Loss: 8.97693855480523 \n",
      "Epoch: 311   Validation Loss: 8.719665606872281 \n",
      "Epoch: 312   Training Loss: 8.976790364611304 \n",
      "Epoch: 312   Validation Loss: 8.719703876739471 \n",
      "Epoch: 313   Training Loss: 8.976529087021063 \n",
      "Epoch: 313   Validation Loss: 8.720056634299043 \n",
      "Epoch: 314   Training Loss: 8.976121189294027 \n",
      "Epoch: 314   Validation Loss: 8.720461025023937 \n",
      "Epoch: 315   Training Loss: 8.976007968079392 \n",
      "Epoch: 315   Validation Loss: 8.720635284012701 \n",
      "Epoch: 316   Training Loss: 8.976029497170863 \n",
      "Epoch: 316   Validation Loss: 8.720102451875702 \n",
      "Epoch: 317   Training Loss: 8.976584794291124 \n",
      "Epoch: 317   Validation Loss: 8.720497963914337 \n",
      "Epoch: 318   Training Loss: 8.975994870799367 \n",
      "Epoch: 318   Validation Loss: 8.720411301496336 \n",
      "Epoch: 319   Training Loss: 8.97687729937258 \n",
      "Epoch: 319   Validation Loss: 8.71905119523706 \n",
      "Epoch: 320   Training Loss: 8.976609839452376 \n",
      "Epoch: 320   Validation Loss: 8.719320535643854 \n",
      "Epoch: 321   Training Loss: 8.976364191082132 \n",
      "Epoch: 321   Validation Loss: 8.72008082987161 \n",
      "Epoch: 322   Training Loss: 8.976461046765458 \n",
      "Epoch: 322   Validation Loss: 8.719332183158128 \n",
      "Epoch: 323   Training Loss: 8.975878579376277 \n",
      "Epoch: 323   Validation Loss: 8.718871824196436 \n",
      "Epoch: 324   Training Loss: 8.975762584047695 \n",
      "Epoch: 324   Validation Loss: 8.719654596647247 \n",
      "Epoch: 325   Training Loss: 8.975005513662143 \n",
      "Epoch: 325   Validation Loss: 8.721340156995588 \n",
      "Epoch: 326   Training Loss: 8.97543153777586 \n",
      "Epoch: 326   Validation Loss: 8.721785928107074 \n",
      "Epoch: 327   Training Loss: 8.975329120882948 \n",
      "Epoch: 327   Validation Loss: 8.720881630993448 \n",
      "Epoch: 328   Training Loss: 8.975115254063825 \n",
      "Epoch: 328   Validation Loss: 8.720144017949494 \n",
      "Epoch: 329   Training Loss: 8.975921706851945 \n",
      "Epoch: 329   Validation Loss: 8.719362827906075 \n",
      "Epoch: 330   Training Loss: 8.974746664412576 \n",
      "Epoch: 330   Validation Loss: 8.720308989738218 \n",
      "Epoch: 331   Training Loss: 8.973918667397536 \n",
      "Epoch: 331   Validation Loss: 8.720098325821985 \n",
      "Epoch: 332   Training Loss: 8.975038255436608 \n",
      "Epoch: 332   Validation Loss: 8.719728790085762 \n",
      "Epoch: 333   Training Loss: 8.974703794620737 \n",
      "Epoch: 333   Validation Loss: 8.71884468837279 \n",
      "Epoch: 334   Training Loss: 8.976005673265957 \n",
      "Epoch: 334   Validation Loss: 8.719071154529642 \n",
      "Epoch: 335   Training Loss: 8.976349277203667 \n",
      "Epoch: 335   Validation Loss: 8.719023741434846 \n",
      "Epoch: 336   Training Loss: 8.975356933492044 \n",
      "Epoch: 336   Validation Loss: 8.719011927273538 \n",
      "Epoch: 337   Training Loss: 8.975491616478077 \n",
      "Epoch: 337   Validation Loss: 8.718733678848793 \n",
      "Epoch: 338   Training Loss: 8.97572021494736 \n",
      "Epoch: 338   Validation Loss: 8.718921901324764 \n",
      "Epoch: 339   Training Loss: 8.97581036148909 \n",
      "Epoch: 339   Validation Loss: 8.71877228863589 \n",
      "Epoch: 340   Training Loss: 8.976177509600877 \n",
      "Epoch: 340   Validation Loss: 8.720529767230774 \n",
      "Epoch: 341   Training Loss: 8.975104606178586 \n",
      "Epoch: 341   Validation Loss: 8.71914940183258 \n",
      "Epoch: 342   Training Loss: 8.974346588843643 \n",
      "Epoch: 342   Validation Loss: 8.720928405317427 \n",
      "Epoch: 343   Training Loss: 8.97544420520369 \n",
      "Epoch: 343   Validation Loss: 8.72068641869341 \n",
      "Epoch: 344   Training Loss: 8.975513860954031 \n",
      "Epoch: 344   Validation Loss: 8.719001773213243 \n",
      "Epoch: 345   Training Loss: 8.9745431988909 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 345   Validation Loss: 8.720129074762266 \n",
      "Epoch: 346   Training Loss: 8.973227328142743 \n",
      "Epoch: 346   Validation Loss: 8.722153338140757 \n",
      "Epoch: 347   Training Loss: 8.973080901892565 \n",
      "Epoch: 347   Validation Loss: 8.724106038008506 \n",
      "Epoch: 348   Training Loss: 8.972986118176738 \n",
      "Epoch: 348   Validation Loss: 8.724294354469464 \n",
      "Epoch: 349   Training Loss: 8.973419308788868 \n",
      "Epoch: 349   Validation Loss: 8.723737346051701 \n",
      "Epoch: 350   Training Loss: 8.973039489824496 \n",
      "Epoch: 350   Validation Loss: 8.723313133075937 \n",
      "Epoch: 351   Training Loss: 8.972288680793193 \n",
      "Epoch: 351   Validation Loss: 8.723848817508824 \n",
      "Epoch: 352   Training Loss: 8.972965576584693 \n",
      "Epoch: 352   Validation Loss: 8.725830072685456 \n",
      "Epoch: 353   Training Loss: 8.972442745691358 \n",
      "Epoch: 353   Validation Loss: 8.726725774928047 \n",
      "Epoch: 354   Training Loss: 8.972637951311253 \n",
      "Epoch: 354   Validation Loss: 8.725299418488518 \n",
      "Epoch: 355   Training Loss: 8.973129309950872 \n",
      "Epoch: 355   Validation Loss: 8.72397719519518 \n",
      "Epoch: 356   Training Loss: 8.97562960698477 \n",
      "Epoch: 356   Validation Loss: 8.719654973592712 \n",
      "Epoch: 357   Training Loss: 8.975387141364317 \n",
      "Epoch: 357   Validation Loss: 8.720616976170179 \n",
      "Epoch: 358   Training Loss: 8.974653091297508 \n",
      "Epoch: 358   Validation Loss: 8.720516477136044 \n",
      "Epoch: 359   Training Loss: 8.97532662681147 \n",
      "Epoch: 359   Validation Loss: 8.719831430750382 \n",
      "Epoch: 360   Training Loss: 8.976598105938912 \n",
      "Epoch: 360   Validation Loss: 8.719929266182707 \n",
      "Epoch: 361   Training Loss: 8.97600541294753 \n",
      "Epoch: 361   Validation Loss: 8.719923991647653 \n",
      "Epoch: 362   Training Loss: 8.975096592835339 \n",
      "Epoch: 362   Validation Loss: 8.720520471456771 \n",
      "Epoch: 363   Training Loss: 8.97410692335755 \n",
      "Epoch: 363   Validation Loss: 8.722099939931908 \n",
      "Epoch: 364   Training Loss: 8.974163606327778 \n",
      "Epoch: 364   Validation Loss: 8.721055906850742 \n",
      "Epoch: 365   Training Loss: 8.973395186564005 \n",
      "Epoch: 365   Validation Loss: 8.723246020682053 \n",
      "Epoch: 366   Training Loss: 8.972827341431648 \n",
      "Epoch: 366   Validation Loss: 8.722384294041795 \n",
      "Epoch: 367   Training Loss: 8.971047729324996 \n",
      "Epoch: 367   Validation Loss: 8.723666488594375 \n",
      "Epoch: 368   Training Loss: 8.971024466793214 \n",
      "Epoch: 368   Validation Loss: 8.721870056004885 \n",
      "Epoch: 369   Training Loss: 8.970196110040527 \n",
      "Epoch: 369   Validation Loss: 8.72090438918552 \n",
      "Epoch: 370   Training Loss: 8.970491868929951 \n",
      "Epoch: 370   Validation Loss: 8.71983029350804 \n",
      "Epoch: 371   Training Loss: 8.970517362748643 \n",
      "Epoch: 371   Validation Loss: 8.72007612131598 \n",
      "Epoch: 372   Training Loss: 8.972597654337454 \n",
      "Epoch: 372   Validation Loss: 8.720300822088701 \n",
      "Epoch: 373   Training Loss: 8.970918670874234 \n",
      "Epoch: 373   Validation Loss: 8.721748038244847 \n",
      "Epoch: 374   Training Loss: 8.970293187788046 \n",
      "Epoch: 374   Validation Loss: 8.721686565867211 \n",
      "Epoch: 375   Training Loss: 8.969413964175153 \n",
      "Epoch: 375   Validation Loss: 8.723113653913709 \n",
      "Epoch: 376   Training Loss: 8.970109984350772 \n",
      "Epoch: 376   Validation Loss: 8.72150880868412 \n",
      "Epoch: 377   Training Loss: 8.969507169080863 \n",
      "Epoch: 377   Validation Loss: 8.723285418187372 \n",
      "Epoch: 378   Training Loss: 8.968987752381942 \n",
      "Epoch: 378   Validation Loss: 8.724477258173852 \n",
      "Epoch: 379   Training Loss: 8.968192890375532 \n",
      "Epoch: 379   Validation Loss: 8.724188615379544 \n",
      "Epoch: 380   Training Loss: 8.96683673031344 \n",
      "Epoch: 380   Validation Loss: 8.725934481117305 \n",
      "Epoch: 381   Training Loss: 8.964855204415338 \n",
      "Epoch: 381   Validation Loss: 8.724813637099514 \n",
      "Epoch: 382   Training Loss: 8.964539630249856 \n",
      "Epoch: 382   Validation Loss: 8.722903050498623 \n",
      "Epoch: 383   Training Loss: 8.966045607109628 \n",
      "Epoch: 383   Validation Loss: 8.721184532624617 \n",
      "Epoch: 384   Training Loss: 8.96200039231507 \n",
      "Epoch: 384   Validation Loss: 8.723515453697752 \n",
      "Epoch: 385   Training Loss: 8.959711426462166 \n",
      "Epoch: 385   Validation Loss: 8.723078560202447 \n",
      "Epoch: 386   Training Loss: 8.961217401471055 \n",
      "Epoch: 386   Validation Loss: 8.72201698811069 \n",
      "Epoch: 387   Training Loss: 8.970125104409377 \n",
      "Epoch: 387   Validation Loss: 8.718402243125873 \n",
      "Epoch: 388   Training Loss: 8.976975699490064 \n",
      "Epoch: 388   Validation Loss: 8.719374671433535 \n",
      "Epoch: 389   Training Loss: 8.975616042587927 \n",
      "Epoch: 389   Validation Loss: 8.71975870127593 \n",
      "Epoch: 390   Training Loss: 8.974310205355316 \n",
      "Epoch: 390   Validation Loss: 8.719073748918975 \n",
      "Epoch: 391   Training Loss: 8.973577809483764 \n",
      "Epoch: 391   Validation Loss: 8.720209575145608 \n",
      "Epoch: 392   Training Loss: 8.973053460726058 \n",
      "Epoch: 392   Validation Loss: 8.719403366210509 \n",
      "Epoch: 393   Training Loss: 8.972282348788609 \n",
      "Epoch: 393   Validation Loss: 8.71978257375746 \n",
      "Epoch: 394   Training Loss: 8.971622206116624 \n",
      "Epoch: 394   Validation Loss: 8.720879248189274 \n",
      "Epoch: 395   Training Loss: 8.97182764758652 \n",
      "Epoch: 395   Validation Loss: 8.721208545034285 \n",
      "Epoch: 396   Training Loss: 8.973019531114543 \n",
      "Epoch: 396   Validation Loss: 8.721947267575603 \n",
      "Epoch: 397   Training Loss: 8.971671560925712 \n",
      "Epoch: 397   Validation Loss: 8.721163299585383 \n",
      "Epoch: 398   Training Loss: 8.97076936499863 \n",
      "Epoch: 398   Validation Loss: 8.72200680595626 \n",
      "Epoch: 399   Training Loss: 8.969340417304497 \n",
      "Epoch: 399   Validation Loss: 8.721688936112605 \n",
      "Epoch: 400   Training Loss: 8.969164333359965 \n",
      "Epoch: 400   Validation Loss: 8.720941247838704 \n",
      "Epoch: 401   Training Loss: 8.971816480334446 \n",
      "Epoch: 401   Validation Loss: 8.71997807419057 \n",
      "Epoch: 402   Training Loss: 8.973677304017382 \n",
      "Epoch: 402   Validation Loss: 8.721798697415274 \n",
      "Epoch: 403   Training Loss: 8.9727553164426 \n",
      "Epoch: 403   Validation Loss: 8.721070364181935 \n",
      "Epoch: 404   Training Loss: 8.971938692448761 \n",
      "Epoch: 404   Validation Loss: 8.722039812630612 \n",
      "Epoch: 405   Training Loss: 8.97208680278094 \n",
      "Epoch: 405   Validation Loss: 8.72369161120709 \n",
      "Epoch: 406   Training Loss: 8.970850621783429 \n",
      "Epoch: 406   Validation Loss: 8.726366075552832 \n",
      "Epoch: 407   Training Loss: 8.970020193852617 \n",
      "Epoch: 407   Validation Loss: 8.725734679354199 \n",
      "Epoch: 408   Training Loss: 8.972029767801631 \n",
      "Epoch: 408   Validation Loss: 8.725019465166248 \n",
      "Epoch: 409   Training Loss: 8.969242625690823 \n",
      "Epoch: 409   Validation Loss: 8.726446293433765 \n",
      "Epoch: 410   Training Loss: 8.97103762741541 \n",
      "Epoch: 410   Validation Loss: 8.72728153902758 \n",
      "Epoch: 411   Training Loss: 8.969276031143902 \n",
      "Epoch: 411   Validation Loss: 8.727296472688877 \n",
      "Epoch: 412   Training Loss: 8.967801385257948 \n",
      "Epoch: 412   Validation Loss: 8.726174535943091 \n",
      "Epoch: 413   Training Loss: 8.968235034793201 \n",
      "Epoch: 413   Validation Loss: 8.7249646480278 \n",
      "Epoch: 414   Training Loss: 8.967008050740422 \n",
      "Epoch: 414   Validation Loss: 8.727923990890021 \n",
      "Epoch: 415   Training Loss: 8.968358245812466 \n",
      "Epoch: 415   Validation Loss: 8.731886280727107 \n",
      "Epoch: 416   Training Loss: 8.966788047044648 \n",
      "Epoch: 416   Validation Loss: 8.732348757163674 \n",
      "Epoch: 417   Training Loss: 8.967993055167595 \n",
      "Epoch: 417   Validation Loss: 8.732048918161851 \n",
      "Epoch: 418   Training Loss: 8.966123657375082 \n",
      "Epoch: 418   Validation Loss: 8.736908871271519 \n",
      "Epoch: 419   Training Loss: 8.967244097480165 \n",
      "Epoch: 419   Validation Loss: 8.730328490590624 \n",
      "Epoch: 420   Training Loss: 8.966625224586297 \n",
      "Epoch: 420   Validation Loss: 8.724079539942194 \n",
      "Epoch: 421   Training Loss: 8.969648394890799 \n",
      "Epoch: 421   Validation Loss: 8.72478617859404 \n",
      "Epoch: 422   Training Loss: 8.971345537675038 \n",
      "Epoch: 422   Validation Loss: 8.724892942922732 \n",
      "Epoch: 423   Training Loss: 8.970850328738951 \n",
      "Epoch: 423   Validation Loss: 8.723563125868315 \n",
      "Epoch: 424   Training Loss: 8.973447588696827 \n",
      "Epoch: 424   Validation Loss: 8.728369709987401 \n",
      "Epoch: 425   Training Loss: 8.971466225168943 \n",
      "Epoch: 425   Validation Loss: 8.726116144005701 \n",
      "Epoch: 426   Training Loss: 8.970690318495313 \n",
      "Epoch: 426   Validation Loss: 8.724814016635634 \n",
      "Epoch: 427   Training Loss: 8.97098069447701 \n",
      "Epoch: 427   Validation Loss: 8.723337298329609 \n",
      "Epoch: 428   Training Loss: 8.976286788344071 \n",
      "Epoch: 428   Validation Loss: 8.720473934559358 \n",
      "Epoch: 429   Training Loss: 8.975573605549553 \n",
      "Epoch: 429   Validation Loss: 8.722468964221214 \n",
      "Epoch: 430   Training Loss: 8.975359886501169 \n",
      "Epoch: 430   Validation Loss: 8.7193288574345 \n",
      "Epoch: 431   Training Loss: 8.974129233774745 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 431   Validation Loss: 8.72007622487129 \n",
      "Epoch: 432   Training Loss: 8.97487148769237 \n",
      "Epoch: 432   Validation Loss: 8.720390409059926 \n",
      "Epoch: 433   Training Loss: 8.97415393070633 \n",
      "Epoch: 433   Validation Loss: 8.715535508913712 \n",
      "Epoch: 434   Training Loss: 8.974400795107304 \n",
      "Epoch: 434   Validation Loss: 8.716847402211796 \n",
      "Epoch: 435   Training Loss: 8.97374934457938 \n",
      "Epoch: 435   Validation Loss: 8.719075705383155 \n",
      "Epoch: 436   Training Loss: 8.974837872443826 \n",
      "Epoch: 436   Validation Loss: 8.717231906341182 \n",
      "Epoch: 437   Training Loss: 8.972759203659423 \n",
      "Epoch: 437   Validation Loss: 8.718619949341182 \n",
      "Epoch: 438   Training Loss: 8.971730591644231 \n",
      "Epoch: 438   Validation Loss: 8.718715928366356 \n",
      "Epoch: 439   Training Loss: 8.97153956791604 \n",
      "Epoch: 439   Validation Loss: 8.718716736319527 \n",
      "Epoch: 440   Training Loss: 8.971747213988378 \n",
      "Epoch: 440   Validation Loss: 8.722121709945922 \n",
      "Epoch: 441   Training Loss: 8.972039109313137 \n",
      "Epoch: 441   Validation Loss: 8.722172410615151 \n",
      "Epoch: 442   Training Loss: 8.97144360183218 \n",
      "Epoch: 442   Validation Loss: 8.720665122375513 \n",
      "Epoch: 443   Training Loss: 8.97089789201933 \n",
      "Epoch: 443   Validation Loss: 8.720903543387228 \n",
      "Epoch: 444   Training Loss: 8.969673452428527 \n",
      "Epoch: 444   Validation Loss: 8.721398488054207 \n",
      "Epoch: 445   Training Loss: 8.969425690451859 \n",
      "Epoch: 445   Validation Loss: 8.720602357248724 \n",
      "Epoch: 446   Training Loss: 8.969704730282265 \n",
      "Epoch: 446   Validation Loss: 8.721307640760205 \n",
      "Epoch: 447   Training Loss: 8.968160568872825 \n",
      "Epoch: 447   Validation Loss: 8.719606059184867 \n",
      "Epoch: 448   Training Loss: 8.969927765216973 \n",
      "Epoch: 448   Validation Loss: 8.720344182618842 \n",
      "Epoch: 449   Training Loss: 8.975284215002773 \n",
      "Epoch: 449   Validation Loss: 8.718731614057907 \n",
      "Epoch: 450   Training Loss: 8.975766704601043 \n",
      "Epoch: 450   Validation Loss: 8.722124677900432 \n",
      "Epoch: 451   Training Loss: 8.973660456370435 \n",
      "Epoch: 451   Validation Loss: 8.719159330580384 \n",
      "Epoch: 452   Training Loss: 8.976276724406684 \n",
      "Epoch: 452   Validation Loss: 8.721615047810186 \n",
      "Epoch: 453   Training Loss: 8.972884453950773 \n",
      "Epoch: 453   Validation Loss: 8.719442116610447 \n",
      "Epoch: 454   Training Loss: 8.968678882706806 \n",
      "Epoch: 454   Validation Loss: 8.720148952432753 \n",
      "Epoch: 455   Training Loss: 8.967926742085117 \n",
      "Epoch: 455   Validation Loss: 8.72165780226061 \n",
      "Epoch: 456   Training Loss: 8.969157381629966 \n",
      "Epoch: 456   Validation Loss: 8.72401382549146 \n",
      "Epoch: 457   Training Loss: 8.966281231363253 \n",
      "Epoch: 457   Validation Loss: 8.723689537385738 \n",
      "Epoch: 458   Training Loss: 8.96526485090012 \n",
      "Epoch: 458   Validation Loss: 8.722279253494925 \n",
      "Epoch: 459   Training Loss: 8.96387139117401 \n",
      "Epoch: 459   Validation Loss: 8.722598247442084 \n",
      "Epoch: 460   Training Loss: 8.962770721652651 \n",
      "Epoch: 460   Validation Loss: 8.722708581341909 \n",
      "Epoch: 461   Training Loss: 8.96154749772465 \n",
      "Epoch: 461   Validation Loss: 8.721609870848633 \n",
      "Epoch: 462   Training Loss: 8.959654352449125 \n",
      "Epoch: 462   Validation Loss: 8.721788376732574 \n",
      "Epoch: 463   Training Loss: 8.957038513887834 \n",
      "Epoch: 463   Validation Loss: 8.722127880736823 \n",
      "Epoch: 464   Training Loss: 8.957221836139324 \n",
      "Epoch: 464   Validation Loss: 8.722231412761463 \n",
      "Epoch: 465   Training Loss: 8.955438572501082 \n",
      "Epoch: 465   Validation Loss: 8.721148165516498 \n",
      "Epoch: 466   Training Loss: 8.954437226562465 \n",
      "Epoch: 466   Validation Loss: 8.72428183269211 \n",
      "Epoch: 467   Training Loss: 8.953475499569956 \n",
      "Epoch: 467   Validation Loss: 8.727861854220249 \n",
      "Epoch: 468   Training Loss: 8.952216334717614 \n",
      "Epoch: 468   Validation Loss: 8.721701229504323 \n",
      "Epoch: 469   Training Loss: 8.951453394329985 \n",
      "Epoch: 469   Validation Loss: 8.721484168715154 \n",
      "Epoch: 470   Training Loss: 8.945180317692737 \n",
      "Epoch: 470   Validation Loss: 8.72479359312574 \n",
      "Epoch: 471   Training Loss: 8.941637581183423 \n",
      "Epoch: 471   Validation Loss: 8.730178645423148 \n",
      "Epoch: 472   Training Loss: 8.939113788730518 \n",
      "Epoch: 472   Validation Loss: 8.726976471903436 \n",
      "Epoch: 473   Training Loss: 8.935200102126526 \n",
      "Epoch: 473   Validation Loss: 8.737195621413852 \n",
      "Epoch: 474   Training Loss: 8.931922236292174 \n",
      "Epoch: 474   Validation Loss: 8.727298736414768 \n",
      "Epoch: 475   Training Loss: 8.928294372816096 \n",
      "Epoch: 475   Validation Loss: 8.728020459993264 \n",
      "Epoch: 476   Training Loss: 8.953301241349166 \n",
      "Epoch: 476   Validation Loss: 8.719531324534522 \n",
      "Epoch: 477   Training Loss: 8.977040227666391 \n",
      "Epoch: 477   Validation Loss: 8.720340226211016 \n",
      "Epoch: 478   Training Loss: 8.975461959945282 \n",
      "Epoch: 478   Validation Loss: 8.720504177364521 \n",
      "Epoch: 479   Training Loss: 8.973398116621787 \n",
      "Epoch: 479   Validation Loss: 8.722831413412983 \n",
      "Epoch: 480   Training Loss: 8.970692558886098 \n",
      "Epoch: 480   Validation Loss: 8.723086192386898 \n",
      "Epoch: 481   Training Loss: 8.970809155704435 \n",
      "Epoch: 481   Validation Loss: 8.724365898488815 \n",
      "Epoch: 482   Training Loss: 8.970040548073593 \n",
      "Epoch: 482   Validation Loss: 8.724105289209168 \n",
      "Epoch: 483   Training Loss: 8.968680093908732 \n",
      "Epoch: 483   Validation Loss: 8.722584596076256 \n",
      "Epoch: 484   Training Loss: 8.964733079706777 \n",
      "Epoch: 484   Validation Loss: 8.725717771648792 \n",
      "Epoch: 485   Training Loss: 8.965359368718437 \n",
      "Epoch: 485   Validation Loss: 8.722163130977785 \n",
      "Epoch: 486   Training Loss: 8.961932542327304 \n",
      "Epoch: 486   Validation Loss: 8.724079633454966 \n",
      "Epoch: 487   Training Loss: 8.958092734751885 \n",
      "Epoch: 487   Validation Loss: 8.730953752051159 \n",
      "Epoch: 488   Training Loss: 8.956639186396652 \n",
      "Epoch: 488   Validation Loss: 8.733550602411338 \n",
      "Epoch: 489   Training Loss: 8.952678817694892 \n",
      "Epoch: 489   Validation Loss: 8.736132397935764 \n",
      "Epoch: 490   Training Loss: 8.950700156773815 \n",
      "Epoch: 490   Validation Loss: 8.737373128916758 \n",
      "Epoch: 491   Training Loss: 8.946140736921256 \n",
      "Epoch: 491   Validation Loss: 8.741705977640812 \n",
      "Epoch: 492   Training Loss: 8.944034354710743 \n",
      "Epoch: 492   Validation Loss: 8.744528946804973 \n",
      "Epoch: 493   Training Loss: 8.944281580280492 \n",
      "Epoch: 493   Validation Loss: 8.745342064569355 \n",
      "Epoch: 494   Training Loss: 8.935161629223298 \n",
      "Epoch: 494   Validation Loss: 8.742957381705782 \n",
      "Epoch: 495   Training Loss: 8.961866036745459 \n",
      "Epoch: 495   Validation Loss: 8.718919518410539 \n",
      "Epoch: 496   Training Loss: 8.930967546773793 \n",
      "Epoch: 496   Validation Loss: 8.748895395254678 \n",
      "Epoch: 497   Training Loss: 8.953531130372763 \n",
      "Epoch: 497   Validation Loss: 8.72095771675936 \n",
      "Epoch: 498   Training Loss: 8.977278159812307 \n",
      "Epoch: 498   Validation Loss: 8.721574029493974 \n",
      "Epoch: 499   Training Loss: 8.97627216808832 \n",
      "Epoch: 499   Validation Loss: 8.72361996967956 \n",
      "Epoch: 500   Training Loss: 8.975743514675138 \n",
      "Epoch: 500   Validation Loss: 8.72377432499919 \n",
      "Epoch: 501   Training Loss: 8.97482536058983 \n",
      "Epoch: 501   Validation Loss: 8.724711217643435 \n",
      "Epoch: 502   Training Loss: 8.971844815607609 \n",
      "Epoch: 502   Validation Loss: 8.723817783993669 \n",
      "Epoch: 503   Training Loss: 8.972364163501432 \n",
      "Epoch: 503   Validation Loss: 8.724193645454038 \n",
      "Epoch: 504   Training Loss: 8.974412322923923 \n",
      "Epoch: 504   Validation Loss: 8.721708251181576 \n",
      "Epoch: 505   Training Loss: 8.970577775073014 \n",
      "Epoch: 505   Validation Loss: 8.723697560366528 \n",
      "Epoch: 506   Training Loss: 8.967276785069112 \n",
      "Epoch: 506   Validation Loss: 8.72343869797963 \n",
      "Epoch: 507   Training Loss: 8.965598546812824 \n",
      "Epoch: 507   Validation Loss: 8.726710546046082 \n",
      "Epoch: 508   Training Loss: 8.965680118865041 \n",
      "Epoch: 508   Validation Loss: 8.72759632201648 \n",
      "Epoch: 509   Training Loss: 8.964942805294 \n",
      "Epoch: 509   Validation Loss: 8.727819029533253 \n",
      "Epoch: 510   Training Loss: 8.96536786380751 \n",
      "Epoch: 510   Validation Loss: 8.731174472184826 \n",
      "Epoch: 511   Training Loss: 8.961276289425832 \n",
      "Epoch: 511   Validation Loss: 8.730904157970127 \n",
      "Epoch: 512   Training Loss: 8.96254596394009 \n",
      "Epoch: 512   Validation Loss: 8.728494219858627 \n",
      "Epoch: 513   Training Loss: 8.969259852874702 \n",
      "Epoch: 513   Validation Loss: 8.723227226787506 \n",
      "Epoch: 514   Training Loss: 8.967980673089162 \n",
      "Epoch: 514   Validation Loss: 8.725561013273094 \n",
      "Epoch: 515   Training Loss: 8.9656517752819 \n",
      "Epoch: 515   Validation Loss: 8.723983468128447 \n",
      "Epoch: 516   Training Loss: 8.963634808284594 \n",
      "Epoch: 516   Validation Loss: 8.725222008156047 \n",
      "Epoch: 517   Training Loss: 8.960106222475002 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 517   Validation Loss: 8.725379034369615 \n",
      "Epoch: 518   Training Loss: 8.959587072209278 \n",
      "Epoch: 518   Validation Loss: 8.726532616308269 \n",
      "Epoch: 519   Training Loss: 8.955193111665244 \n",
      "Epoch: 519   Validation Loss: 8.729063471511719 \n",
      "Epoch: 520   Training Loss: 8.95195080623684 \n",
      "Epoch: 520   Validation Loss: 8.728506004549187 \n",
      "Epoch: 521   Training Loss: 8.947522322347364 \n",
      "Epoch: 521   Validation Loss: 8.72968468786807 \n",
      "Epoch: 522   Training Loss: 8.945876481588218 \n",
      "Epoch: 522   Validation Loss: 8.729061761036386 \n",
      "Epoch: 523   Training Loss: 8.956566475569634 \n",
      "Epoch: 523   Validation Loss: 8.729106492074969 \n",
      "Epoch: 524   Training Loss: 8.942866145753168 \n",
      "Epoch: 524   Validation Loss: 8.729067409675736 \n",
      "Epoch: 525   Training Loss: 8.955799275555838 \n",
      "Epoch: 525   Validation Loss: 8.727367305048343 \n",
      "Epoch: 526   Training Loss: 8.951884962399046 \n",
      "Epoch: 526   Validation Loss: 8.726324553536976 \n",
      "Epoch: 527   Training Loss: 8.950869360442093 \n",
      "Epoch: 527   Validation Loss: 8.719927362160007 \n",
      "Epoch: 528   Training Loss: 8.946741813165778 \n",
      "Epoch: 528   Validation Loss: 8.723613846948604 \n",
      "Epoch: 529   Training Loss: 8.943741762657323 \n",
      "Epoch: 529   Validation Loss: 8.72304536721435 \n",
      "Epoch: 530   Training Loss: 8.93471093865315 \n",
      "Epoch: 530   Validation Loss: 8.72428599767189 \n",
      "Epoch: 531   Training Loss: 8.941831446605722 \n",
      "Epoch: 531   Validation Loss: 8.72405983737267 \n",
      "Epoch: 532   Training Loss: 8.942817887928232 \n",
      "Epoch: 532   Validation Loss: 8.725158079487167 \n",
      "Epoch: 533   Training Loss: 8.936302416329015 \n",
      "Epoch: 533   Validation Loss: 8.726762877783116 \n",
      "Epoch: 534   Training Loss: 8.932783255751813 \n",
      "Epoch: 534   Validation Loss: 8.726893407748339 \n",
      "Epoch: 535   Training Loss: 8.94057193446164 \n",
      "Epoch: 535   Validation Loss: 8.72338328182741 \n",
      "Epoch: 536   Training Loss: 8.930440662447387 \n",
      "Epoch: 536   Validation Loss: 8.7307544991262 \n",
      "Epoch: 537   Training Loss: 8.931678406396173 \n",
      "Epoch: 537   Validation Loss: 8.725360237259748 \n",
      "Epoch: 538   Training Loss: 8.914778162814818 \n",
      "Epoch: 538   Validation Loss: 8.728480693206603 \n",
      "Epoch: 539   Training Loss: 8.907664446928594 \n",
      "Epoch: 539   Validation Loss: 8.74884745069751 \n",
      "Epoch: 540   Training Loss: 8.967602645237928 \n",
      "Epoch: 540   Validation Loss: 8.742004666250187 \n",
      "Epoch: 541   Training Loss: 8.972706338526358 \n",
      "Epoch: 541   Validation Loss: 8.727283752511573 \n",
      "Epoch: 542   Training Loss: 8.967524734745226 \n",
      "Epoch: 542   Validation Loss: 8.738913465632715 \n",
      "Epoch: 543   Training Loss: 8.96307619828857 \n",
      "Epoch: 543   Validation Loss: 8.73456055339001 \n",
      "Epoch: 544   Training Loss: 8.959881159253513 \n",
      "Epoch: 544   Validation Loss: 8.742644364502494 \n",
      "Epoch: 545   Training Loss: 8.956439710283819 \n",
      "Epoch: 545   Validation Loss: 8.742770035140314 \n",
      "Epoch: 546   Training Loss: 8.956791895557108 \n",
      "Epoch: 546   Validation Loss: 8.749186215566318 \n",
      "Epoch: 547   Training Loss: 8.949424425029644 \n",
      "Epoch: 547   Validation Loss: 8.761276819443104 \n",
      "Epoch: 548   Training Loss: 8.942410845692425 \n",
      "Epoch: 548   Validation Loss: 8.763007356740891 \n",
      "Epoch: 549   Training Loss: 8.938840049253795 \n",
      "Epoch: 549   Validation Loss: 8.776366630461306 \n",
      "Epoch: 550   Training Loss: 8.930768452455707 \n",
      "Epoch: 550   Validation Loss: 8.78613025580767 \n",
      "Epoch: 551   Training Loss: 8.93746016992652 \n",
      "Epoch: 551   Validation Loss: 8.76870237714281 \n",
      "Epoch: 552   Training Loss: 8.949806121765214 \n",
      "Epoch: 552   Validation Loss: 8.730904052348697 \n",
      "Epoch: 553   Training Loss: 8.95064775786678 \n",
      "Epoch: 553   Validation Loss: 8.732801096887295 \n",
      "Epoch: 554   Training Loss: 8.945319968819211 \n",
      "Epoch: 554   Validation Loss: 8.729432882981461 \n",
      "Epoch: 555   Training Loss: 8.94335371899519 \n",
      "Epoch: 555   Validation Loss: 8.73077416054055 \n",
      "Epoch: 556   Training Loss: 8.937294840478154 \n",
      "Epoch: 556   Validation Loss: 8.723715255442544 \n",
      "Epoch: 557   Training Loss: 8.932088108665225 \n",
      "Epoch: 557   Validation Loss: 8.722636214192756 \n",
      "Epoch: 558   Training Loss: 8.928963188465831 \n",
      "Epoch: 558   Validation Loss: 8.728606447791849 \n",
      "Epoch: 559   Training Loss: 8.921957020893302 \n",
      "Epoch: 559   Validation Loss: 8.730819680700634 \n",
      "Epoch: 560   Training Loss: 8.908493812253713 \n",
      "Epoch: 560   Validation Loss: 8.738930054165204 \n",
      "Epoch: 561   Training Loss: 8.894191914785743 \n",
      "Epoch: 561   Validation Loss: 8.73834333962737 \n",
      "Epoch: 562   Training Loss: 8.88777290548707 \n",
      "Epoch: 562   Validation Loss: 8.746568301100938 \n",
      "Epoch: 563   Training Loss: 8.97319512413542 \n",
      "Epoch: 563   Validation Loss: 8.7215068532274 \n",
      "Epoch: 564   Training Loss: 8.887991120875167 \n",
      "Epoch: 564   Validation Loss: 8.761405988183483 \n",
      "Epoch: 565   Training Loss: 8.903464475699101 \n",
      "Epoch: 565   Validation Loss: 8.754090344651226 \n",
      "Epoch: 566   Training Loss: 8.966293924731229 \n",
      "Epoch: 566   Validation Loss: 8.75665272988703 \n",
      "Epoch: 567   Training Loss: 8.887308599469115 \n",
      "Epoch: 567   Validation Loss: 8.767194142815423 \n",
      "Epoch: 568   Training Loss: 8.884417744212756 \n",
      "Epoch: 568   Validation Loss: 8.759944605064295 \n",
      "Epoch: 569   Training Loss: 8.912087124315514 \n",
      "Epoch: 569   Validation Loss: 8.722077632818824 \n",
      "Epoch: 570   Training Loss: 8.975562295632905 \n",
      "Epoch: 570   Validation Loss: 8.718447522213243 \n",
      "Epoch: 571   Training Loss: 8.971103467430735 \n",
      "Epoch: 571   Validation Loss: 8.721156016992266 \n",
      "Epoch: 572   Training Loss: 8.967651568989519 \n",
      "Epoch: 572   Validation Loss: 8.723861905813813 \n",
      "Epoch: 573   Training Loss: 8.964308652377918 \n",
      "Epoch: 573   Validation Loss: 8.72660642663643 \n",
      "Epoch: 574   Training Loss: 8.963246275289809 \n",
      "Epoch: 574   Validation Loss: 8.725370355432108 \n",
      "Epoch: 575   Training Loss: 8.960267789780138 \n",
      "Epoch: 575   Validation Loss: 8.731165417627773 \n",
      "Epoch: 576   Training Loss: 8.958310520972224 \n",
      "Epoch: 576   Validation Loss: 8.731561084346051 \n",
      "Epoch: 577   Training Loss: 8.959395263819204 \n",
      "Epoch: 577   Validation Loss: 8.736411073204026 \n",
      "Epoch: 578   Training Loss: 8.956975764462141 \n",
      "Epoch: 578   Validation Loss: 8.738126763944107 \n",
      "Epoch: 579   Training Loss: 8.954195994773475 \n",
      "Epoch: 579   Validation Loss: 8.73400006547123 \n",
      "Epoch: 580   Training Loss: 8.953229074198962 \n",
      "Epoch: 580   Validation Loss: 8.742768186829338 \n",
      "Epoch: 581   Training Loss: 8.951130594637577 \n",
      "Epoch: 581   Validation Loss: 8.751417678457381 \n",
      "Epoch: 582   Training Loss: 8.96031598069414 \n",
      "Epoch: 582   Validation Loss: 8.726462934425317 \n",
      "Epoch: 583   Training Loss: 8.956706151303097 \n",
      "Epoch: 583   Validation Loss: 8.739404833196735 \n",
      "Epoch: 584   Training Loss: 8.947630334562255 \n",
      "Epoch: 584   Validation Loss: 8.746278887319274 \n",
      "Epoch: 585   Training Loss: 8.939702371898528 \n",
      "Epoch: 585   Validation Loss: 8.746945758454425 \n",
      "Epoch: 586   Training Loss: 8.934594880676354 \n",
      "Epoch: 586   Validation Loss: 8.73838527537623 \n",
      "Epoch: 587   Training Loss: 8.966577007763826 \n",
      "Epoch: 587   Validation Loss: 8.729848679200273 \n",
      "Epoch: 588   Training Loss: 8.961597126828165 \n",
      "Epoch: 588   Validation Loss: 8.737109437152032 \n",
      "Epoch: 589   Training Loss: 8.953749972957004 \n",
      "Epoch: 589   Validation Loss: 8.730450238435584 \n",
      "Epoch: 590   Training Loss: 8.950894763099113 \n",
      "Epoch: 590   Validation Loss: 8.741425561008814 \n",
      "Epoch: 591   Training Loss: 8.942860484181558 \n",
      "Epoch: 591   Validation Loss: 8.722018339855829 \n",
      "Epoch: 592   Training Loss: 8.957731991190798 \n",
      "Epoch: 592   Validation Loss: 8.729712705321338 \n",
      "Epoch: 593   Training Loss: 8.952979112313361 \n",
      "Epoch: 593   Validation Loss: 8.732807987198195 \n",
      "Epoch: 594   Training Loss: 8.944346450206313 \n",
      "Epoch: 594   Validation Loss: 8.743225669737072 \n",
      "Epoch: 595   Training Loss: 8.936077592850669 \n",
      "Epoch: 595   Validation Loss: 8.754820258550396 \n",
      "Epoch: 596   Training Loss: 8.934247971450672 \n",
      "Epoch: 596   Validation Loss: 8.740752219396876 \n",
      "Epoch: 597   Training Loss: 8.925675005992588 \n",
      "Epoch: 597   Validation Loss: 8.747184759836639 \n",
      "Epoch: 598   Training Loss: 8.917325820287552 \n",
      "Epoch: 598   Validation Loss: 8.744020162473003 \n",
      "Epoch: 599   Training Loss: 8.905286514550902 \n",
      "Epoch: 599   Validation Loss: 8.741712690375525 \n",
      "Epoch: 600   Training Loss: 8.892907851481645 \n",
      "Epoch: 600   Validation Loss: 8.746888313810153 \n",
      "Epoch: 601   Training Loss: 8.879425592223017 \n",
      "Epoch: 601   Validation Loss: 8.753549037906648 \n",
      "Epoch: 602   Training Loss: 8.876924796493277 \n",
      "Epoch: 602   Validation Loss: 8.753650874968958 \n",
      "Epoch: 603   Training Loss: 8.860232311334865 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 603   Validation Loss: 8.753781889248083 \n",
      "Epoch: 604   Training Loss: 8.846838541169197 \n",
      "Epoch: 604   Validation Loss: 8.754795235908745 \n",
      "Epoch: 605   Training Loss: 8.84551049676172 \n",
      "Epoch: 605   Validation Loss: 8.74942390495101 \n",
      "Epoch: 606   Training Loss: 8.828560829173792 \n",
      "Epoch: 606   Validation Loss: 8.754681283268368 \n",
      "Epoch: 607   Training Loss: 8.935316992813036 \n",
      "Epoch: 607   Validation Loss: 8.71933160097359 \n",
      "Epoch: 608   Training Loss: 8.9702792369593 \n",
      "Epoch: 608   Validation Loss: 8.722853758213505 \n",
      "Epoch: 609   Training Loss: 8.963881970456146 \n",
      "Epoch: 609   Validation Loss: 8.728239702423 \n",
      "Epoch: 610   Training Loss: 8.95906976622301 \n",
      "Epoch: 610   Validation Loss: 8.7292130025766 \n",
      "Epoch: 611   Training Loss: 8.956063184958262 \n",
      "Epoch: 611   Validation Loss: 8.72267239482461 \n",
      "Epoch: 612   Training Loss: 8.952207510898033 \n",
      "Epoch: 612   Validation Loss: 8.724816275576725 \n",
      "Epoch: 613   Training Loss: 8.962085048444303 \n",
      "Epoch: 613   Validation Loss: 8.722909136776657 \n",
      "Epoch: 614   Training Loss: 8.968909637940216 \n",
      "Epoch: 614   Validation Loss: 8.725742100867045 \n",
      "Epoch: 615   Training Loss: 8.94691832819374 \n",
      "Epoch: 615   Validation Loss: 8.728803324519252 \n",
      "Epoch: 616   Training Loss: 8.945203090404108 \n",
      "Epoch: 616   Validation Loss: 8.730870255598035 \n",
      "Epoch: 617   Training Loss: 8.932894885970997 \n",
      "Epoch: 617   Validation Loss: 8.726696760975543 \n",
      "Epoch: 618   Training Loss: 8.972770898542858 \n",
      "Epoch: 618   Validation Loss: 8.716963151413042 \n",
      "Epoch: 619   Training Loss: 8.970598262038399 \n",
      "Epoch: 619   Validation Loss: 8.725747752635403 \n",
      "Epoch: 620   Training Loss: 8.944432538498678 \n",
      "Epoch: 620   Validation Loss: 8.735837726693095 \n",
      "Epoch: 621   Training Loss: 8.933565685112818 \n",
      "Epoch: 621   Validation Loss: 8.736177489097663 \n",
      "Epoch: 622   Training Loss: 8.932572929661916 \n",
      "Epoch: 622   Validation Loss: 8.739946784310698 \n",
      "Epoch: 623   Training Loss: 8.968523398008633 \n",
      "Epoch: 623   Validation Loss: 8.722376544263563 \n",
      "Epoch: 624   Training Loss: 8.963260743743572 \n",
      "Epoch: 624   Validation Loss: 8.737066844426325 \n",
      "Epoch: 625   Training Loss: 8.94088428860347 \n",
      "Epoch: 625   Validation Loss: 8.74790552548151 \n",
      "Epoch: 626   Training Loss: 8.929489972771211 \n",
      "Epoch: 626   Validation Loss: 8.74509190053907 \n",
      "Epoch: 627   Training Loss: 8.926195587758633 \n",
      "Epoch: 627   Validation Loss: 8.754231713373324 \n",
      "Epoch: 628   Training Loss: 8.91285586635035 \n",
      "Epoch: 628   Validation Loss: 8.758234235249379 \n",
      "Epoch: 629   Training Loss: 8.905129513365228 \n",
      "Epoch: 629   Validation Loss: 8.758940957705958 \n",
      "Epoch: 630   Training Loss: 8.894763683115398 \n",
      "Epoch: 630   Validation Loss: 8.777378458546469 \n",
      "Epoch: 631   Training Loss: 8.911339755300224 \n",
      "Epoch: 631   Validation Loss: 8.771548491729678 \n",
      "Epoch: 632   Training Loss: 8.892803663639725 \n",
      "Epoch: 632   Validation Loss: 8.755181684057584 \n",
      "Epoch: 633   Training Loss: 8.962469868755623 \n",
      "Epoch: 633   Validation Loss: 8.72229460270593 \n",
      "Epoch: 634   Training Loss: 8.940100951391155 \n",
      "Epoch: 634   Validation Loss: 8.785125473454766 \n",
      "Epoch: 635   Training Loss: 8.88149476791946 \n",
      "Epoch: 635   Validation Loss: 8.780750059295794 \n",
      "Epoch: 636   Training Loss: 8.963012169155073 \n",
      "Epoch: 636   Validation Loss: 8.719018699794724 \n",
      "Epoch: 637   Training Loss: 8.974281797092498 \n",
      "Epoch: 637   Validation Loss: 8.721769430572115 \n",
      "Epoch: 638   Training Loss: 8.970785018086298 \n",
      "Epoch: 638   Validation Loss: 8.722942362015239 \n",
      "Epoch: 639   Training Loss: 8.967310242496705 \n",
      "Epoch: 639   Validation Loss: 8.729134643882102 \n",
      "Epoch: 640   Training Loss: 8.964378663380245 \n",
      "Epoch: 640   Validation Loss: 8.736598580697153 \n",
      "Epoch: 641   Training Loss: 8.958690198627183 \n",
      "Epoch: 641   Validation Loss: 8.73717980100945 \n",
      "Epoch: 642   Training Loss: 8.954416873464794 \n",
      "Epoch: 642   Validation Loss: 8.727915176655715 \n",
      "Epoch: 643   Training Loss: 8.949957699936864 \n",
      "Epoch: 643   Validation Loss: 8.73431669416765 \n",
      "Epoch: 644   Training Loss: 8.943531982353424 \n",
      "Epoch: 644   Validation Loss: 8.729095804438561 \n",
      "Epoch: 645   Training Loss: 8.938026300141948 \n",
      "Epoch: 645   Validation Loss: 8.731911711255977 \n",
      "Epoch: 646   Training Loss: 8.937626680129732 \n",
      "Epoch: 646   Validation Loss: 8.729495932356874 \n",
      "Epoch: 647   Training Loss: 8.931652201939032 \n",
      "Epoch: 647   Validation Loss: 8.73236751491393 \n",
      "Epoch: 648   Training Loss: 8.93554426087969 \n",
      "Epoch: 648   Validation Loss: 8.733766708271155 \n",
      "Epoch: 649   Training Loss: 8.918103088645582 \n",
      "Epoch: 649   Validation Loss: 8.74027728787459 \n",
      "Epoch: 650   Training Loss: 8.942399412551273 \n",
      "Epoch: 650   Validation Loss: 8.718819086757163 \n",
      "Epoch: 651   Training Loss: 8.97719694591665 \n",
      "Epoch: 651   Validation Loss: 8.718194810112173 \n",
      "Epoch: 652   Training Loss: 8.976219807313232 \n",
      "Epoch: 652   Validation Loss: 8.719338983810038 \n",
      "Epoch: 653   Training Loss: 8.973304961879586 \n",
      "Epoch: 653   Validation Loss: 8.719664324128754 \n",
      "Epoch: 654   Training Loss: 8.971192112445818 \n",
      "Epoch: 654   Validation Loss: 8.720338327217458 \n",
      "Epoch: 655   Training Loss: 8.96766436043266 \n",
      "Epoch: 655   Validation Loss: 8.721307203627735 \n",
      "Epoch: 656   Training Loss: 8.966469084386352 \n",
      "Epoch: 656   Validation Loss: 8.724076608135455 \n",
      "Epoch: 657   Training Loss: 8.968145720868607 \n",
      "Epoch: 657   Validation Loss: 8.71904704867619 \n",
      "Epoch: 658   Training Loss: 8.966374466450318 \n",
      "Epoch: 658   Validation Loss: 8.716893174906527 \n",
      "Epoch: 659   Training Loss: 8.956807071081826 \n",
      "Epoch: 659   Validation Loss: 8.71929780104683 \n",
      "Epoch: 660   Training Loss: 8.946855948024863 \n",
      "Epoch: 660   Validation Loss: 8.716810941376412 \n",
      "Epoch: 661   Training Loss: 8.945269321924906 \n",
      "Epoch: 661   Validation Loss: 8.720879245216972 \n",
      "Epoch: 662   Training Loss: 8.935951188884193 \n",
      "Epoch: 662   Validation Loss: 8.725503975397322 \n",
      "Epoch: 663   Training Loss: 8.927509077949948 \n",
      "Epoch: 663   Validation Loss: 8.731720560975901 \n",
      "Epoch: 664   Training Loss: 8.9139582196918 \n",
      "Epoch: 664   Validation Loss: 8.73172263623561 \n",
      "Epoch: 665   Training Loss: 8.902082024845955 \n",
      "Epoch: 665   Validation Loss: 8.734136663757367 \n",
      "Epoch: 666   Training Loss: 8.90972418407257 \n",
      "Epoch: 666   Validation Loss: 8.731503453374293 \n",
      "Epoch: 667   Training Loss: 8.917056112480825 \n",
      "Epoch: 667   Validation Loss: 8.7325515930738 \n",
      "Epoch: 668   Training Loss: 8.924654848577356 \n",
      "Epoch: 668   Validation Loss: 8.72056395486801 \n",
      "Epoch: 669   Training Loss: 8.945331735294276 \n",
      "Epoch: 669   Validation Loss: 8.726034191460995 \n",
      "Epoch: 670   Training Loss: 8.936856620478629 \n",
      "Epoch: 670   Validation Loss: 8.72921849070047 \n",
      "Epoch: 671   Training Loss: 8.933914146616331 \n",
      "Epoch: 671   Validation Loss: 8.734312306009567 \n",
      "Epoch: 672   Training Loss: 8.913208284376571 \n",
      "Epoch: 672   Validation Loss: 8.736118403566918 \n",
      "Epoch: 673   Training Loss: 8.918349936433103 \n",
      "Epoch: 673   Validation Loss: 8.739559481293389 \n",
      "Epoch: 674   Training Loss: 8.898505131504418 \n",
      "Epoch: 674   Validation Loss: 8.73023406783362 \n",
      "Epoch: 675   Training Loss: 8.88464532636125 \n",
      "Epoch: 675   Validation Loss: 8.73726026896289 \n",
      "Epoch: 676   Training Loss: 8.875419053423558 \n",
      "Epoch: 676   Validation Loss: 8.748372934837962 \n",
      "Epoch: 677   Training Loss: 8.898056575751749 \n",
      "Epoch: 677   Validation Loss: 8.73451683048155 \n",
      "Epoch: 678   Training Loss: 8.88885183418809 \n",
      "Epoch: 678   Validation Loss: 8.73900878933614 \n",
      "Epoch: 679   Training Loss: 8.862359464376139 \n",
      "Epoch: 679   Validation Loss: 8.734545750796558 \n",
      "Epoch: 680   Training Loss: 8.873904482184761 \n",
      "Epoch: 680   Validation Loss: 8.735173482145019 \n",
      "Epoch: 681   Training Loss: 8.93714177326897 \n",
      "Epoch: 681   Validation Loss: 8.721096970354143 \n",
      "Epoch: 682   Training Loss: 8.943051964869634 \n",
      "Epoch: 682   Validation Loss: 8.725832916919375 \n",
      "Epoch: 683   Training Loss: 8.936415047623711 \n",
      "Epoch: 683   Validation Loss: 8.726153749006038 \n",
      "Epoch: 684   Training Loss: 8.92236232340239 \n",
      "Epoch: 684   Validation Loss: 8.732301074779624 \n",
      "Epoch: 685   Training Loss: 8.914102836588802 \n",
      "Epoch: 685   Validation Loss: 8.740928085990387 \n",
      "Epoch: 686   Training Loss: 8.909385078111548 \n",
      "Epoch: 686   Validation Loss: 8.730171951584033 \n",
      "Epoch: 687   Training Loss: 8.90673539748029 \n",
      "Epoch: 687   Validation Loss: 8.718068301246463 \n",
      "Epoch: 688   Training Loss: 8.975855238544389 \n",
      "Epoch: 688   Validation Loss: 8.720130687733565 \n",
      "Epoch: 689   Training Loss: 8.973612869545274 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 689   Validation Loss: 8.722062686590029 \n",
      "Epoch: 690   Training Loss: 8.9693762349682 \n",
      "Epoch: 690   Validation Loss: 8.724586628561504 \n",
      "Epoch: 691   Training Loss: 8.967065085859783 \n",
      "Epoch: 691   Validation Loss: 8.722351984514466 \n",
      "Epoch: 692   Training Loss: 8.963497950842209 \n",
      "Epoch: 692   Validation Loss: 8.72541066507853 \n",
      "Epoch: 693   Training Loss: 8.957126201770537 \n",
      "Epoch: 693   Validation Loss: 8.727895842977807 \n",
      "Epoch: 694   Training Loss: 8.953494436816833 \n",
      "Epoch: 694   Validation Loss: 8.73009993499306 \n",
      "Epoch: 695   Training Loss: 8.953780636921175 \n",
      "Epoch: 695   Validation Loss: 8.735941375808688 \n",
      "Epoch: 696   Training Loss: 8.945428457214723 \n",
      "Epoch: 696   Validation Loss: 8.727665818349957 \n",
      "Epoch: 697   Training Loss: 8.941174303758519 \n",
      "Epoch: 697   Validation Loss: 8.721646296160337 \n",
      "Epoch: 698   Training Loss: 8.964925942028403 \n",
      "Epoch: 698   Validation Loss: 8.723220464513442 \n",
      "Epoch: 699   Training Loss: 8.958122871134803 \n",
      "Epoch: 699   Validation Loss: 8.724984785142341 \n",
      "Epoch: 700   Training Loss: 8.94977509433995 \n",
      "Epoch: 700   Validation Loss: 8.733335139234638 \n",
      "Epoch: 701   Training Loss: 8.962243658457403 \n",
      "Epoch: 701   Validation Loss: 8.720276985542657 \n",
      "Epoch: 702   Training Loss: 8.963539493309844 \n",
      "Epoch: 702   Validation Loss: 8.72977113322656 \n",
      "Epoch: 703   Training Loss: 8.943626294931692 \n",
      "Epoch: 703   Validation Loss: 8.732060571169558 \n",
      "Epoch: 704   Training Loss: 8.935463952588869 \n",
      "Epoch: 704   Validation Loss: 8.735273488966554 \n",
      "Epoch: 705   Training Loss: 8.920105717729701 \n",
      "Epoch: 705   Validation Loss: 8.737557333568903 \n",
      "Epoch: 706   Training Loss: 8.903397563711415 \n",
      "Epoch: 706   Validation Loss: 8.74550889405307 \n",
      "Epoch: 707   Training Loss: 8.893154015054256 \n",
      "Epoch: 707   Validation Loss: 8.751012537703634 \n",
      "Epoch: 708   Training Loss: 8.92822819143232 \n",
      "Epoch: 708   Validation Loss: 8.748132169910637 \n",
      "Epoch: 709   Training Loss: 8.872219560505126 \n",
      "Epoch: 709   Validation Loss: 8.726188450249603 \n",
      "Epoch: 710   Training Loss: 8.935671695555087 \n",
      "Epoch: 710   Validation Loss: 8.736556491265048 \n",
      "Epoch: 711   Training Loss: 8.926834781445038 \n",
      "Epoch: 711   Validation Loss: 8.737718294123104 \n",
      "Epoch: 712   Training Loss: 8.906843578091477 \n",
      "Epoch: 712   Validation Loss: 8.746840240443726 \n",
      "Epoch: 713   Training Loss: 8.961627278781922 \n",
      "Epoch: 713   Validation Loss: 8.720915510708698 \n",
      "Epoch: 714   Training Loss: 8.977537235208146 \n",
      "Epoch: 714   Validation Loss: 8.724562323211188 \n",
      "Epoch: 715   Training Loss: 8.973072659861957 \n",
      "Epoch: 715   Validation Loss: 8.726955681698112 \n",
      "Epoch: 716   Training Loss: 8.971739721998112 \n",
      "Epoch: 716   Validation Loss: 8.729126416014534 \n",
      "Epoch: 717   Training Loss: 8.966790170434544 \n",
      "Epoch: 717   Validation Loss: 8.729187663793052 \n",
      "Epoch: 718   Training Loss: 8.964556453666665 \n",
      "Epoch: 718   Validation Loss: 8.729395589499537 \n",
      "Epoch: 719   Training Loss: 8.959079894215485 \n",
      "Epoch: 719   Validation Loss: 8.734301198398695 \n",
      "Epoch: 720   Training Loss: 8.953576143102632 \n",
      "Epoch: 720   Validation Loss: 8.739178313825363 \n",
      "Epoch: 721   Training Loss: 8.951634215419507 \n",
      "Epoch: 721   Validation Loss: 8.741638535384428 \n",
      "Epoch: 722   Training Loss: 8.95164506720349 \n",
      "Epoch: 722   Validation Loss: 8.725968958391265 \n",
      "Epoch: 723   Training Loss: 8.972024336872467 \n",
      "Epoch: 723   Validation Loss: 8.727912818167967 \n",
      "Epoch: 724   Training Loss: 8.96638666898075 \n",
      "Epoch: 724   Validation Loss: 8.727627895903606 \n",
      "Epoch: 725   Training Loss: 8.960172840886235 \n",
      "Epoch: 725   Validation Loss: 8.736168405262173 \n",
      "Epoch: 726   Training Loss: 8.955078671866067 \n",
      "Epoch: 726   Validation Loss: 8.751214874327642 \n",
      "Epoch: 727   Training Loss: 8.952038828501667 \n",
      "Epoch: 727   Validation Loss: 8.752969426084913 \n",
      "Epoch: 728   Training Loss: 8.945248534046863 \n",
      "Epoch: 728   Validation Loss: 8.754302634511363 \n",
      "Epoch: 729   Training Loss: 8.939572484397868 \n",
      "Epoch: 729   Validation Loss: 8.758467054614837 \n",
      "Epoch: 730   Training Loss: 8.931324125059238 \n",
      "Epoch: 730   Validation Loss: 8.768035012237426 \n",
      "Epoch: 731   Training Loss: 8.926166471389738 \n",
      "Epoch: 731   Validation Loss: 8.765135864175235 \n",
      "Epoch: 732   Training Loss: 8.918929652544318 \n",
      "Epoch: 732   Validation Loss: 8.777398799057249 \n",
      "Epoch: 733   Training Loss: 8.905360808443902 \n",
      "Epoch: 733   Validation Loss: 8.789060315847351 \n",
      "Epoch: 734   Training Loss: 8.90009869542355 \n",
      "Epoch: 734   Validation Loss: 8.802348410949033 \n",
      "Epoch: 735   Training Loss: 8.887131845152574 \n",
      "Epoch: 735   Validation Loss: 8.81851855135123 \n",
      "Epoch: 736   Training Loss: 8.871386174077594 \n",
      "Epoch: 736   Validation Loss: 8.842385485555399 \n",
      "Epoch: 737   Training Loss: 8.955288309486859 \n",
      "Epoch: 737   Validation Loss: 8.718820583950949 \n",
      "Epoch: 738   Training Loss: 8.975847617989123 \n",
      "Epoch: 738   Validation Loss: 8.719397061420903 \n",
      "Epoch: 739   Training Loss: 8.97105795340568 \n",
      "Epoch: 739   Validation Loss: 8.72468538305902 \n",
      "Epoch: 740   Training Loss: 8.967728094504615 \n",
      "Epoch: 740   Validation Loss: 8.725013089759528 \n",
      "Epoch: 741   Training Loss: 8.890997998981804 \n",
      "Epoch: 741   Validation Loss: 8.916723259666027 \n",
      "Epoch: 742   Training Loss: 8.935350997885486 \n",
      "Epoch: 742   Validation Loss: 8.76551860416345 \n",
      "Epoch: 743   Training Loss: 8.856191500304964 \n",
      "Epoch: 743   Validation Loss: 8.912178329865371 \n",
      "Epoch: 744   Training Loss: 8.82812052086839 \n",
      "Epoch: 744   Validation Loss: 8.979032502082248 \n",
      "Epoch: 745   Training Loss: 8.800904251293085 \n",
      "Epoch: 745   Validation Loss: 8.945615883501006 \n",
      "Epoch: 746   Training Loss: 8.7870140931835 \n",
      "Epoch: 746   Validation Loss: 9.03792766903431 \n",
      "Epoch: 747   Training Loss: 8.766761629621916 \n",
      "Epoch: 747   Validation Loss: 9.089352115205918 \n",
      "Epoch: 748   Training Loss: 8.72827472381063 \n",
      "Epoch: 748   Validation Loss: 9.068078587259636 \n",
      "Epoch: 749   Training Loss: 8.732289402192274 \n",
      "Epoch: 749   Validation Loss: 9.131470081120925 \n",
      "Epoch: 750   Training Loss: 8.712494061535292 \n",
      "Epoch: 750   Validation Loss: 9.15064813393243 \n",
      "Epoch: 751   Training Loss: 8.75904830467812 \n",
      "Epoch: 751   Validation Loss: 8.740828038814218 \n",
      "Epoch: 752   Training Loss: 8.97713113470513 \n",
      "Epoch: 752   Validation Loss: 8.722519068644505 \n",
      "Epoch: 753   Training Loss: 8.971166311984824 \n",
      "Epoch: 753   Validation Loss: 8.74574767803318 \n",
      "Epoch: 754   Training Loss: 8.967659905546356 \n",
      "Epoch: 754   Validation Loss: 8.73015625386806 \n",
      "Epoch: 755   Training Loss: 8.96048132632752 \n",
      "Epoch: 755   Validation Loss: 8.729351158732953 \n",
      "Epoch: 756   Training Loss: 8.954467505841668 \n",
      "Epoch: 756   Validation Loss: 8.728477781374403 \n",
      "Epoch: 757   Training Loss: 8.945422970103177 \n",
      "Epoch: 757   Validation Loss: 8.734074582096403 \n",
      "Epoch: 758   Training Loss: 8.933960419932864 \n",
      "Epoch: 758   Validation Loss: 8.735303525607389 \n",
      "Epoch: 759   Training Loss: 8.924292572791309 \n",
      "Epoch: 759   Validation Loss: 8.74112639539182 \n",
      "Epoch: 760   Training Loss: 8.917982147386148 \n",
      "Epoch: 760   Validation Loss: 8.751052121443143 \n",
      "Epoch: 761   Training Loss: 8.905584546905981 \n",
      "Epoch: 761   Validation Loss: 8.760617603377732 \n",
      "Epoch: 762   Training Loss: 8.886373841058814 \n",
      "Epoch: 762   Validation Loss: 8.772642539689055 \n",
      "Epoch: 763   Training Loss: 8.863927334583794 \n",
      "Epoch: 763   Validation Loss: 8.78024825123908 \n",
      "Epoch: 764   Training Loss: 8.854834517356256 \n",
      "Epoch: 764   Validation Loss: 8.821867856803038 \n",
      "Epoch: 765   Training Loss: 8.823693105757872 \n",
      "Epoch: 765   Validation Loss: 8.775749676938986 \n",
      "Epoch: 766   Training Loss: 8.79313303529126 \n",
      "Epoch: 766   Validation Loss: 8.78864354185479 \n",
      "Epoch: 767   Training Loss: 8.778572831363022 \n",
      "Epoch: 767   Validation Loss: 8.792455619768202 \n",
      "Epoch: 768   Training Loss: 8.731836978429477 \n",
      "Epoch: 768   Validation Loss: 8.809806860932067 \n",
      "Epoch: 769   Training Loss: 8.696428262131738 \n",
      "Epoch: 769   Validation Loss: 8.825957479937358 \n",
      "Epoch: 770   Training Loss: 8.679545634201313 \n",
      "Epoch: 770   Validation Loss: 8.856058666490375 \n",
      "Epoch: 771   Training Loss: 8.854376171821121 \n",
      "Epoch: 771   Validation Loss: 8.727405496566515 \n",
      "Epoch: 772   Training Loss: 8.970768280646949 \n",
      "Epoch: 772   Validation Loss: 8.727675979526627 \n",
      "Epoch: 773   Training Loss: 8.966087669092856 \n",
      "Epoch: 773   Validation Loss: 8.724643127093778 \n",
      "Epoch: 774   Training Loss: 8.959094862164111 \n",
      "Epoch: 774   Validation Loss: 8.73597161758258 \n",
      "Epoch: 775   Training Loss: 8.845774042668648 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 775   Validation Loss: 8.726481136456307 \n",
      "Epoch: 776   Training Loss: 8.964703515597133 \n",
      "Epoch: 776   Validation Loss: 8.732400385558789 \n",
      "Epoch: 777   Training Loss: 8.961528243600036 \n",
      "Epoch: 777   Validation Loss: 8.74195577854117 \n",
      "Epoch: 778   Training Loss: 8.949440297472002 \n",
      "Epoch: 778   Validation Loss: 8.755990929274716 \n",
      "Epoch: 779   Training Loss: 8.935953386429805 \n",
      "Epoch: 779   Validation Loss: 8.781454235784453 \n",
      "Epoch: 780   Training Loss: 8.925308527291563 \n",
      "Epoch: 780   Validation Loss: 8.818936285221891 \n",
      "Epoch: 781   Training Loss: 8.907393200276285 \n",
      "Epoch: 781   Validation Loss: 8.885113915265054 \n",
      "Epoch: 782   Training Loss: 8.885106747735808 \n",
      "Epoch: 782   Validation Loss: 8.896632242636214 \n",
      "Epoch: 783   Training Loss: 8.855309682683643 \n",
      "Epoch: 783   Validation Loss: 8.965704152385996 \n",
      "Epoch: 784   Training Loss: 8.836752152787506 \n",
      "Epoch: 784   Validation Loss: 8.981008513840377 \n",
      "Epoch: 785   Training Loss: 8.916403245608464 \n",
      "Epoch: 785   Validation Loss: 9.094991880438961 \n",
      "Epoch: 786   Training Loss: 8.84902868667503 \n",
      "Epoch: 786   Validation Loss: 9.146342230277918 \n",
      "Epoch: 787   Training Loss: 8.8410688308597 \n",
      "Epoch: 787   Validation Loss: 9.279044880901235 \n",
      "Epoch: 788   Training Loss: 8.798115865605157 \n",
      "Epoch: 788   Validation Loss: 9.293046557989381 \n",
      "Epoch: 789   Training Loss: 8.760700475971946 \n",
      "Epoch: 789   Validation Loss: 9.419645792125483 \n",
      "Epoch: 790   Training Loss: 8.733381803561869 \n",
      "Epoch: 790   Validation Loss: 9.528217864133346 \n",
      "Epoch: 791   Training Loss: 8.740499738977661 \n",
      "Epoch: 791   Validation Loss: 9.560640405751093 \n",
      "Epoch: 792   Training Loss: 8.703206653693995 \n",
      "Epoch: 792   Validation Loss: 9.428956524020554 \n",
      "Epoch: 793   Training Loss: 8.661950083082301 \n",
      "Epoch: 793   Validation Loss: 9.6163678138747 \n",
      "Epoch: 794   Training Loss: 8.647031374609242 \n",
      "Epoch: 794   Validation Loss: 9.040684620623523 \n",
      "Epoch: 795   Training Loss: 8.988267685953188 \n",
      "Epoch: 795   Validation Loss: 8.71909013616454 \n",
      "Epoch: 796   Training Loss: 8.975975436906431 \n",
      "Epoch: 796   Validation Loss: 8.72731104404185 \n",
      "Epoch: 797   Training Loss: 8.970023698104852 \n",
      "Epoch: 797   Validation Loss: 8.724331198452992 \n",
      "Epoch: 798   Training Loss: 8.970578469003492 \n",
      "Epoch: 798   Validation Loss: 8.722137846861267 \n",
      "Epoch: 799   Training Loss: 8.964927302485567 \n",
      "Epoch: 799   Validation Loss: 8.731544883988436 \n",
      "Epoch: 800   Training Loss: 8.956985683725163 \n",
      "Epoch: 800   Validation Loss: 8.737666187235686 \n",
      "Epoch: 801   Training Loss: 8.952686431783558 \n",
      "Epoch: 801   Validation Loss: 8.744015638992149 \n",
      "Epoch: 802   Training Loss: 8.94645185166142 \n",
      "Epoch: 802   Validation Loss: 8.75401742582365 \n",
      "Epoch: 803   Training Loss: 8.933546853790356 \n",
      "Epoch: 803   Validation Loss: 8.760475474052464 \n",
      "Epoch: 804   Training Loss: 8.925211037105912 \n",
      "Epoch: 804   Validation Loss: 8.785288116858172 \n",
      "Epoch: 805   Training Loss: 8.911907776359078 \n",
      "Epoch: 805   Validation Loss: 8.811850314168941 \n",
      "Epoch: 806   Training Loss: 8.89074294077903 \n",
      "Epoch: 806   Validation Loss: 8.86538096067851 \n",
      "Epoch: 807   Training Loss: 8.869009157170554 \n",
      "Epoch: 807   Validation Loss: 8.899118839589958 \n",
      "Epoch: 808   Training Loss: 8.846586047055183 \n",
      "Epoch: 808   Validation Loss: 8.950022960794318 \n",
      "Epoch: 809   Training Loss: 8.822577722272198 \n",
      "Epoch: 809   Validation Loss: 9.040094869874702 \n",
      "Epoch: 810   Training Loss: 8.80828483453967 \n",
      "Epoch: 810   Validation Loss: 9.048367800392576 \n",
      "Epoch: 811   Training Loss: 8.779233651442318 \n",
      "Epoch: 811   Validation Loss: 9.145834948289874 \n",
      "Epoch: 812   Training Loss: 8.757027623241946 \n",
      "Epoch: 812   Validation Loss: 9.257821978951718 \n",
      "Epoch: 813   Training Loss: 8.697010953873322 \n",
      "Epoch: 813   Validation Loss: 9.2450353023394 \n",
      "Epoch: 814   Training Loss: 8.65775429155849 \n",
      "Epoch: 814   Validation Loss: 9.367722715643493 \n",
      "Epoch: 815   Training Loss: 8.60812033777239 \n",
      "Epoch: 815   Validation Loss: 9.579773520870173 \n",
      "Epoch: 816   Training Loss: 8.631258213310343 \n",
      "Epoch: 816   Validation Loss: 9.505182231446806 \n",
      "Epoch: 817   Training Loss: 8.582341088725363 \n",
      "Epoch: 817   Validation Loss: 9.571804202498923 \n",
      "Epoch: 818   Training Loss: 8.69065793111258 \n",
      "Epoch: 818   Validation Loss: 8.75198351110496 \n",
      "Epoch: 819   Training Loss: 8.980994254159363 \n",
      "Epoch: 819   Validation Loss: 8.721354477347225 \n",
      "Epoch: 820   Training Loss: 8.973020495776309 \n",
      "Epoch: 820   Validation Loss: 8.720082980148659 \n",
      "Epoch: 821   Training Loss: 8.966421657116019 \n",
      "Epoch: 821   Validation Loss: 8.728467934165392 \n",
      "Epoch: 822   Training Loss: 8.966543054756507 \n",
      "Epoch: 822   Validation Loss: 8.734160140635959 \n",
      "Epoch: 823   Training Loss: 8.957680805148977 \n",
      "Epoch: 823   Validation Loss: 8.745249373726919 \n",
      "Epoch: 824   Training Loss: 8.952965020200187 \n",
      "Epoch: 824   Validation Loss: 8.733048925050939 \n",
      "Epoch: 825   Training Loss: 8.93890115074164 \n",
      "Epoch: 825   Validation Loss: 8.752715317271155 \n",
      "Epoch: 826   Training Loss: 8.927835975891531 \n",
      "Epoch: 826   Validation Loss: 8.777822270092232 \n",
      "Epoch: 827   Training Loss: 8.916980168519885 \n",
      "Epoch: 827   Validation Loss: 8.809059371316831 \n",
      "Epoch: 828   Training Loss: 8.894247332744305 \n",
      "Epoch: 828   Validation Loss: 8.826657201208747 \n",
      "Epoch: 829   Training Loss: 8.88016186338 \n",
      "Epoch: 829   Validation Loss: 8.82732969690548 \n",
      "Epoch: 830   Training Loss: 8.85508462656714 \n",
      "Epoch: 830   Validation Loss: 8.886190280070593 \n",
      "Epoch: 831   Training Loss: 8.83521707406429 \n",
      "Epoch: 831   Validation Loss: 8.96904707612213 \n",
      "Epoch: 832   Training Loss: 8.91111405175275 \n",
      "Epoch: 832   Validation Loss: 8.724614378329033 \n",
      "Epoch: 833   Training Loss: 8.978990709950333 \n",
      "Epoch: 833   Validation Loss: 8.72156856383955 \n",
      "Epoch: 834   Training Loss: 8.975899001567182 \n",
      "Epoch: 834   Validation Loss: 8.722578622223292 \n",
      "Epoch: 835   Training Loss: 8.970764726926236 \n",
      "Epoch: 835   Validation Loss: 8.728354961785795 \n",
      "Epoch: 836   Training Loss: 8.964856711220786 \n",
      "Epoch: 836   Validation Loss: 8.73331099053462 \n",
      "Epoch: 837   Training Loss: 8.956816267670321 \n",
      "Epoch: 837   Validation Loss: 8.740915225205741 \n",
      "Epoch: 838   Training Loss: 8.949174166788678 \n",
      "Epoch: 838   Validation Loss: 8.735190451632617 \n",
      "Epoch: 839   Training Loss: 8.939478058204825 \n",
      "Epoch: 839   Validation Loss: 8.73757667101665 \n",
      "Epoch: 840   Training Loss: 8.929865341881756 \n",
      "Epoch: 840   Validation Loss: 8.748798894782539 \n",
      "Epoch: 841   Training Loss: 8.919610492522606 \n",
      "Epoch: 841   Validation Loss: 8.780113007525003 \n",
      "Epoch: 842   Training Loss: 8.903879374889383 \n",
      "Epoch: 842   Validation Loss: 8.806821549497123 \n",
      "Epoch: 843   Training Loss: 8.885666656435761 \n",
      "Epoch: 843   Validation Loss: 8.856097894063183 \n",
      "Epoch: 844   Training Loss: 8.865998383611954 \n",
      "Epoch: 844   Validation Loss: 8.843009606666099 \n",
      "Epoch: 845   Training Loss: 8.852954887261495 \n",
      "Epoch: 845   Validation Loss: 8.830372948240305 \n",
      "Epoch: 846   Training Loss: 8.827350933117586 \n",
      "Epoch: 846   Validation Loss: 8.868502637996157 \n",
      "Epoch: 847   Training Loss: 8.810487335322222 \n",
      "Epoch: 847   Validation Loss: 8.88695406446102 \n",
      "Epoch: 848   Training Loss: 8.78437257917996 \n",
      "Epoch: 848   Validation Loss: 9.007249702588819 \n",
      "Epoch: 849   Training Loss: 8.73410460530357 \n",
      "Epoch: 849   Validation Loss: 9.027277099324312 \n",
      "Epoch: 850   Training Loss: 8.941059749257846 \n",
      "Epoch: 850   Validation Loss: 8.780955541104648 \n",
      "Epoch: 851   Training Loss: 8.683974021882563 \n",
      "Epoch: 851   Validation Loss: 9.162934778648163 \n",
      "Epoch: 852   Training Loss: 8.919583277271727 \n",
      "Epoch: 852   Validation Loss: 8.718445807301777 \n",
      "Epoch: 853   Training Loss: 8.976460906247146 \n",
      "Epoch: 853   Validation Loss: 8.722169295701717 \n",
      "Epoch: 854   Training Loss: 8.970754976732907 \n",
      "Epoch: 854   Validation Loss: 8.726975196391667 \n",
      "Epoch: 855   Training Loss: 8.951231005353472 \n",
      "Epoch: 855   Validation Loss: 8.725027432086904 \n",
      "Epoch: 856   Training Loss: 8.759069822956205 \n",
      "Epoch: 856   Validation Loss: 9.235612016044868 \n",
      "Epoch: 857   Training Loss: 8.632498835780948 \n",
      "Epoch: 857   Validation Loss: 9.368654338963806 \n",
      "Epoch: 858   Training Loss: 8.592861817121191 \n",
      "Epoch: 858   Validation Loss: 9.4413053753915 \n",
      "Epoch: 859   Training Loss: 8.536460881783135 \n",
      "Epoch: 859   Validation Loss: 9.432576034470895 \n",
      "Epoch: 860   Training Loss: 8.474210556860562 \n",
      "Epoch: 860   Validation Loss: 9.643363273759718 \n",
      "Epoch: 861   Training Loss: 8.375672229387147 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 861   Validation Loss: 9.774339623325067 \n",
      "Epoch: 862   Training Loss: 8.647142602882969 \n",
      "Epoch: 862   Validation Loss: 8.743152921036403 \n",
      "Epoch: 863   Training Loss: 8.976628494166066 \n",
      "Epoch: 863   Validation Loss: 8.758462939430506 \n",
      "Epoch: 864   Training Loss: 8.96948645300326 \n",
      "Epoch: 864   Validation Loss: 8.762615196710076 \n",
      "Epoch: 865   Training Loss: 8.958830880371732 \n",
      "Epoch: 865   Validation Loss: 8.771011549529648 \n",
      "Epoch: 866   Training Loss: 8.946959293024827 \n",
      "Epoch: 866   Validation Loss: 8.77447194399003 \n",
      "Epoch: 867   Training Loss: 8.927477257780192 \n",
      "Epoch: 867   Validation Loss: 8.797379499262224 \n",
      "Epoch: 868   Training Loss: 8.913710065871879 \n",
      "Epoch: 868   Validation Loss: 8.865736640697921 \n",
      "Epoch: 869   Training Loss: 8.890820439037425 \n",
      "Epoch: 869   Validation Loss: 8.963371532505223 \n",
      "Epoch: 870   Training Loss: 8.881168577760866 \n",
      "Epoch: 870   Validation Loss: 8.988198819978894 \n",
      "Epoch: 871   Training Loss: 8.746129078367371 \n",
      "Epoch: 871   Validation Loss: 8.740931510820985 \n",
      "Epoch: 872   Training Loss: 8.980366667035742 \n",
      "Epoch: 872   Validation Loss: 8.72810575002821 \n",
      "Epoch: 873   Training Loss: 8.974210566245299 \n",
      "Epoch: 873   Validation Loss: 8.729774656824514 \n",
      "Epoch: 874   Training Loss: 8.967459837566349 \n",
      "Epoch: 874   Validation Loss: 8.728810349870646 \n",
      "Epoch: 875   Training Loss: 8.955604072008324 \n",
      "Epoch: 875   Validation Loss: 8.743558813008981 \n",
      "Epoch: 876   Training Loss: 8.950110430584376 \n",
      "Epoch: 876   Validation Loss: 8.749882237727263 \n",
      "Epoch: 877   Training Loss: 8.939214363251216 \n",
      "Epoch: 877   Validation Loss: 8.755867450435309 \n",
      "Epoch: 878   Training Loss: 8.927119810365248 \n",
      "Epoch: 878   Validation Loss: 8.75214329546254 \n",
      "Epoch: 879   Training Loss: 8.914807613530595 \n",
      "Epoch: 879   Validation Loss: 8.769990622123764 \n",
      "Epoch: 880   Training Loss: 8.89868300649274 \n",
      "Epoch: 880   Validation Loss: 8.773029623054498 \n",
      "Epoch: 881   Training Loss: 8.884031509963062 \n",
      "Epoch: 881   Validation Loss: 8.793746091816313 \n",
      "Epoch: 882   Training Loss: 8.862986282842442 \n",
      "Epoch: 882   Validation Loss: 8.807573497411365 \n",
      "Epoch: 883   Training Loss: 8.829925889968557 \n",
      "Epoch: 883   Validation Loss: 8.850303146187413 \n",
      "Epoch: 884   Training Loss: 8.777189366474685 \n",
      "Epoch: 884   Validation Loss: 8.86021849609429 \n",
      "Epoch: 885   Training Loss: 8.749081698167652 \n",
      "Epoch: 885   Validation Loss: 8.914898972776198 \n",
      "Epoch: 886   Training Loss: 8.690736115589017 \n",
      "Epoch: 886   Validation Loss: 8.952040148154936 \n",
      "Epoch: 887   Training Loss: 8.623184547953185 \n",
      "Epoch: 887   Validation Loss: 9.014290289623487 \n",
      "Epoch: 888   Training Loss: 8.560558768549122 \n",
      "Epoch: 888   Validation Loss: 9.173131347855922 \n",
      "Epoch: 889   Training Loss: 8.490058983064284 \n",
      "Epoch: 889   Validation Loss: 9.226103417616136 \n",
      "Epoch: 890   Training Loss: 8.38431190399171 \n",
      "Epoch: 890   Validation Loss: 9.339275346643422 \n",
      "Epoch: 891   Training Loss: 8.474323100806545 \n",
      "Epoch: 891   Validation Loss: 8.754644930236138 \n",
      "Epoch: 892   Training Loss: 8.819452700696948 \n",
      "Epoch: 892   Validation Loss: 9.372385246588195 \n",
      "Epoch: 893   Training Loss: 8.472505292176406 \n",
      "Epoch: 893   Validation Loss: 8.721782739200009 \n",
      "Epoch: 894   Training Loss: 8.97855076818899 \n",
      "Epoch: 894   Validation Loss: 8.719973846778784 \n",
      "Epoch: 895   Training Loss: 8.96767239699426 \n",
      "Epoch: 895   Validation Loss: 8.723632386572628 \n",
      "Epoch: 896   Training Loss: 8.960572774392945 \n",
      "Epoch: 896   Validation Loss: 8.72608487926196 \n",
      "Epoch: 897   Training Loss: 8.950196014445215 \n",
      "Epoch: 897   Validation Loss: 8.73503294826528 \n",
      "Epoch: 898   Training Loss: 8.93721350038874 \n",
      "Epoch: 898   Validation Loss: 8.754564413744209 \n",
      "Epoch: 899   Training Loss: 8.922275559936379 \n",
      "Epoch: 899   Validation Loss: 8.766431934143439 \n",
      "Epoch: 900   Training Loss: 8.89492368198715 \n",
      "Epoch: 900   Validation Loss: 8.805387273711753 \n",
      "Epoch: 901   Training Loss: 8.861970201383443 \n",
      "Epoch: 901   Validation Loss: 8.817084076176585 \n",
      "Epoch: 902   Training Loss: 8.83019278380694 \n",
      "Epoch: 902   Validation Loss: 8.833078556089523 \n",
      "Epoch: 903   Training Loss: 8.779027670458563 \n",
      "Epoch: 903   Validation Loss: 8.87459903550297 \n",
      "Epoch: 904   Training Loss: 8.75034833735234 \n",
      "Epoch: 904   Validation Loss: 9.020524357154834 \n",
      "Epoch: 905   Training Loss: 8.714526317136276 \n",
      "Epoch: 905   Validation Loss: 9.047519800005821 \n",
      "Epoch: 906   Training Loss: 8.661708796568139 \n",
      "Epoch: 906   Validation Loss: 9.173774751395948 \n",
      "Epoch: 907   Training Loss: 8.638975055267798 \n",
      "Epoch: 907   Validation Loss: 9.185258275154556 \n",
      "Epoch: 908   Training Loss: 8.58388979853392 \n",
      "Epoch: 908   Validation Loss: 9.37869793909116 \n",
      "Epoch: 909   Training Loss: 8.520445958972394 \n",
      "Epoch: 909   Validation Loss: 9.518262745237193 \n",
      "Epoch: 910   Training Loss: 8.49443414300512 \n",
      "Epoch: 910   Validation Loss: 9.651134423797355 \n",
      "Epoch: 911   Training Loss: 8.471294243419065 \n",
      "Epoch: 911   Validation Loss: 9.899791119109723 \n",
      "Epoch: 912   Training Loss: 8.529449769675788 \n",
      "Epoch: 912   Validation Loss: 9.879894586415517 \n",
      "Epoch: 913   Training Loss: 8.431556666560715 \n",
      "Epoch: 913   Validation Loss: 9.947236490868873 \n",
      "Epoch: 914   Training Loss: 8.756690062079308 \n",
      "Epoch: 914   Validation Loss: 9.955417904241163 \n",
      "Epoch: 915   Training Loss: 8.536110013181283 \n",
      "Epoch: 915   Validation Loss: 9.988449174436916 \n",
      "Epoch: 916   Training Loss: 8.47521371750033 \n",
      "Epoch: 916   Validation Loss: 10.244936031396941 \n",
      "Epoch: 917   Training Loss: 8.297796816391585 \n",
      "Epoch: 917   Validation Loss: 10.261213761441047 \n",
      "Epoch: 918   Training Loss: 8.15964856067779 \n",
      "Epoch: 918   Validation Loss: 10.34929554925277 \n",
      "Epoch: 919   Training Loss: 8.699582423844422 \n",
      "Epoch: 919   Validation Loss: 8.72849362664918 \n",
      "Epoch: 920   Training Loss: 8.972100182451177 \n",
      "Epoch: 920   Validation Loss: 8.726356840915118 \n",
      "Epoch: 921   Training Loss: 8.960293791681377 \n",
      "Epoch: 921   Validation Loss: 8.732727262167277 \n",
      "Epoch: 922   Training Loss: 8.948871382405716 \n",
      "Epoch: 922   Validation Loss: 8.737798286910081 \n",
      "Epoch: 923   Training Loss: 8.93528820878074 \n",
      "Epoch: 923   Validation Loss: 8.743591557400794 \n",
      "Epoch: 924   Training Loss: 8.91612558515902 \n",
      "Epoch: 924   Validation Loss: 8.758131079946038 \n",
      "Epoch: 925   Training Loss: 8.89850100949035 \n",
      "Epoch: 925   Validation Loss: 8.767124733999085 \n",
      "Epoch: 926   Training Loss: 8.883262076031714 \n",
      "Epoch: 926   Validation Loss: 8.76968903487207 \n",
      "Epoch: 927   Training Loss: 8.85282526757669 \n",
      "Epoch: 927   Validation Loss: 8.770804911156516 \n",
      "Epoch: 928   Training Loss: 8.829461323218617 \n",
      "Epoch: 928   Validation Loss: 8.781948768606604 \n",
      "Epoch: 929   Training Loss: 8.79759571678907 \n",
      "Epoch: 929   Validation Loss: 8.790693008729946 \n",
      "Epoch: 930   Training Loss: 8.766863275155577 \n",
      "Epoch: 930   Validation Loss: 8.81430964431694 \n",
      "Epoch: 931   Training Loss: 8.717559899933025 \n",
      "Epoch: 931   Validation Loss: 8.855352689982654 \n",
      "Epoch: 932   Training Loss: 8.659227911904335 \n",
      "Epoch: 932   Validation Loss: 8.8543116854333 \n",
      "Epoch: 933   Training Loss: 8.591824777588231 \n",
      "Epoch: 933   Validation Loss: 8.914288114779348 \n",
      "Epoch: 934   Training Loss: 8.522809743263135 \n",
      "Epoch: 934   Validation Loss: 8.925907322907273 \n",
      "Epoch: 935   Training Loss: 8.447094967813914 \n",
      "Epoch: 935   Validation Loss: 9.002739501666676 \n",
      "Epoch: 936   Training Loss: 8.365421437599803 \n",
      "Epoch: 936   Validation Loss: 9.01475229240314 \n",
      "Epoch: 937   Training Loss: 8.490539980628105 \n",
      "Epoch: 937   Validation Loss: 8.727015850821461 \n",
      "Epoch: 938   Training Loss: 8.978536666647537 \n",
      "Epoch: 938   Validation Loss: 8.718620347576275 \n",
      "Epoch: 939   Training Loss: 8.9707008594734 \n",
      "Epoch: 939   Validation Loss: 8.730240269093539 \n",
      "Epoch: 940   Training Loss: 8.96048747510158 \n",
      "Epoch: 940   Validation Loss: 8.741008149393574 \n",
      "Epoch: 941   Training Loss: 8.95107565898422 \n",
      "Epoch: 941   Validation Loss: 8.768782331164173 \n",
      "Epoch: 942   Training Loss: 8.93492287879566 \n",
      "Epoch: 942   Validation Loss: 8.789555803279312 \n",
      "Epoch: 943   Training Loss: 8.917002462889206 \n",
      "Epoch: 943   Validation Loss: 8.836408241485266 \n",
      "Epoch: 944   Training Loss: 8.89428661089272 \n",
      "Epoch: 944   Validation Loss: 8.89383092712372 \n",
      "Epoch: 945   Training Loss: 8.87845409453564 \n",
      "Epoch: 945   Validation Loss: 8.969931776682712 \n",
      "Epoch: 946   Training Loss: 8.841077113179278 \n",
      "Epoch: 946   Validation Loss: 9.084792154647749 \n",
      "Epoch: 947   Training Loss: 8.81364355083595 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 947   Validation Loss: 9.135374339479062 \n",
      "Epoch: 948   Training Loss: 8.770447674401927 \n",
      "Epoch: 948   Validation Loss: 9.281845174319539 \n",
      "Epoch: 949   Training Loss: 8.72650478991152 \n",
      "Epoch: 949   Validation Loss: 9.448599265189541 \n",
      "Epoch: 950   Training Loss: 8.689458090712309 \n",
      "Epoch: 950   Validation Loss: 9.310501658074545 \n",
      "Epoch: 951   Training Loss: 8.670664813159561 \n",
      "Epoch: 951   Validation Loss: 9.52102935021698 \n",
      "Epoch: 952   Training Loss: 8.571125908402678 \n",
      "Epoch: 952   Validation Loss: 9.664493780654798 \n",
      "Epoch: 953   Training Loss: 8.509794423052082 \n",
      "Epoch: 953   Validation Loss: 9.848209429213906 \n",
      "Epoch: 954   Training Loss: 8.453915131275034 \n",
      "Epoch: 954   Validation Loss: 9.629805906872022 \n",
      "Epoch: 955   Training Loss: 8.376242675897545 \n",
      "Epoch: 955   Validation Loss: 9.819638428733496 \n",
      "Epoch: 956   Training Loss: 8.294407506274277 \n",
      "Epoch: 956   Validation Loss: 9.942003891445191 \n",
      "Epoch: 957   Training Loss: 8.269539116885813 \n",
      "Epoch: 957   Validation Loss: 9.84427909828875 \n",
      "Epoch: 958   Training Loss: 8.119580928031304 \n",
      "Epoch: 958   Validation Loss: 10.144703937572983 \n",
      "Epoch: 959   Training Loss: 8.619145787965458 \n",
      "Epoch: 959   Validation Loss: 8.71973043834982 \n",
      "Epoch: 960   Training Loss: 8.973054060070469 \n",
      "Epoch: 960   Validation Loss: 8.727463310450561 \n",
      "Epoch: 961   Training Loss: 8.96824038327009 \n",
      "Epoch: 961   Validation Loss: 8.730811312827312 \n",
      "Epoch: 962   Training Loss: 8.9577149387891 \n",
      "Epoch: 962   Validation Loss: 8.747473010674922 \n",
      "Epoch: 963   Training Loss: 8.948492705816696 \n",
      "Epoch: 963   Validation Loss: 8.756263514130676 \n",
      "Epoch: 964   Training Loss: 8.93134000226032 \n",
      "Epoch: 964   Validation Loss: 8.770611512780368 \n",
      "Epoch: 965   Training Loss: 8.914271790857137 \n",
      "Epoch: 965   Validation Loss: 8.776912960390103 \n",
      "Epoch: 966   Training Loss: 8.893502569139015 \n",
      "Epoch: 966   Validation Loss: 8.802106703717664 \n",
      "Epoch: 967   Training Loss: 8.865051882279243 \n",
      "Epoch: 967   Validation Loss: 8.82239676480958 \n",
      "Epoch: 968   Training Loss: 8.851525701598845 \n",
      "Epoch: 968   Validation Loss: 8.835755059598922 \n",
      "Epoch: 969   Training Loss: 8.801951840362586 \n",
      "Epoch: 969   Validation Loss: 8.879820508765132 \n",
      "Epoch: 970   Training Loss: 8.760273975644639 \n",
      "Epoch: 970   Validation Loss: 8.94536434952174 \n",
      "Epoch: 971   Training Loss: 8.711871883905804 \n",
      "Epoch: 971   Validation Loss: 9.03497788088835 \n",
      "Epoch: 972   Training Loss: 8.672033956137131 \n",
      "Epoch: 972   Validation Loss: 9.06288474219912 \n",
      "Epoch: 973   Training Loss: 8.619207352464986 \n",
      "Epoch: 973   Validation Loss: 9.193473307539369 \n",
      "Epoch: 974   Training Loss: 8.543700992500836 \n",
      "Epoch: 974   Validation Loss: 9.288423950639434 \n",
      "Epoch: 975   Training Loss: 8.459697373117562 \n",
      "Epoch: 975   Validation Loss: 9.565386840954993 \n",
      "Epoch: 976   Training Loss: 8.343254363251233 \n",
      "Epoch: 976   Validation Loss: 9.66501528941999 \n",
      "Epoch: 977   Training Loss: 8.28995470157365 \n",
      "Epoch: 977   Validation Loss: 9.754034125757048 \n",
      "Epoch: 978   Training Loss: 8.175244391128475 \n",
      "Epoch: 978   Validation Loss: 9.894598381807246 \n",
      "Epoch: 979   Training Loss: 8.281562534419866 \n",
      "Epoch: 979   Validation Loss: 9.919043796537741 \n",
      "Epoch: 980   Training Loss: 8.0912534616806 \n",
      "Epoch: 980   Validation Loss: 9.980984617805285 \n",
      "Epoch: 981   Training Loss: 7.960279426018547 \n",
      "Epoch: 981   Validation Loss: 10.58255970684658 \n",
      "Epoch: 982   Training Loss: 7.971128176804871 \n",
      "Epoch: 982   Validation Loss: 10.591866301889203 \n",
      "Epoch: 983   Training Loss: 7.811147284556911 \n",
      "Epoch: 983   Validation Loss: 10.941509432889683 \n",
      "Epoch: 984   Training Loss: 8.456296092483127 \n",
      "Epoch: 984   Validation Loss: 8.732905143301094 \n",
      "Epoch: 985   Training Loss: 8.973213386239326 \n",
      "Epoch: 985   Validation Loss: 8.736921941750392 \n",
      "Epoch: 986   Training Loss: 8.9565839907682 \n",
      "Epoch: 986   Validation Loss: 8.763352846264723 \n",
      "Epoch: 987   Training Loss: 8.939454726715468 \n",
      "Epoch: 987   Validation Loss: 8.777914267908372 \n",
      "Epoch: 988   Training Loss: 8.915447033641142 \n",
      "Epoch: 988   Validation Loss: 8.79130496936165 \n",
      "Epoch: 989   Training Loss: 8.883248514244741 \n",
      "Epoch: 989   Validation Loss: 8.908589869281297 \n",
      "Epoch: 990   Training Loss: 8.83818660690313 \n",
      "Epoch: 990   Validation Loss: 8.881752671864001 \n",
      "Epoch: 991   Training Loss: 8.787739244940553 \n",
      "Epoch: 991   Validation Loss: 8.957226215408497 \n",
      "Epoch: 992   Training Loss: 8.7401620834999 \n",
      "Epoch: 992   Validation Loss: 9.040390758955276 \n",
      "Epoch: 993   Training Loss: 8.651521244240906 \n",
      "Epoch: 993   Validation Loss: 9.145266042712834 \n",
      "Epoch: 994   Training Loss: 8.583606119485221 \n",
      "Epoch: 994   Validation Loss: 9.193393373926659 \n",
      "Epoch: 995   Training Loss: 8.539739891422139 \n",
      "Epoch: 995   Validation Loss: 9.201501983727697 \n",
      "Epoch: 996   Training Loss: 8.446532204513307 \n",
      "Epoch: 996   Validation Loss: 9.27114529162243 \n",
      "Epoch: 997   Training Loss: 8.379067978868475 \n",
      "Epoch: 997   Validation Loss: 9.14535606709972 \n",
      "Epoch: 998   Training Loss: 8.296207289236287 \n",
      "Epoch: 998   Validation Loss: 9.265774301950765 \n",
      "Epoch: 999   Training Loss: 8.163191563158628 \n",
      "Epoch: 999   Validation Loss: 9.30972445515656 \n",
      "Epoch: 1000   Training Loss: 8.039494144667948 \n",
      "Epoch: 1000   Validation Loss: 9.439326483707344 \n",
      "Epoch: 1001   Training Loss: 7.934366991707385 \n",
      "Epoch: 1001   Validation Loss: 9.495626636005214 \n",
      "Epoch: 1002   Training Loss: 7.80033548545688 \n",
      "Epoch: 1002   Validation Loss: 9.701309141530029 \n",
      "Epoch: 1003   Training Loss: 7.715202549213493 \n",
      "Epoch: 1003   Validation Loss: 9.733877572303289 \n",
      "Epoch: 1004   Training Loss: 8.117156757658744 \n",
      "Epoch: 1004   Validation Loss: 8.732620012749479 \n",
      "Epoch: 1005   Training Loss: 8.981617264864216 \n",
      "Epoch: 1005   Validation Loss: 8.728712150548533 \n",
      "Epoch: 1006   Training Loss: 8.964964297083206 \n",
      "Epoch: 1006   Validation Loss: 8.727679983571857 \n",
      "Epoch: 1007   Training Loss: 8.95484919740017 \n",
      "Epoch: 1007   Validation Loss: 8.744649130617365 \n",
      "Epoch: 1008   Training Loss: 8.939786511019465 \n",
      "Epoch: 1008   Validation Loss: 8.747916432622585 \n",
      "Epoch: 1009   Training Loss: 8.921191909701648 \n",
      "Epoch: 1009   Validation Loss: 8.753146461619995 \n",
      "Epoch: 1010   Training Loss: 8.891611891661302 \n",
      "Epoch: 1010   Validation Loss: 8.789608572951265 \n",
      "Epoch: 1011   Training Loss: 8.86944383461053 \n",
      "Epoch: 1011   Validation Loss: 8.809909915666351 \n",
      "Epoch: 1012   Training Loss: 8.835070648276117 \n",
      "Epoch: 1012   Validation Loss: 8.846498746005938 \n",
      "Epoch: 1013   Training Loss: 8.800549408995723 \n",
      "Epoch: 1013   Validation Loss: 8.891261917172644 \n",
      "Epoch: 1014   Training Loss: 8.745634906285474 \n",
      "Epoch: 1014   Validation Loss: 8.9165215831086 \n",
      "Epoch: 1015   Training Loss: 8.706333975969722 \n",
      "Epoch: 1015   Validation Loss: 8.991039792487928 \n",
      "Epoch: 1016   Training Loss: 8.646678898793315 \n",
      "Epoch: 1016   Validation Loss: 8.906723498742702 \n",
      "Epoch: 1017   Training Loss: 8.604327264298012 \n",
      "Epoch: 1017   Validation Loss: 8.981713652824784 \n",
      "Epoch: 1018   Training Loss: 8.535841998902344 \n",
      "Epoch: 1018   Validation Loss: 9.134817950028603 \n",
      "Epoch: 1019   Training Loss: 8.453833781985862 \n",
      "Epoch: 1019   Validation Loss: 9.238667501949989 \n",
      "Epoch: 1020   Training Loss: 8.359452378970333 \n",
      "Epoch: 1020   Validation Loss: 9.341403232282053 \n",
      "Epoch: 1021   Training Loss: 8.252669296008337 \n",
      "Epoch: 1021   Validation Loss: 9.478059216508147 \n",
      "Epoch: 1022   Training Loss: 8.19226679014966 \n",
      "Epoch: 1022   Validation Loss: 9.53927842142951 \n",
      "Epoch: 1023   Training Loss: 8.073039315508689 \n",
      "Epoch: 1023   Validation Loss: 9.533896377413809 \n",
      "Epoch: 1024   Training Loss: 7.9781336506947245 \n",
      "Epoch: 1024   Validation Loss: 9.614280612425592 \n",
      "Epoch: 1025   Training Loss: 7.925713953927918 \n",
      "Epoch: 1025   Validation Loss: 9.246533654190287 \n",
      "Epoch: 1026   Training Loss: 7.919596552385127 \n",
      "Epoch: 1026   Validation Loss: 9.269444046664375 \n",
      "Epoch: 1027   Training Loss: 7.669230565025594 \n",
      "Epoch: 1027   Validation Loss: 9.308228340834788 \n",
      "Epoch: 1028   Training Loss: 7.7527732498472774 \n",
      "Epoch: 1028   Validation Loss: 8.75887519096507 \n",
      "Epoch: 1029   Training Loss: 8.98609977665926 \n",
      "Epoch: 1029   Validation Loss: 8.743062916213468 \n",
      "Epoch: 1030   Training Loss: 8.959470172570779 \n",
      "Epoch: 1030   Validation Loss: 8.745797577353118 \n",
      "Epoch: 1031   Training Loss: 8.927346400085032 \n",
      "Epoch: 1031   Validation Loss: 8.766458997767845 \n",
      "Epoch: 1032   Training Loss: 8.892232221419327 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1032   Validation Loss: 8.792257506976776 \n",
      "Epoch: 1033   Training Loss: 8.846857736757183 \n",
      "Epoch: 1033   Validation Loss: 8.813470022293687 \n",
      "Epoch: 1034   Training Loss: 8.798749302223976 \n",
      "Epoch: 1034   Validation Loss: 8.855231216491555 \n",
      "Epoch: 1035   Training Loss: 8.747995204729396 \n",
      "Epoch: 1035   Validation Loss: 8.865999082761228 \n",
      "Epoch: 1036   Training Loss: 8.664487686189998 \n",
      "Epoch: 1036   Validation Loss: 8.923000653199878 \n",
      "Epoch: 1037   Training Loss: 8.589915018706508 \n",
      "Epoch: 1037   Validation Loss: 9.01222015907563 \n",
      "Epoch: 1038   Training Loss: 8.48357053277691 \n",
      "Epoch: 1038   Validation Loss: 9.057968008052955 \n",
      "Epoch: 1039   Training Loss: 8.40702009989555 \n",
      "Epoch: 1039   Validation Loss: 9.145242708162638 \n",
      "Epoch: 1040   Training Loss: 8.316369700371327 \n",
      "Epoch: 1040   Validation Loss: 9.222836868356628 \n",
      "Epoch: 1041   Training Loss: 8.172643016056343 \n",
      "Epoch: 1041   Validation Loss: 9.334133630181848 \n",
      "Epoch: 1042   Training Loss: 8.029899548952843 \n",
      "Epoch: 1042   Validation Loss: 9.509871197470394 \n",
      "Epoch: 1043   Training Loss: 7.916294332171089 \n",
      "Epoch: 1043   Validation Loss: 9.453577224508464 \n",
      "Epoch: 1044   Training Loss: 7.75838861351115 \n",
      "Epoch: 1044   Validation Loss: 9.65184310030831 \n",
      "Epoch: 1045   Training Loss: 7.600309721034355 \n",
      "Epoch: 1045   Validation Loss: 9.721587103622985 \n",
      "Epoch: 1046   Training Loss: 7.513899316030804 \n",
      "Epoch: 1046   Validation Loss: 9.825130522920103 \n",
      "Epoch: 1047   Training Loss: 7.253693073375438 \n",
      "Epoch: 1047   Validation Loss: 9.737119915547364 \n",
      "Epoch: 1048   Training Loss: 7.145311340884935 \n",
      "Epoch: 1048   Validation Loss: 9.890301535265934 \n",
      "Epoch: 1049   Training Loss: 6.987424545961319 \n",
      "Epoch: 1049   Validation Loss: 9.939258580339752 \n",
      "Epoch: 1050   Training Loss: 6.823725615771051 \n",
      "Epoch: 1050   Validation Loss: 10.097191361227397 \n",
      "Epoch: 1051   Training Loss: 6.710562408133816 \n",
      "Epoch: 1051   Validation Loss: 8.856835111486447 \n",
      "Epoch: 1052   Training Loss: 8.415443564977009 \n",
      "Epoch: 1052   Validation Loss: 9.005172049360418 \n",
      "Epoch: 1053   Training Loss: 8.32703171910954 \n",
      "Epoch: 1053   Validation Loss: 9.077142375753514 \n",
      "Epoch: 1054   Training Loss: 8.275715687955962 \n",
      "Epoch: 1054   Validation Loss: 9.17879897444322 \n",
      "Epoch: 1055   Training Loss: 8.113348054288819 \n",
      "Epoch: 1055   Validation Loss: 9.210228579111462 \n",
      "Epoch: 1056   Training Loss: 7.952415024924749 \n",
      "Epoch: 1056   Validation Loss: 9.434713018756089 \n",
      "Epoch: 1057   Training Loss: 7.840387572516484 \n",
      "Epoch: 1057   Validation Loss: 9.52250443565481 \n",
      "Epoch: 1058   Training Loss: 7.68653643536703 \n",
      "Epoch: 1058   Validation Loss: 9.783313451497254 \n",
      "Epoch: 1059   Training Loss: 7.5669092159791616 \n",
      "Epoch: 1059   Validation Loss: 9.90639108068272 \n",
      "Epoch: 1060   Training Loss: 7.635527931262963 \n",
      "Epoch: 1060   Validation Loss: 9.77782763038693 \n",
      "Epoch: 1061   Training Loss: 7.796952404197042 \n",
      "Epoch: 1061   Validation Loss: 8.851432227008967 \n",
      "Epoch: 1062   Training Loss: 7.532902244411914 \n",
      "Epoch: 1062   Validation Loss: 9.995429711221005 \n",
      "Epoch: 1063   Training Loss: 7.221449333479926 \n",
      "Epoch: 1063   Validation Loss: 8.83963608537679 \n",
      "Epoch: 1064   Training Loss: 7.224278984117337 \n",
      "Epoch: 1064   Validation Loss: 8.98594655136767 \n",
      "Epoch: 1065   Training Loss: 7.057140795749108 \n",
      "Epoch: 1065   Validation Loss: 9.130160543851972 \n",
      "Epoch: 1066   Training Loss: 7.025959583217299 \n",
      "Epoch: 1066   Validation Loss: 8.814063201574399 \n",
      "Epoch: 1067   Training Loss: 8.998908741289254 \n",
      "Epoch: 1067   Validation Loss: 8.734737686234922 \n",
      "Epoch: 1068   Training Loss: 8.966031017247543 \n",
      "Epoch: 1068   Validation Loss: 8.743787185624733 \n",
      "Epoch: 1069   Training Loss: 8.951287375063355 \n",
      "Epoch: 1069   Validation Loss: 8.769860016848872 \n",
      "Epoch: 1070   Training Loss: 8.929977657027782 \n",
      "Epoch: 1070   Validation Loss: 8.78413339028466 \n",
      "Epoch: 1071   Training Loss: 8.902528810392603 \n",
      "Epoch: 1071   Validation Loss: 8.827480213716392 \n",
      "Epoch: 1072   Training Loss: 8.873050976439838 \n",
      "Epoch: 1072   Validation Loss: 8.844822661130097 \n",
      "Epoch: 1073   Training Loss: 8.836736600851133 \n",
      "Epoch: 1073   Validation Loss: 8.907113163458797 \n",
      "Epoch: 1074   Training Loss: 8.796628136307795 \n",
      "Epoch: 1074   Validation Loss: 8.953440557147715 \n",
      "Epoch: 1075   Training Loss: 8.729682201687805 \n",
      "Epoch: 1075   Validation Loss: 9.008465211921893 \n",
      "Epoch: 1076   Training Loss: 8.658548881291308 \n",
      "Epoch: 1076   Validation Loss: 9.107037778858103 \n",
      "Epoch: 1077   Training Loss: 8.588448165439472 \n",
      "Epoch: 1077   Validation Loss: 9.15493728548688 \n",
      "Epoch: 1078   Training Loss: 8.488390810509797 \n",
      "Epoch: 1078   Validation Loss: 9.231013550838727 \n",
      "Epoch: 1079   Training Loss: 8.38225542235479 \n",
      "Epoch: 1079   Validation Loss: 9.371717056046414 \n",
      "Epoch: 1080   Training Loss: 8.268120049199501 \n",
      "Epoch: 1080   Validation Loss: 9.455585708015327 \n",
      "Epoch: 1081   Training Loss: 8.118931853512224 \n",
      "Epoch: 1081   Validation Loss: 9.581199525690305 \n",
      "Epoch: 1082   Training Loss: 8.007934543186947 \n",
      "Epoch: 1082   Validation Loss: 9.611993049631415 \n",
      "Epoch: 1083   Training Loss: 7.848498038168405 \n",
      "Epoch: 1083   Validation Loss: 9.83630618887004 \n",
      "Epoch: 1084   Training Loss: 7.655757615693984 \n",
      "Epoch: 1084   Validation Loss: 9.872364996470614 \n",
      "Epoch: 1085   Training Loss: 7.3641429121119435 \n",
      "Epoch: 1085   Validation Loss: 10.155881131099303 \n",
      "Epoch: 1086   Training Loss: 7.180019388945566 \n",
      "Epoch: 1086   Validation Loss: 10.274319368527271 \n",
      "Epoch: 1087   Training Loss: 7.0767766717642875 \n",
      "Epoch: 1087   Validation Loss: 10.382999752456174 \n",
      "Epoch: 1088   Training Loss: 6.76616479191321 \n",
      "Epoch: 1088   Validation Loss: 11.189899618250958 \n",
      "Epoch: 1089   Training Loss: 6.6341244237955745 \n",
      "Epoch: 1089   Validation Loss: 10.880350751247201 \n",
      "Epoch: 1090   Training Loss: 6.326559422085193 \n",
      "Epoch: 1090   Validation Loss: 11.180039207072367 \n",
      "Epoch: 1091   Training Loss: 6.110097764377948 \n",
      "Epoch: 1091   Validation Loss: 11.133980941090842 \n",
      "Epoch: 1092   Training Loss: 5.805926165175325 \n",
      "Epoch: 1092   Validation Loss: 11.384459281546578 \n",
      "Epoch: 1093   Training Loss: 8.460374477767784 \n",
      "Epoch: 1093   Validation Loss: 8.728198121688607 \n",
      "Epoch: 1094   Training Loss: 8.968584047787612 \n",
      "Epoch: 1094   Validation Loss: 8.740053554367131 \n",
      "Epoch: 1095   Training Loss: 8.948170330708153 \n",
      "Epoch: 1095   Validation Loss: 8.742563821590307 \n",
      "Epoch: 1096   Training Loss: 8.920162817773194 \n",
      "Epoch: 1096   Validation Loss: 8.751539573784697 \n",
      "Epoch: 1097   Training Loss: 8.888703927849917 \n",
      "Epoch: 1097   Validation Loss: 8.765006906587688 \n",
      "Epoch: 1098   Training Loss: 8.84639906869961 \n",
      "Epoch: 1098   Validation Loss: 8.78961586887132 \n",
      "Epoch: 1099   Training Loss: 8.795009999639607 \n",
      "Epoch: 1099   Validation Loss: 8.824274773259246 \n",
      "Epoch: 1100   Training Loss: 8.736120147534287 \n",
      "Epoch: 1100   Validation Loss: 8.86162825210677 \n",
      "Epoch: 1101   Training Loss: 8.671119202324729 \n",
      "Epoch: 1101   Validation Loss: 8.90878859186743 \n",
      "Epoch: 1102   Training Loss: 8.580856600963676 \n",
      "Epoch: 1102   Validation Loss: 9.001103229105444 \n",
      "Epoch: 1103   Training Loss: 8.482093693407514 \n",
      "Epoch: 1103   Validation Loss: 9.043528154491199 \n",
      "Epoch: 1104   Training Loss: 8.369936728908142 \n",
      "Epoch: 1104   Validation Loss: 9.164469716488325 \n",
      "Epoch: 1105   Training Loss: 8.233103528621884 \n",
      "Epoch: 1105   Validation Loss: 9.355337449619457 \n",
      "Epoch: 1106   Training Loss: 8.092703710574492 \n",
      "Epoch: 1106   Validation Loss: 9.548878124079687 \n",
      "Epoch: 1107   Training Loss: 7.977576046372067 \n",
      "Epoch: 1107   Validation Loss: 9.829727198853979 \n",
      "Epoch: 1108   Training Loss: 7.783217396251718 \n",
      "Epoch: 1108   Validation Loss: 9.996516169862085 \n",
      "Epoch: 1109   Training Loss: 7.601394053892756 \n",
      "Epoch: 1109   Validation Loss: 10.097240596365527 \n",
      "Epoch: 1110   Training Loss: 7.405777851625197 \n",
      "Epoch: 1110   Validation Loss: 10.269419558101267 \n",
      "Epoch: 1111   Training Loss: 7.152832351877507 \n",
      "Epoch: 1111   Validation Loss: 10.351134046556416 \n",
      "Epoch: 1112   Training Loss: 6.896307178848408 \n",
      "Epoch: 1112   Validation Loss: 10.801525099327375 \n",
      "Epoch: 1113   Training Loss: 6.71204574041794 \n",
      "Epoch: 1113   Validation Loss: 10.728140619094257 \n",
      "Epoch: 1114   Training Loss: 6.4863031693748825 \n",
      "Epoch: 1114   Validation Loss: 11.075474190807226 \n",
      "Epoch: 1115   Training Loss: 6.17039411333767 \n",
      "Epoch: 1115   Validation Loss: 11.100639933018481 \n",
      "Epoch: 1116   Training Loss: 5.915485871221682 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1116   Validation Loss: 11.594014985544938 \n",
      "Epoch: 1117   Training Loss: 5.750692718264654 \n",
      "Epoch: 1117   Validation Loss: 11.50271164831374 \n",
      "Epoch: 1118   Training Loss: 5.535343591044143 \n",
      "Epoch: 1118   Validation Loss: 10.186482177142551 \n",
      "Epoch: 1119   Training Loss: 5.3228286709994626 \n",
      "Epoch: 1119   Validation Loss: 10.204103901406775 \n",
      "Epoch: 1120   Training Loss: 5.088768124737813 \n",
      "Epoch: 1120   Validation Loss: 10.705107396346358 \n",
      "Epoch: 1121   Training Loss: 4.801784763966574 \n",
      "Epoch: 1121   Validation Loss: 10.797300100590695 \n",
      "Epoch: 1122   Training Loss: 4.568955945868657 \n",
      "Epoch: 1122   Validation Loss: 11.399245650168133 \n",
      "Epoch: 1123   Training Loss: 4.3593320475332344 \n",
      "Epoch: 1123   Validation Loss: 11.635092109606617 \n",
      "Epoch: 1124   Training Loss: 4.150962179724158 \n",
      "Epoch: 1124   Validation Loss: 11.444659297202552 \n",
      "Epoch: 1125   Training Loss: 3.9500257491183404 \n",
      "Epoch: 1125   Validation Loss: 11.296061095345378 \n",
      "Epoch: 1126   Training Loss: 3.749685181130222 \n",
      "Epoch: 1126   Validation Loss: 12.058294783257065 \n",
      "Epoch: 1127   Training Loss: 8.944211093291313 \n",
      "Epoch: 1127   Validation Loss: 8.734545922592542 \n",
      "Epoch: 1128   Training Loss: 8.966495332838386 \n",
      "Epoch: 1128   Validation Loss: 8.743704622694363 \n",
      "Epoch: 1129   Training Loss: 8.940203657554669 \n",
      "Epoch: 1129   Validation Loss: 8.76110621268178 \n",
      "Epoch: 1130   Training Loss: 8.897053197620618 \n",
      "Epoch: 1130   Validation Loss: 8.769672549586977 \n",
      "Epoch: 1131   Training Loss: 8.839912617609915 \n",
      "Epoch: 1131   Validation Loss: 8.792400840494885 \n",
      "Epoch: 1132   Training Loss: 8.77601556140468 \n",
      "Epoch: 1132   Validation Loss: 8.8085861926586 \n",
      "Epoch: 1133   Training Loss: 8.685451668241964 \n",
      "Epoch: 1133   Validation Loss: 8.855804583238298 \n",
      "Epoch: 1134   Training Loss: 8.577594730074773 \n",
      "Epoch: 1134   Validation Loss: 8.931630767789969 \n",
      "Epoch: 1135   Training Loss: 8.451264862967731 \n",
      "Epoch: 1135   Validation Loss: 8.9930512151651 \n",
      "Epoch: 1136   Training Loss: 8.296775711894993 \n",
      "Epoch: 1136   Validation Loss: 9.045157324835825 \n",
      "Epoch: 1137   Training Loss: 8.134786637278792 \n",
      "Epoch: 1137   Validation Loss: 9.241055103548543 \n",
      "Epoch: 1138   Training Loss: 7.94186448475609 \n",
      "Epoch: 1138   Validation Loss: 9.229355039140628 \n",
      "Epoch: 1139   Training Loss: 7.684315858340162 \n",
      "Epoch: 1139   Validation Loss: 9.378067775866478 \n",
      "Epoch: 1140   Training Loss: 8.251445430430064 \n",
      "Epoch: 1140   Validation Loss: 8.751386270246085 \n",
      "Epoch: 1141   Training Loss: 8.281944743596648 \n",
      "Epoch: 1141   Validation Loss: 9.41271398623079 \n",
      "Epoch: 1142   Training Loss: 7.994136537203449 \n",
      "Epoch: 1142   Validation Loss: 8.7443548733011 \n",
      "Epoch: 1143   Training Loss: 8.969858718040308 \n",
      "Epoch: 1143   Validation Loss: 8.763686994397663 \n",
      "Epoch: 1144   Training Loss: 8.257097388482798 \n",
      "Epoch: 1144   Validation Loss: 9.290834615282366 \n",
      "Epoch: 1145   Training Loss: 7.074385312352228 \n",
      "Epoch: 1145   Validation Loss: 9.38989341682166 \n",
      "Epoch: 1146   Training Loss: 6.803637308475233 \n",
      "Epoch: 1146   Validation Loss: 9.310673991539458 \n",
      "Epoch: 1147   Training Loss: 6.509470795145467 \n",
      "Epoch: 1147   Validation Loss: 9.623659655111398 \n",
      "Epoch: 1148   Training Loss: 6.241719506603503 \n",
      "Epoch: 1148   Validation Loss: 9.79220284940446 \n",
      "Epoch: 1149   Training Loss: 6.011976066388752 \n",
      "Epoch: 1149   Validation Loss: 10.056685093482503 \n",
      "Epoch: 1150   Training Loss: 5.708781013651054 \n",
      "Epoch: 1150   Validation Loss: 10.314786282516161 \n",
      "Epoch: 1151   Training Loss: 5.457432137879474 \n",
      "Epoch: 1151   Validation Loss: 10.672103833611304 \n",
      "Epoch: 1152   Training Loss: 5.187748759026377 \n",
      "Epoch: 1152   Validation Loss: 10.7862468008222 \n",
      "Epoch: 1153   Training Loss: 4.975421813335576 \n",
      "Epoch: 1153   Validation Loss: 11.100679595706408 \n",
      "Epoch: 1154   Training Loss: 4.667002381899315 \n",
      "Epoch: 1154   Validation Loss: 11.153140202739037 \n",
      "Epoch: 1155   Training Loss: 4.4566391626948745 \n",
      "Epoch: 1155   Validation Loss: 11.166739409274655 \n",
      "Epoch: 1156   Training Loss: 4.255498404426206 \n",
      "Epoch: 1156   Validation Loss: 11.144928717084268 \n",
      "Epoch: 1157   Training Loss: 4.0464810485445515 \n",
      "Epoch: 1157   Validation Loss: 11.407920117005803 \n",
      "Epoch: 1158   Training Loss: 3.8840973501744744 \n",
      "Epoch: 1158   Validation Loss: 12.08054947064427 \n",
      "Epoch: 1159   Training Loss: 3.7001521111359033 \n",
      "Epoch: 1159   Validation Loss: 11.801357092471804 \n",
      "Epoch: 1160   Training Loss: 3.5968982559569707 \n",
      "Epoch: 1160   Validation Loss: 12.13329591438579 \n",
      "Epoch: 1161   Training Loss: 3.402928761579618 \n",
      "Epoch: 1161   Validation Loss: 12.224897984504599 \n",
      "Epoch: 1162   Training Loss: 3.339597162198302 \n",
      "Epoch: 1162   Validation Loss: 12.397103652818187 \n",
      "Epoch: 1163   Training Loss: 3.28788955397499 \n",
      "Epoch: 1163   Validation Loss: 12.442072568669268 \n",
      "Epoch: 1164   Training Loss: 3.28452845484257 \n",
      "Epoch: 1164   Validation Loss: 12.641470627928191 \n",
      "Epoch: 1165   Training Loss: 3.013054358288918 \n",
      "Epoch: 1165   Validation Loss: 12.300823826020011 \n",
      "Epoch: 1166   Training Loss: 2.866161409261366 \n",
      "Epoch: 1166   Validation Loss: 12.352515190890065 \n",
      "Epoch: 1167   Training Loss: 2.781795546746258 \n",
      "Epoch: 1167   Validation Loss: 12.737924743015345 \n",
      "Epoch: 1168   Training Loss: 2.680807991363487 \n",
      "Epoch: 1168   Validation Loss: 13.006533387762488 \n",
      "Epoch: 1169   Training Loss: 2.5644834778974497 \n",
      "Epoch: 1169   Validation Loss: 12.272044013782274 \n",
      "Epoch: 1170   Training Loss: 2.4780342239721853 \n",
      "Epoch: 1170   Validation Loss: 12.659180926389368 \n",
      "Epoch: 1171   Training Loss: 2.404091709869739 \n",
      "Epoch: 1171   Validation Loss: 12.359636196849877 \n",
      "Epoch: 1172   Training Loss: 2.3483538374108974 \n",
      "Epoch: 1172   Validation Loss: 12.308443997141856 \n",
      "Epoch: 1173   Training Loss: 2.3001244909982077 \n",
      "Epoch: 1173   Validation Loss: 12.623555153704727 \n",
      "Epoch: 1174   Training Loss: 2.277892678574119 \n",
      "Epoch: 1174   Validation Loss: 12.81650606368151 \n",
      "Epoch: 1175   Training Loss: 2.242177898886676 \n",
      "Epoch: 1175   Validation Loss: 13.08340371153443 \n",
      "Epoch: 1176   Training Loss: 2.2173861421507217 \n",
      "Epoch: 1176   Validation Loss: 12.930190850923498 \n",
      "Epoch: 1177   Training Loss: 2.2347397634136064 \n",
      "Epoch: 1177   Validation Loss: 12.914839964219524 \n",
      "Epoch: 1178   Training Loss: 2.121461385807486 \n",
      "Epoch: 1178   Validation Loss: 12.63073110283809 \n",
      "Epoch: 1179   Training Loss: 2.084225803576728 \n",
      "Epoch: 1179   Validation Loss: 12.568451916435164 \n",
      "Epoch: 1180   Training Loss: 2.0680697216879493 \n",
      "Epoch: 1180   Validation Loss: 12.632133751259797 \n",
      "Epoch: 1181   Training Loss: 1.9958950451339597 \n",
      "Epoch: 1181   Validation Loss: 12.733136367027285 \n",
      "Epoch: 1182   Training Loss: 1.972465215252345 \n",
      "Epoch: 1182   Validation Loss: 12.675982794119726 \n",
      "Epoch: 1183   Training Loss: 1.8920415176336844 \n",
      "Epoch: 1183   Validation Loss: 12.148492729113938 \n",
      "Epoch: 1184   Training Loss: 1.8322040806428406 \n",
      "Epoch: 1184   Validation Loss: 12.180033364486574 \n",
      "Epoch: 1185   Training Loss: 1.7936552414168494 \n",
      "Epoch: 1185   Validation Loss: 12.896267267908241 \n",
      "Epoch: 1186   Training Loss: 1.7659341936155712 \n",
      "Epoch: 1186   Validation Loss: 12.938104790129055 \n",
      "Epoch: 1187   Training Loss: 1.7884744622718471 \n",
      "Epoch: 1187   Validation Loss: 12.534300488301191 \n",
      "Epoch: 1188   Training Loss: 1.7598133911538856 \n",
      "Epoch: 1188   Validation Loss: 13.100347147625198 \n",
      "Epoch: 1189   Training Loss: 1.7013330808752394 \n",
      "Epoch: 1189   Validation Loss: 13.312684585829142 \n",
      "Epoch: 1190   Training Loss: 1.6591769700294474 \n",
      "Epoch: 1190   Validation Loss: 12.938146418208158 \n",
      "Epoch: 1191   Training Loss: 1.6622852827572285 \n",
      "Epoch: 1191   Validation Loss: 12.70848839332266 \n",
      "Epoch: 1192   Training Loss: 1.6381502056337889 \n",
      "Epoch: 1192   Validation Loss: 13.270865888439074 \n",
      "Epoch: 1193   Training Loss: 1.6090114151784694 \n",
      "Epoch: 1193   Validation Loss: 13.447140184617258 \n",
      "Epoch: 1194   Training Loss: 1.5936403598222715 \n",
      "Epoch: 1194   Validation Loss: 13.332396003584249 \n",
      "Epoch: 1195   Training Loss: 1.5974489009310824 \n",
      "Epoch: 1195   Validation Loss: 13.214089995080993 \n",
      "Epoch: 1196   Training Loss: 1.6214434975541632 \n",
      "Epoch: 1196   Validation Loss: 12.932530746883758 \n",
      "Epoch: 1197   Training Loss: 1.5855505776488923 \n",
      "Epoch: 1197   Validation Loss: 13.027041123332031 \n",
      "Epoch: 1198   Training Loss: 1.5503619864837483 \n",
      "Epoch: 1198   Validation Loss: 12.879855174384579 \n",
      "Epoch: 1199   Training Loss: 1.5839851320092508 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1199   Validation Loss: 12.873776771063064 \n",
      "Epoch: 1200   Training Loss: 1.5667263416353052 \n",
      "Epoch: 1200   Validation Loss: 12.973824763397696 \n",
      "Epoch: 1201   Training Loss: 1.5445803252508108 \n",
      "Epoch: 1201   Validation Loss: 12.75735128165039 \n",
      "Epoch: 1202   Training Loss: 1.5446354488996443 \n",
      "Epoch: 1202   Validation Loss: 12.812377917490776 \n",
      "Epoch: 1203   Training Loss: 1.4860143624655402 \n",
      "Epoch: 1203   Validation Loss: 12.818801179988142 \n",
      "Epoch: 1204   Training Loss: 1.4716999188641566 \n",
      "Epoch: 1204   Validation Loss: 12.428603153780465 \n",
      "Epoch: 1205   Training Loss: 1.490041285968743 \n",
      "Epoch: 1205   Validation Loss: 12.449157912623312 \n",
      "Epoch: 1206   Training Loss: 1.440923587326856 \n",
      "Epoch: 1206   Validation Loss: 12.648927251103089 \n",
      "Epoch: 1207   Training Loss: 1.468038661368919 \n",
      "Epoch: 1207   Validation Loss: 12.49580748110771 \n",
      "Epoch: 1208   Training Loss: 1.4584610098753992 \n",
      "Epoch: 1208   Validation Loss: 12.243272038957013 \n",
      "Epoch: 1209   Training Loss: 1.4447559255619669 \n",
      "Epoch: 1209   Validation Loss: 12.497589622789373 \n",
      "Epoch: 1210   Training Loss: 1.4306605970961144 \n",
      "Epoch: 1210   Validation Loss: 12.533586843551308 \n",
      "Epoch: 1211   Training Loss: 1.4130975328737025 \n",
      "Epoch: 1211   Validation Loss: 12.142208698633166 \n",
      "Epoch: 1212   Training Loss: 1.3907465906485157 \n",
      "Epoch: 1212   Validation Loss: 12.329745726241956 \n",
      "Epoch: 1213   Training Loss: 1.390015719442839 \n",
      "Epoch: 1213   Validation Loss: 12.387234870827607 \n",
      "Epoch: 1214   Training Loss: 1.405277585763807 \n",
      "Epoch: 1214   Validation Loss: 12.368187068109258 \n",
      "Epoch: 1215   Training Loss: 1.3962454746193915 \n",
      "Epoch: 1215   Validation Loss: 12.44694156847131 \n",
      "Epoch: 1216   Training Loss: 1.377163032935649 \n",
      "Epoch: 1216   Validation Loss: 12.23082034541444 \n",
      "Epoch: 1217   Training Loss: 1.3683450498051197 \n",
      "Epoch: 1217   Validation Loss: 12.208570055649782 \n",
      "Epoch: 1218   Training Loss: 1.335891625686482 \n",
      "Epoch: 1218   Validation Loss: 12.307470299844539 \n",
      "Epoch: 1219   Training Loss: 1.3499655794585557 \n",
      "Epoch: 1219   Validation Loss: 12.28319844933089 \n",
      "Epoch: 1220   Training Loss: 1.3131838425874358 \n",
      "Epoch: 1220   Validation Loss: 12.124375182448926 \n",
      "Epoch: 1221   Training Loss: 1.3004020491326465 \n",
      "Epoch: 1221   Validation Loss: 12.75998625208187 \n",
      "Epoch: 1222   Training Loss: 1.2993580510733458 \n",
      "Epoch: 1222   Validation Loss: 12.095876560133648 \n",
      "Epoch: 1223   Training Loss: 1.3231260011907575 \n",
      "Epoch: 1223   Validation Loss: 12.22564758196865 \n",
      "Epoch: 1224   Training Loss: 1.3198450455355035 \n",
      "Epoch: 1224   Validation Loss: 12.1259811184988 \n",
      "Epoch: 1225   Training Loss: 1.2937955139577637 \n",
      "Epoch: 1225   Validation Loss: 12.5108122076355 \n",
      "Epoch: 1226   Training Loss: 1.281868086171906 \n",
      "Epoch: 1226   Validation Loss: 12.101785366445915 \n",
      "Epoch: 1227   Training Loss: 1.2717729557828494 \n",
      "Epoch: 1227   Validation Loss: 12.14622111618351 \n",
      "Epoch: 1228   Training Loss: 1.3015059067065584 \n",
      "Epoch: 1228   Validation Loss: 12.135821660715415 \n",
      "Epoch: 1229   Training Loss: 1.2567069263686441 \n",
      "Epoch: 1229   Validation Loss: 12.199054420725831 \n",
      "Epoch: 1230   Training Loss: 1.2389136406078023 \n",
      "Epoch: 1230   Validation Loss: 12.215110317884314 \n",
      "Epoch: 1231   Training Loss: 1.2508017873046828 \n",
      "Epoch: 1231   Validation Loss: 12.05383189188978 \n",
      "Epoch: 1232   Training Loss: 1.2422762134705634 \n",
      "Epoch: 1232   Validation Loss: 12.006685332136403 \n",
      "Epoch: 1233   Training Loss: 1.2465838914226377 \n",
      "Epoch: 1233   Validation Loss: 11.965628822687961 \n",
      "Epoch: 1234   Training Loss: 1.2376316594866577 \n",
      "Epoch: 1234   Validation Loss: 11.829434326005194 \n",
      "Epoch: 1235   Training Loss: 1.2326951183160726 \n",
      "Epoch: 1235   Validation Loss: 12.109502890805665 \n",
      "Epoch: 1236   Training Loss: 1.2228583997025717 \n",
      "Epoch: 1236   Validation Loss: 12.04343430437407 \n",
      "Epoch: 1237   Training Loss: 1.2142800029275713 \n",
      "Epoch: 1237   Validation Loss: 12.051856978186635 \n",
      "Epoch: 1238   Training Loss: 1.1941813797968641 \n",
      "Epoch: 1238   Validation Loss: 12.018058787663383 \n",
      "Epoch: 1239   Training Loss: 1.1958525674451155 \n",
      "Epoch: 1239   Validation Loss: 12.117489883379552 \n",
      "Epoch: 1240   Training Loss: 1.220627352989933 \n",
      "Epoch: 1240   Validation Loss: 11.971866943602668 \n",
      "Epoch: 1241   Training Loss: 1.1983636757040046 \n",
      "Epoch: 1241   Validation Loss: 12.150868172707582 \n",
      "Epoch: 1242   Training Loss: 1.211748102238755 \n",
      "Epoch: 1242   Validation Loss: 11.878463497035877 \n",
      "Epoch: 1243   Training Loss: 1.2027021952737067 \n",
      "Epoch: 1243   Validation Loss: 11.88152527434912 \n",
      "Epoch: 1244   Training Loss: 1.1962500089602184 \n",
      "Epoch: 1244   Validation Loss: 12.010701374051965 \n",
      "Epoch: 1245   Training Loss: 1.1639357957738623 \n",
      "Epoch: 1245   Validation Loss: 12.134242330490409 \n",
      "Epoch: 1246   Training Loss: 1.1699477565054006 \n",
      "Epoch: 1246   Validation Loss: 12.165043573234527 \n",
      "Epoch: 1247   Training Loss: 1.1619125745495347 \n",
      "Epoch: 1247   Validation Loss: 12.043141043027285 \n",
      "Epoch: 1248   Training Loss: 1.1587382422892263 \n",
      "Epoch: 1248   Validation Loss: 11.887531862634475 \n",
      "Epoch: 1249   Training Loss: 1.1538274186640136 \n",
      "Epoch: 1249   Validation Loss: 12.316657670427642 \n",
      "Epoch: 1250   Training Loss: 1.1446589951670108 \n",
      "Epoch: 1250   Validation Loss: 12.441942551815707 \n",
      "Epoch: 1251   Training Loss: 1.155933878480046 \n",
      "Epoch: 1251   Validation Loss: 12.229579021014583 \n",
      "Epoch: 1252   Training Loss: 1.1421543096834161 \n",
      "Epoch: 1252   Validation Loss: 11.994662433955973 \n",
      "Epoch: 1253   Training Loss: 1.123557574095465 \n",
      "Epoch: 1253   Validation Loss: 12.175774357978758 \n",
      "Epoch: 1254   Training Loss: 1.1101955412669973 \n",
      "Epoch: 1254   Validation Loss: 12.086059888555598 \n",
      "Epoch: 1255   Training Loss: 1.0903954101134654 \n",
      "Epoch: 1255   Validation Loss: 12.220689455868538 \n",
      "Epoch: 1256   Training Loss: 1.1204033378937432 \n",
      "Epoch: 1256   Validation Loss: 12.349897414084447 \n",
      "Epoch: 1257   Training Loss: 1.0961875194947939 \n",
      "Epoch: 1257   Validation Loss: 12.253499758084175 \n",
      "Epoch: 1258   Training Loss: 1.102817231155377 \n",
      "Epoch: 1258   Validation Loss: 12.745489211700233 \n",
      "Epoch: 1259   Training Loss: 1.0757314833625684 \n",
      "Epoch: 1259   Validation Loss: 12.675733602730965 \n",
      "Epoch: 1260   Training Loss: 1.0451241185874893 \n",
      "Epoch: 1260   Validation Loss: 12.802636344837438 \n",
      "Epoch: 1261   Training Loss: 1.0456997522256328 \n",
      "Epoch: 1261   Validation Loss: 12.422047672157786 \n",
      "Epoch: 1262   Training Loss: 1.0430909558941293 \n",
      "Epoch: 1262   Validation Loss: 12.69372907093246 \n",
      "Epoch: 1263   Training Loss: 1.0559017261677646 \n",
      "Epoch: 1263   Validation Loss: 12.411723921588653 \n",
      "Epoch: 1264   Training Loss: 1.0491615325059518 \n",
      "Epoch: 1264   Validation Loss: 12.28020112957746 \n",
      "Epoch: 1265   Training Loss: 1.019878368036766 \n",
      "Epoch: 1265   Validation Loss: 12.530818277068889 \n",
      "Epoch: 1266   Training Loss: 1.0107979995247376 \n",
      "Epoch: 1266   Validation Loss: 12.471769965663357 \n",
      "Epoch: 1267   Training Loss: 0.9727400456234538 \n",
      "Epoch: 1267   Validation Loss: 12.6295368386729 \n",
      "Epoch: 1268   Training Loss: 0.9834473420722623 \n",
      "Epoch: 1268   Validation Loss: 12.166052549154465 \n",
      "Epoch: 1269   Training Loss: 1.0116360821880368 \n",
      "Epoch: 1269   Validation Loss: 12.497804096558973 \n",
      "Epoch: 1270   Training Loss: 0.9793584961556178 \n",
      "Epoch: 1270   Validation Loss: 12.51454231801522 \n",
      "Epoch: 1271   Training Loss: 0.9634090199444755 \n",
      "Epoch: 1271   Validation Loss: 12.544426738144313 \n",
      "Epoch: 1272   Training Loss: 0.9499674218255423 \n",
      "Epoch: 1272   Validation Loss: 12.474133505086018 \n",
      "Epoch: 1273   Training Loss: 0.9648805612368465 \n",
      "Epoch: 1273   Validation Loss: 12.560425577554616 \n",
      "Epoch: 1274   Training Loss: 0.9769009582801051 \n",
      "Epoch: 1274   Validation Loss: 12.355181650812536 \n",
      "Epoch: 1275   Training Loss: 1.0136154040090748 \n",
      "Epoch: 1275   Validation Loss: 12.506152035110636 \n",
      "Epoch: 1276   Training Loss: 0.9781850122304006 \n",
      "Epoch: 1276   Validation Loss: 12.332576583947352 \n",
      "Epoch: 1277   Training Loss: 0.9573815403674855 \n",
      "Epoch: 1277   Validation Loss: 12.576482636537824 \n",
      "Epoch: 1278   Training Loss: 0.9389498602958033 \n",
      "Epoch: 1278   Validation Loss: 12.381826325747365 \n",
      "Epoch: 1279   Training Loss: 0.9050410089981219 \n",
      "Epoch: 1279   Validation Loss: 12.396627814826557 \n",
      "Epoch: 1280   Training Loss: 0.9039728090339019 \n",
      "Epoch: 1280   Validation Loss: 12.262078453913107 \n",
      "Epoch: 1281   Training Loss: 0.9225099753087322 \n",
      "Epoch: 1281   Validation Loss: 12.28851661865293 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1282   Training Loss: 0.9020484410490933 \n",
      "Epoch: 1282   Validation Loss: 12.22887662999669 \n",
      "Epoch: 1283   Training Loss: 0.9130688801479565 \n",
      "Epoch: 1283   Validation Loss: 12.268066294508909 \n",
      "Epoch: 1284   Training Loss: 0.8989694625227871 \n",
      "Epoch: 1284   Validation Loss: 12.046853919913895 \n",
      "Epoch: 1285   Training Loss: 0.8927588285388991 \n",
      "Epoch: 1285   Validation Loss: 12.140959097453209 \n",
      "Epoch: 1286   Training Loss: 0.8984184591132067 \n",
      "Epoch: 1286   Validation Loss: 12.063693856808609 \n",
      "Epoch: 1287   Training Loss: 0.8953013842145601 \n",
      "Epoch: 1287   Validation Loss: 11.972609215752001 \n",
      "Epoch: 1288   Training Loss: 0.8721615567092373 \n",
      "Epoch: 1288   Validation Loss: 12.131041852227877 \n",
      "Epoch: 1289   Training Loss: 0.8624805778037481 \n",
      "Epoch: 1289   Validation Loss: 12.088680622747594 \n",
      "Epoch: 1290   Training Loss: 0.8538415878930886 \n",
      "Epoch: 1290   Validation Loss: 12.367363878699994 \n",
      "Epoch: 1291   Training Loss: 0.8604144904606944 \n",
      "Epoch: 1291   Validation Loss: 11.995135473312079 \n",
      "Epoch: 1292   Training Loss: 0.8818693842506472 \n",
      "Epoch: 1292   Validation Loss: 12.026138419484068 \n",
      "Epoch: 1293   Training Loss: 0.8659128171128262 \n",
      "Epoch: 1293   Validation Loss: 11.766373294782358 \n",
      "Epoch: 1294   Training Loss: 0.8489256467761019 \n",
      "Epoch: 1294   Validation Loss: 12.144368055366975 \n",
      "Epoch: 1295   Training Loss: 0.8270366439901495 \n",
      "Epoch: 1295   Validation Loss: 12.108608507102364 \n",
      "Epoch: 1296   Training Loss: 0.8136592773199459 \n",
      "Epoch: 1296   Validation Loss: 12.07586293955948 \n",
      "Epoch: 1297   Training Loss: 0.8146123276667234 \n",
      "Epoch: 1297   Validation Loss: 12.287815055894582 \n",
      "Epoch: 1298   Training Loss: 0.8017369315426435 \n",
      "Epoch: 1298   Validation Loss: 12.473470642947245 \n",
      "Epoch: 1299   Training Loss: 0.8212876672974315 \n",
      "Epoch: 1299   Validation Loss: 12.42022153531359 \n",
      "Epoch: 1300   Training Loss: 0.8282962945424491 \n",
      "Epoch: 1300   Validation Loss: 12.19714872064455 \n",
      "Epoch: 1301   Training Loss: 0.8410928727927223 \n",
      "Epoch: 1301   Validation Loss: 12.177612533102186 \n",
      "Epoch: 1302   Training Loss: 0.8375607321689608 \n",
      "Epoch: 1302   Validation Loss: 12.308474443537959 \n",
      "Epoch: 1303   Training Loss: 0.8357089079710623 \n",
      "Epoch: 1303   Validation Loss: 12.423765720911371 \n",
      "Epoch: 1304   Training Loss: 0.8198057532325557 \n",
      "Epoch: 1304   Validation Loss: 12.65718877671124 \n",
      "Epoch: 1305   Training Loss: 0.7941989309746422 \n",
      "Epoch: 1305   Validation Loss: 12.74245738699174 \n",
      "Epoch: 1306   Training Loss: 0.782186787868206 \n",
      "Epoch: 1306   Validation Loss: 12.523289340460105 \n",
      "Epoch: 1307   Training Loss: 0.7642772699243044 \n",
      "Epoch: 1307   Validation Loss: 12.29112580817718 \n",
      "Epoch: 1308   Training Loss: 0.7572184435042394 \n",
      "Epoch: 1308   Validation Loss: 12.43581797829725 \n",
      "Epoch: 1309   Training Loss: 0.7985503493798632 \n",
      "Epoch: 1309   Validation Loss: 12.42652306388401 \n",
      "Epoch: 1310   Training Loss: 0.8169282777768193 \n",
      "Epoch: 1310   Validation Loss: 12.268980731489266 \n",
      "Epoch: 1311   Training Loss: 0.786771245011355 \n",
      "Epoch: 1311   Validation Loss: 12.43701857187242 \n",
      "Epoch: 1312   Training Loss: 0.7974159788276753 \n",
      "Epoch: 1312   Validation Loss: 12.296954033585822 \n",
      "Epoch: 1313   Training Loss: 0.787813964289932 \n",
      "Epoch: 1313   Validation Loss: 12.196668427446397 \n",
      "Epoch: 1314   Training Loss: 0.7974016315875765 \n",
      "Epoch: 1314   Validation Loss: 12.483434136119257 \n",
      "Epoch: 1315   Training Loss: 0.771196395956307 \n",
      "Epoch: 1315   Validation Loss: 12.361571049873705 \n",
      "Epoch: 1316   Training Loss: 0.7611335655717228 \n",
      "Epoch: 1316   Validation Loss: 12.361427077855355 \n",
      "Epoch: 1317   Training Loss: 0.7416717174650639 \n",
      "Epoch: 1317   Validation Loss: 12.320395704898692 \n",
      "Epoch: 1318   Training Loss: 0.7231822450308167 \n",
      "Epoch: 1318   Validation Loss: 12.375088072734885 \n",
      "Epoch: 1319   Training Loss: 0.7323124371591095 \n",
      "Epoch: 1319   Validation Loss: 12.276967292510713 \n",
      "Epoch: 1320   Training Loss: 0.7421687572884956 \n",
      "Epoch: 1320   Validation Loss: 12.425232179092502 \n",
      "Epoch: 1321   Training Loss: 0.7532525721146591 \n",
      "Epoch: 1321   Validation Loss: 12.299735648278874 \n",
      "Epoch: 1322   Training Loss: 0.7436231479065434 \n",
      "Epoch: 1322   Validation Loss: 12.258525291684034 \n",
      "Epoch: 1323   Training Loss: 0.7384251462060093 \n",
      "Epoch: 1323   Validation Loss: 12.47571165737644 \n",
      "Epoch: 1324   Training Loss: 0.7374274909527067 \n",
      "Epoch: 1324   Validation Loss: 12.252567122454632 \n",
      "Epoch: 1325   Training Loss: 0.7165654998889021 \n",
      "Epoch: 1325   Validation Loss: 12.592511023984802 \n",
      "Epoch: 1326   Training Loss: 0.7171128667432214 \n",
      "Epoch: 1326   Validation Loss: 12.190619361816466 \n",
      "Epoch: 1327   Training Loss: 0.7142760556833075 \n",
      "Epoch: 1327   Validation Loss: 12.152251475899329 \n",
      "Epoch: 1328   Training Loss: 0.6895496880884409 \n",
      "Epoch: 1328   Validation Loss: 12.31150482800781 \n",
      "Epoch: 1329   Training Loss: 0.6894335363582914 \n",
      "Epoch: 1329   Validation Loss: 12.338315653471062 \n",
      "Epoch: 1330   Training Loss: 0.69902559272691 \n",
      "Epoch: 1330   Validation Loss: 12.132665657775632 \n",
      "Epoch: 1331   Training Loss: 0.6959233526894372 \n",
      "Epoch: 1331   Validation Loss: 12.305727136160792 \n",
      "Epoch: 1332   Training Loss: 0.6833662951383589 \n",
      "Epoch: 1332   Validation Loss: 12.378970679824693 \n",
      "Epoch: 1333   Training Loss: 0.7027319874135705 \n",
      "Epoch: 1333   Validation Loss: 12.1868846649266 \n",
      "Epoch: 1334   Training Loss: 0.7054928605488017 \n",
      "Epoch: 1334   Validation Loss: 12.31524172261491 \n",
      "Epoch: 1335   Training Loss: 0.706641344672472 \n",
      "Epoch: 1335   Validation Loss: 12.32134075843968 \n",
      "Epoch: 1336   Training Loss: 0.6830316932971199 \n",
      "Epoch: 1336   Validation Loss: 12.473375709649208 \n",
      "Epoch: 1337   Training Loss: 0.6905985012875311 \n",
      "Epoch: 1337   Validation Loss: 12.22204841908475 \n",
      "Epoch: 1338   Training Loss: 0.6623965475851433 \n",
      "Epoch: 1338   Validation Loss: 12.481639102279326 \n",
      "Epoch: 1339   Training Loss: 0.6752120515467378 \n",
      "Epoch: 1339   Validation Loss: 12.415598435829342 \n",
      "Epoch: 1340   Training Loss: 0.6758017405320832 \n",
      "Epoch: 1340   Validation Loss: 12.268269926151312 \n",
      "Epoch: 1341   Training Loss: 0.6711614048427639 \n",
      "Epoch: 1341   Validation Loss: 12.529593248328782 \n",
      "Epoch: 1342   Training Loss: 0.6612757005727472 \n",
      "Epoch: 1342   Validation Loss: 12.328750983834173 \n",
      "Epoch: 1343   Training Loss: 0.6655121209310724 \n",
      "Epoch: 1343   Validation Loss: 12.355175058607978 \n",
      "Epoch: 1344   Training Loss: 0.6575751731474381 \n",
      "Epoch: 1344   Validation Loss: 12.346709199196853 \n",
      "Epoch: 1345   Training Loss: 0.6608988341234011 \n",
      "Epoch: 1345   Validation Loss: 12.373682316009194 \n",
      "Epoch: 1346   Training Loss: 0.6586086033204365 \n",
      "Epoch: 1346   Validation Loss: 12.100314097637542 \n",
      "Epoch: 1347   Training Loss: 0.6487764717598313 \n",
      "Epoch: 1347   Validation Loss: 12.217073602936027 \n",
      "Epoch: 1348   Training Loss: 0.6532184326045265 \n",
      "Epoch: 1348   Validation Loss: 12.298159954960923 \n",
      "Epoch: 1349   Training Loss: 0.6665297225984277 \n",
      "Epoch: 1349   Validation Loss: 12.371797948128428 \n",
      "Epoch: 1350   Training Loss: 0.667875519577874 \n",
      "Epoch: 1350   Validation Loss: 12.475551111115724 \n",
      "Epoch: 1351   Training Loss: 0.6715247597435604 \n",
      "Epoch: 1351   Validation Loss: 12.219024256229744 \n",
      "Epoch: 1352   Training Loss: 0.6722085416198149 \n",
      "Epoch: 1352   Validation Loss: 12.487162141224667 \n",
      "Epoch: 1353   Training Loss: 0.6665612496181993 \n",
      "Epoch: 1353   Validation Loss: 12.189809758277509 \n",
      "Epoch: 1354   Training Loss: 0.6374951353295304 \n",
      "Epoch: 1354   Validation Loss: 12.171526502079402 \n",
      "Epoch: 1355   Training Loss: 0.6127507608425091 \n",
      "Epoch: 1355   Validation Loss: 12.291715542888745 \n",
      "Epoch: 1356   Training Loss: 0.6183476167185311 \n",
      "Epoch: 1356   Validation Loss: 12.334737641772948 \n",
      "Epoch: 1357   Training Loss: 0.6292096191894789 \n",
      "Epoch: 1357   Validation Loss: 12.424235578818013 \n",
      "Epoch: 1358   Training Loss: 0.6248235828547574 \n",
      "Epoch: 1358   Validation Loss: 12.280638702543548 \n",
      "Epoch: 1359   Training Loss: 0.5941165791117847 \n",
      "Epoch: 1359   Validation Loss: 12.300702043891704 \n",
      "Epoch: 1360   Training Loss: 0.5929655619740078 \n",
      "Epoch: 1360   Validation Loss: 12.450950755323872 \n",
      "Epoch: 1361   Training Loss: 0.5965337982986194 \n",
      "Epoch: 1361   Validation Loss: 12.217291031457238 \n",
      "Epoch: 1362   Training Loss: 0.607239097364823 \n",
      "Epoch: 1362   Validation Loss: 12.339887601816454 \n",
      "Epoch: 1363   Training Loss: 0.5979782769522004 \n",
      "Epoch: 1363   Validation Loss: 12.607419326857716 \n",
      "Epoch: 1364   Training Loss: 0.6046100936819371 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1364   Validation Loss: 12.12036752218942 \n",
      "Epoch: 1365   Training Loss: 0.5946496691238707 \n",
      "Epoch: 1365   Validation Loss: 12.477105084470667 \n",
      "Epoch: 1366   Training Loss: 0.5848101996836444 \n",
      "Epoch: 1366   Validation Loss: 12.343127807858341 \n",
      "Epoch: 1367   Training Loss: 0.5744482324548905 \n",
      "Epoch: 1367   Validation Loss: 12.570287237501654 \n",
      "Epoch: 1368   Training Loss: 0.5820387238677247 \n",
      "Epoch: 1368   Validation Loss: 12.513449001113859 \n",
      "Epoch: 1369   Training Loss: 0.5748443813054986 \n",
      "Epoch: 1369   Validation Loss: 12.738910667240331 \n",
      "Epoch: 1370   Training Loss: 0.571275252083028 \n",
      "Epoch: 1370   Validation Loss: 12.224190244010378 \n",
      "Epoch: 1371   Training Loss: 0.5865628979636468 \n",
      "Epoch: 1371   Validation Loss: 12.326066843978627 \n",
      "Epoch: 1372   Training Loss: 0.5753379098671975 \n",
      "Epoch: 1372   Validation Loss: 12.264742030653442 \n",
      "Epoch: 1373   Training Loss: 0.5775915035542785 \n",
      "Epoch: 1373   Validation Loss: 12.610412475835757 \n",
      "Epoch: 1374   Training Loss: 0.5871625431624462 \n",
      "Epoch: 1374   Validation Loss: 12.156086318095888 \n",
      "Epoch: 1375   Training Loss: 0.5851193721592712 \n",
      "Epoch: 1375   Validation Loss: 12.522242609976697 \n",
      "Epoch: 1376   Training Loss: 0.5786330588037781 \n",
      "Epoch: 1376   Validation Loss: 12.609706413933122 \n",
      "Epoch: 1377   Training Loss: 0.5765269779741377 \n",
      "Epoch: 1377   Validation Loss: 12.243315233376533 \n",
      "Epoch: 1378   Training Loss: 0.5675265956366958 \n",
      "Epoch: 1378   Validation Loss: 12.321678708017842 \n",
      "Epoch: 1379   Training Loss: 0.5731036118605389 \n",
      "Epoch: 1379   Validation Loss: 12.424663841510046 \n",
      "Epoch: 1380   Training Loss: 0.5851316645552147 \n",
      "Epoch: 1380   Validation Loss: 12.482387890363551 \n",
      "Epoch: 1381   Training Loss: 0.59333706564208 \n",
      "Epoch: 1381   Validation Loss: 12.446159743638205 \n",
      "Epoch: 1382   Training Loss: 0.5846193936366127 \n",
      "Epoch: 1382   Validation Loss: 12.287815824975725 \n",
      "Epoch: 1383   Training Loss: 0.5653623215700059 \n",
      "Epoch: 1383   Validation Loss: 12.571963652334594 \n",
      "Epoch: 1384   Training Loss: 0.5600394526274741 \n",
      "Epoch: 1384   Validation Loss: 12.425160709201966 \n",
      "Epoch: 1385   Training Loss: 0.5487179200674566 \n",
      "Epoch: 1385   Validation Loss: 12.471889974216646 \n",
      "Epoch: 1386   Training Loss: 0.5436027758757163 \n",
      "Epoch: 1386   Validation Loss: 12.330522167937957 \n",
      "Epoch: 1387   Training Loss: 0.5414615512229126 \n",
      "Epoch: 1387   Validation Loss: 12.514446679235203 \n",
      "Epoch: 1388   Training Loss: 0.5474502837356247 \n",
      "Epoch: 1388   Validation Loss: 12.510672778207189 \n",
      "Epoch: 1389   Training Loss: 0.5422944238632981 \n",
      "Epoch: 1389   Validation Loss: 12.47178493174915 \n",
      "Epoch: 1390   Training Loss: 0.5364913272767105 \n",
      "Epoch: 1390   Validation Loss: 12.383426487333606 \n",
      "Epoch: 1391   Training Loss: 0.5221555610731681 \n",
      "Epoch: 1391   Validation Loss: 12.493909390297505 \n",
      "Epoch: 1392   Training Loss: 0.544927950064767 \n",
      "Epoch: 1392   Validation Loss: 12.087194187043373 \n",
      "Epoch: 1393   Training Loss: 0.5742605217697518 \n",
      "Epoch: 1393   Validation Loss: 12.40517001090839 \n",
      "Epoch: 1394   Training Loss: 0.5506037343090486 \n",
      "Epoch: 1394   Validation Loss: 12.131889490370147 \n",
      "Epoch: 1395   Training Loss: 0.5331674412642942 \n",
      "Epoch: 1395   Validation Loss: 12.072671312473766 \n",
      "Epoch: 1396   Training Loss: 0.5179562183827469 \n",
      "Epoch: 1396   Validation Loss: 12.254206520418935 \n",
      "Epoch: 1397   Training Loss: 0.5259195785404511 \n",
      "Epoch: 1397   Validation Loss: 12.253332803678273 \n",
      "Epoch: 1398   Training Loss: 0.52025719723552 \n",
      "Epoch: 1398   Validation Loss: 12.267218381827904 \n",
      "Epoch: 1399   Training Loss: 0.4983809544732455 \n",
      "Epoch: 1399   Validation Loss: 12.319929197715334 \n",
      "Epoch: 1400   Training Loss: 0.49109570150562337 \n",
      "Epoch: 1400   Validation Loss: 12.28271568347091 \n",
      "Epoch: 1401   Training Loss: 0.4928513435931491 \n",
      "Epoch: 1401   Validation Loss: 12.343820159220234 \n",
      "Epoch: 1402   Training Loss: 0.4857500316952722 \n",
      "Epoch: 1402   Validation Loss: 12.432737213750004 \n",
      "Epoch: 1403   Training Loss: 0.4955696115941462 \n",
      "Epoch: 1403   Validation Loss: 12.427586183533396 \n",
      "Epoch: 1404   Training Loss: 0.500887474062223 \n",
      "Epoch: 1404   Validation Loss: 12.31255465167321 \n",
      "Epoch: 1405   Training Loss: 0.49595576180629625 \n",
      "Epoch: 1405   Validation Loss: 12.240813079472922 \n",
      "Epoch: 1406   Training Loss: 0.4897805657896653 \n",
      "Epoch: 1406   Validation Loss: 12.405208173135476 \n",
      "Epoch: 1407   Training Loss: 0.49504425016451425 \n",
      "Epoch: 1407   Validation Loss: 12.333820194391198 \n",
      "Epoch: 1408   Training Loss: 0.49675768089651684 \n",
      "Epoch: 1408   Validation Loss: 12.150963077589669 \n",
      "Epoch: 1409   Training Loss: 0.48281317105920546 \n",
      "Epoch: 1409   Validation Loss: 12.216308434057153 \n",
      "Epoch: 1410   Training Loss: 0.49918810106910716 \n",
      "Epoch: 1410   Validation Loss: 12.230332351108627 \n",
      "Epoch: 1411   Training Loss: 0.5073831716595683 \n",
      "Epoch: 1411   Validation Loss: 12.304564512500306 \n",
      "Epoch: 1412   Training Loss: 0.5363491921269096 \n",
      "Epoch: 1412   Validation Loss: 11.95900990151883 \n",
      "Epoch: 1413   Training Loss: 0.5278787436960908 \n",
      "Epoch: 1413   Validation Loss: 12.292682880507575 \n",
      "Epoch: 1414   Training Loss: 0.5016029731685915 \n",
      "Epoch: 1414   Validation Loss: 12.130929816469195 \n",
      "Epoch: 1415   Training Loss: 0.47304729180937627 \n",
      "Epoch: 1415   Validation Loss: 12.318150271616256 \n",
      "Epoch: 1416   Training Loss: 0.45808559773921176 \n",
      "Epoch: 1416   Validation Loss: 12.32837121901518 \n",
      "Epoch: 1417   Training Loss: 0.46449085359784437 \n",
      "Epoch: 1417   Validation Loss: 12.198364209915368 \n",
      "Epoch: 1418   Training Loss: 0.44891229886489303 \n",
      "Epoch: 1418   Validation Loss: 12.457014299745461 \n",
      "Epoch: 1419   Training Loss: 0.44559030545335165 \n",
      "Epoch: 1419   Validation Loss: 12.440754294311738 \n",
      "Epoch: 1420   Training Loss: 0.4559165726162304 \n",
      "Epoch: 1420   Validation Loss: 12.348313489123319 \n",
      "Epoch: 1421   Training Loss: 0.4734266346330574 \n",
      "Epoch: 1421   Validation Loss: 12.43377167627024 \n",
      "Epoch: 1422   Training Loss: 0.4717594040695307 \n",
      "Epoch: 1422   Validation Loss: 12.257393724454928 \n",
      "Epoch: 1423   Training Loss: 0.46266967538279963 \n",
      "Epoch: 1423   Validation Loss: 12.493976539811985 \n",
      "Epoch: 1424   Training Loss: 0.46603818923495743 \n",
      "Epoch: 1424   Validation Loss: 12.657007018535799 \n",
      "Epoch: 1425   Training Loss: 0.4700638826647473 \n",
      "Epoch: 1425   Validation Loss: 12.336298952252966 \n",
      "Epoch: 1426   Training Loss: 0.47151626534520175 \n",
      "Epoch: 1426   Validation Loss: 12.532149174197862 \n",
      "Epoch: 1427   Training Loss: 0.4668650047642617 \n",
      "Epoch: 1427   Validation Loss: 12.294615948797107 \n",
      "Epoch: 1428   Training Loss: 0.4575839978102546 \n",
      "Epoch: 1428   Validation Loss: 12.250875535097427 \n",
      "Epoch: 1429   Training Loss: 0.44871957989489686 \n",
      "Epoch: 1429   Validation Loss: 12.35098307661433 \n",
      "Epoch: 1430   Training Loss: 0.44534062313697875 \n",
      "Epoch: 1430   Validation Loss: 12.438999745963368 \n",
      "Epoch: 1431   Training Loss: 0.4467397994379708 \n",
      "Epoch: 1431   Validation Loss: 12.310897353882332 \n",
      "Epoch: 1432   Training Loss: 0.43459824524762586 \n",
      "Epoch: 1432   Validation Loss: 12.52949213863538 \n",
      "Epoch: 1433   Training Loss: 0.43571031058122855 \n",
      "Epoch: 1433   Validation Loss: 12.514724126749297 \n",
      "Epoch: 1434   Training Loss: 0.4418089633798456 \n",
      "Epoch: 1434   Validation Loss: 12.614561793098265 \n",
      "Epoch: 1435   Training Loss: 0.4422097222235484 \n",
      "Epoch: 1435   Validation Loss: 12.538014397182573 \n",
      "Epoch: 1436   Training Loss: 0.4487538683902943 \n",
      "Epoch: 1436   Validation Loss: 12.490990256130683 \n",
      "Epoch: 1437   Training Loss: 0.4492738887565057 \n",
      "Epoch: 1437   Validation Loss: 12.676411531946435 \n",
      "Epoch: 1438   Training Loss: 0.44226827621902326 \n",
      "Epoch: 1438   Validation Loss: 12.408328391626714 \n",
      "Epoch: 1439   Training Loss: 0.4400743057061752 \n",
      "Epoch: 1439   Validation Loss: 12.35672778644003 \n",
      "Epoch: 1440   Training Loss: 0.4339840812976021 \n",
      "Epoch: 1440   Validation Loss: 12.332585595034011 \n",
      "Epoch: 1441   Training Loss: 0.4398831568174303 \n",
      "Epoch: 1441   Validation Loss: 12.584354150407753 \n",
      "Epoch: 1442   Training Loss: 0.46219841634293013 \n",
      "Epoch: 1442   Validation Loss: 12.472706648304765 \n",
      "Epoch: 1443   Training Loss: 0.44696609562014744 \n",
      "Epoch: 1443   Validation Loss: 12.541667189387274 \n",
      "Epoch: 1444   Training Loss: 0.437228146326892 \n",
      "Epoch: 1444   Validation Loss: 12.271471599263256 \n",
      "Epoch: 1445   Training Loss: 0.4308887498155452 \n",
      "Epoch: 1445   Validation Loss: 12.23608983489802 \n",
      "Epoch: 1446   Training Loss: 0.4229582696801017 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1446   Validation Loss: 12.431824647898884 \n",
      "Epoch: 1447   Training Loss: 0.41750475214377264 \n",
      "Epoch: 1447   Validation Loss: 12.339910741681518 \n",
      "Epoch: 1448   Training Loss: 0.41963472674271324 \n",
      "Epoch: 1448   Validation Loss: 12.595481435514984 \n",
      "Epoch: 1449   Training Loss: 0.4159391638127211 \n",
      "Epoch: 1449   Validation Loss: 12.609722544363272 \n",
      "Epoch: 1450   Training Loss: 0.4127194814044828 \n",
      "Epoch: 1450   Validation Loss: 12.513107971964972 \n",
      "Epoch: 1451   Training Loss: 0.4142852082496374 \n",
      "Epoch: 1451   Validation Loss: 12.639126772840715 \n",
      "Epoch: 1452   Training Loss: 0.4308059490805453 \n",
      "Epoch: 1452   Validation Loss: 12.561556137399206 \n",
      "Epoch: 1453   Training Loss: 0.41351141468590497 \n",
      "Epoch: 1453   Validation Loss: 12.782931579363103 \n",
      "Epoch: 1454   Training Loss: 0.40972043159298144 \n",
      "Epoch: 1454   Validation Loss: 12.474515000172612 \n",
      "Epoch: 1455   Training Loss: 0.40264459871261127 \n",
      "Epoch: 1455   Validation Loss: 12.575867920084404 \n",
      "Epoch: 1456   Training Loss: 0.41472992724483904 \n",
      "Epoch: 1456   Validation Loss: 12.442823370193448 \n",
      "Epoch: 1457   Training Loss: 0.4269325928835794 \n",
      "Epoch: 1457   Validation Loss: 12.621535360589041 \n",
      "Epoch: 1458   Training Loss: 0.4168171624063741 \n",
      "Epoch: 1458   Validation Loss: 12.534329081312263 \n",
      "Epoch: 1459   Training Loss: 0.41024891082153886 \n",
      "Epoch: 1459   Validation Loss: 12.35565971030575 \n",
      "Epoch: 1460   Training Loss: 0.4025283442490158 \n",
      "Epoch: 1460   Validation Loss: 12.472093947116427 \n",
      "Epoch: 1461   Training Loss: 0.39723252718187924 \n",
      "Epoch: 1461   Validation Loss: 12.443382413653488 \n",
      "Epoch: 1462   Training Loss: 0.40194122025932394 \n",
      "Epoch: 1462   Validation Loss: 12.37897915764346 \n",
      "Epoch: 1463   Training Loss: 0.4035987454713991 \n",
      "Epoch: 1463   Validation Loss: 12.457738219350654 \n",
      "Epoch: 1464   Training Loss: 0.3972018631350982 \n",
      "Epoch: 1464   Validation Loss: 12.696105533536091 \n",
      "Epoch: 1465   Training Loss: 0.39791578595724 \n",
      "Epoch: 1465   Validation Loss: 12.244566311267256 \n",
      "Epoch: 1466   Training Loss: 0.399493888565891 \n",
      "Epoch: 1466   Validation Loss: 12.476240301480168 \n",
      "Epoch: 1467   Training Loss: 0.4146113724080391 \n",
      "Epoch: 1467   Validation Loss: 12.509891669406175 \n",
      "Epoch: 1468   Training Loss: 0.422212016888846 \n",
      "Epoch: 1468   Validation Loss: 12.479877101895978 \n",
      "Epoch: 1469   Training Loss: 0.4016499761954018 \n",
      "Epoch: 1469   Validation Loss: 12.600983149262767 \n",
      "Epoch: 1470   Training Loss: 0.3895246043974657 \n",
      "Epoch: 1470   Validation Loss: 12.634355679262535 \n",
      "Epoch: 1471   Training Loss: 0.38570713061480993 \n",
      "Epoch: 1471   Validation Loss: 12.614188324675979 \n",
      "Epoch: 1472   Training Loss: 0.41532448126520494 \n",
      "Epoch: 1472   Validation Loss: 12.509523873374945 \n",
      "Epoch: 1473   Training Loss: 0.39665631832055526 \n",
      "Epoch: 1473   Validation Loss: 12.446244848523502 \n",
      "Epoch: 1474   Training Loss: 0.38657523123765797 \n",
      "Epoch: 1474   Validation Loss: 12.394175808701716 \n",
      "Epoch: 1475   Training Loss: 0.3891255003672365 \n",
      "Epoch: 1475   Validation Loss: 12.376284695711087 \n",
      "Epoch: 1476   Training Loss: 0.39570951803708665 \n",
      "Epoch: 1476   Validation Loss: 12.296164999023002 \n",
      "Epoch: 1477   Training Loss: 0.4025487098689857 \n",
      "Epoch: 1477   Validation Loss: 12.380582237999931 \n",
      "Epoch: 1478   Training Loss: 0.3987561817683367 \n",
      "Epoch: 1478   Validation Loss: 12.451440808378386 \n",
      "Epoch: 1479   Training Loss: 0.38448379980392977 \n",
      "Epoch: 1479   Validation Loss: 12.479213598891963 \n",
      "Epoch: 1480   Training Loss: 0.3729162744654686 \n",
      "Epoch: 1480   Validation Loss: 12.566563167793333 \n",
      "Epoch: 1481   Training Loss: 0.3782348854248998 \n",
      "Epoch: 1481   Validation Loss: 12.500030356875639 \n",
      "Epoch: 1482   Training Loss: 0.3911293292961444 \n",
      "Epoch: 1482   Validation Loss: 12.530439958800535 \n",
      "Epoch: 1483   Training Loss: 0.4006598474256508 \n",
      "Epoch: 1483   Validation Loss: 12.489398263086073 \n",
      "Epoch: 1484   Training Loss: 0.39469018966986674 \n",
      "Epoch: 1484   Validation Loss: 12.447426116595924 \n",
      "Epoch: 1485   Training Loss: 0.38723794524926924 \n",
      "Epoch: 1485   Validation Loss: 12.442558020171836 \n",
      "Epoch: 1486   Training Loss: 0.37778397261637525 \n",
      "Epoch: 1486   Validation Loss: 12.336129496241915 \n",
      "Epoch: 1487   Training Loss: 0.39167231754884313 \n",
      "Epoch: 1487   Validation Loss: 12.47221298166367 \n",
      "Epoch: 1488   Training Loss: 0.4023302198662361 \n",
      "Epoch: 1488   Validation Loss: 12.50270362769322 \n",
      "Epoch: 1489   Training Loss: 0.4074771289446 \n",
      "Epoch: 1489   Validation Loss: 12.479846059913822 \n",
      "Epoch: 1490   Training Loss: 0.40037159921320226 \n",
      "Epoch: 1490   Validation Loss: 12.594068435059466 \n",
      "Epoch: 1491   Training Loss: 0.39119312578037996 \n",
      "Epoch: 1491   Validation Loss: 12.30852258285201 \n",
      "Epoch: 1492   Training Loss: 0.3731215861212468 \n",
      "Epoch: 1492   Validation Loss: 12.337850896516517 \n",
      "Epoch: 1493   Training Loss: 0.3702330406463725 \n",
      "Epoch: 1493   Validation Loss: 12.516320116931029 \n",
      "Epoch: 1494   Training Loss: 0.39903865864944954 \n",
      "Epoch: 1494   Validation Loss: 12.439287183257619 \n",
      "Epoch: 1495   Training Loss: 0.39432100013622307 \n",
      "Epoch: 1495   Validation Loss: 12.343385963629858 \n",
      "Epoch: 1496   Training Loss: 0.3821663587376021 \n",
      "Epoch: 1496   Validation Loss: 12.487397607548576 \n",
      "Epoch: 1497   Training Loss: 0.3675519305587198 \n",
      "Epoch: 1497   Validation Loss: 12.422143230865128 \n",
      "Epoch: 1498   Training Loss: 0.3596424019168887 \n",
      "Epoch: 1498   Validation Loss: 12.509129872726936 \n",
      "Epoch: 1499   Training Loss: 0.3617002173692462 \n",
      "Epoch: 1499   Validation Loss: 12.269383282572235 \n",
      "Epoch: 1500   Training Loss: 0.364375108442292 \n",
      "Epoch: 1500   Validation Loss: 12.181810751227468 \n",
      "Epoch: 1501   Training Loss: 0.39012778958108474 \n",
      "Epoch: 1501   Validation Loss: 12.311087574319677 \n",
      "Epoch: 1502   Training Loss: 0.38626611153107904 \n",
      "Epoch: 1502   Validation Loss: 12.273029537341584 \n",
      "Epoch: 1503   Training Loss: 0.37473765717909535 \n",
      "Epoch: 1503   Validation Loss: 12.38079322651797 \n",
      "Epoch: 1504   Training Loss: 0.3688408242331685 \n",
      "Epoch: 1504   Validation Loss: 12.336496004337956 \n",
      "Epoch: 1505   Training Loss: 0.3607639665989814 \n",
      "Epoch: 1505   Validation Loss: 12.250202338510858 \n",
      "Epoch: 1506   Training Loss: 0.36597464899247595 \n",
      "Epoch: 1506   Validation Loss: 12.401405855445521 \n",
      "Epoch: 1507   Training Loss: 0.36125666052124383 \n",
      "Epoch: 1507   Validation Loss: 12.216812228918476 \n",
      "Epoch: 1508   Training Loss: 0.3698074636953162 \n",
      "Epoch: 1508   Validation Loss: 12.257528813427223 \n",
      "Epoch: 1509   Training Loss: 0.36930340082771634 \n",
      "Epoch: 1509   Validation Loss: 12.221090067087077 \n",
      "Epoch: 1510   Training Loss: 0.38631204420182336 \n",
      "Epoch: 1510   Validation Loss: 12.207785998690861 \n",
      "Epoch: 1511   Training Loss: 0.3758825102822573 \n",
      "Epoch: 1511   Validation Loss: 12.436368288292497 \n",
      "Epoch: 1512   Training Loss: 0.3659585472834606 \n",
      "Epoch: 1512   Validation Loss: 12.195065884538606 \n",
      "Epoch: 1513   Training Loss: 0.35100280777182385 \n",
      "Epoch: 1513   Validation Loss: 12.276138381796883 \n",
      "Epoch: 1514   Training Loss: 0.3581736983733282 \n",
      "Epoch: 1514   Validation Loss: 12.227996312024725 \n",
      "Epoch: 1515   Training Loss: 0.3564360544383422 \n",
      "Epoch: 1515   Validation Loss: 12.291938229064852 \n",
      "Epoch: 1516   Training Loss: 0.34924574686385695 \n",
      "Epoch: 1516   Validation Loss: 12.13153273328249 \n",
      "Epoch: 1517   Training Loss: 0.3519488577290919 \n",
      "Epoch: 1517   Validation Loss: 12.253036039974798 \n",
      "Epoch: 1518   Training Loss: 0.33797325943092044 \n",
      "Epoch: 1518   Validation Loss: 12.33503053487719 \n",
      "Epoch: 1519   Training Loss: 0.3335780656404074 \n",
      "Epoch: 1519   Validation Loss: 12.346930609910176 \n",
      "Epoch: 1520   Training Loss: 0.33277723679081944 \n",
      "Epoch: 1520   Validation Loss: 12.397018638816476 \n",
      "Epoch: 1521   Training Loss: 0.34432663272368186 \n",
      "Epoch: 1521   Validation Loss: 12.369434991341851 \n",
      "Epoch: 1522   Training Loss: 0.33187341544234017 \n",
      "Epoch: 1522   Validation Loss: 12.165770976181317 \n",
      "Epoch: 1523   Training Loss: 0.3203853561515436 \n",
      "Epoch: 1523   Validation Loss: 12.233629570036461 \n",
      "Epoch: 1524   Training Loss: 0.3251394595958125 \n",
      "Epoch: 1524   Validation Loss: 12.329092862575024 \n",
      "Epoch: 1525   Training Loss: 0.3275628139082341 \n",
      "Epoch: 1525   Validation Loss: 12.228867297509764 \n",
      "Epoch: 1526   Training Loss: 0.33191590944709787 \n",
      "Epoch: 1526   Validation Loss: 12.224786109370982 \n",
      "Epoch: 1527   Training Loss: 0.3322910102617644 \n",
      "Epoch: 1527   Validation Loss: 12.331719689209319 \n",
      "Epoch: 1528   Training Loss: 0.35220201669291856 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1528   Validation Loss: 12.211613063516122 \n",
      "Epoch: 1529   Training Loss: 0.3441557890464326 \n",
      "Epoch: 1529   Validation Loss: 12.105402476100341 \n",
      "Epoch: 1530   Training Loss: 0.33828125646199353 \n",
      "Epoch: 1530   Validation Loss: 12.250783604680231 \n",
      "Epoch: 1531   Training Loss: 0.3318502100140556 \n",
      "Epoch: 1531   Validation Loss: 12.100230366231298 \n",
      "Epoch: 1532   Training Loss: 0.3161433882506711 \n",
      "Epoch: 1532   Validation Loss: 12.094325419891415 \n",
      "Epoch: 1533   Training Loss: 0.317724875427148 \n",
      "Epoch: 1533   Validation Loss: 12.110706953079408 \n",
      "Epoch: 1534   Training Loss: 0.30528581259621446 \n",
      "Epoch: 1534   Validation Loss: 12.028398150275041 \n",
      "Epoch: 1535   Training Loss: 0.3102886451757251 \n",
      "Epoch: 1535   Validation Loss: 12.113813795168483 \n",
      "Epoch: 1536   Training Loss: 0.3196981844084313 \n",
      "Epoch: 1536   Validation Loss: 12.021045645172737 \n",
      "Epoch: 1537   Training Loss: 0.350608934135586 \n",
      "Epoch: 1537   Validation Loss: 12.294293202187783 \n",
      "Epoch: 1538   Training Loss: 0.3687806869926506 \n",
      "Epoch: 1538   Validation Loss: 12.225643074993199 \n",
      "Epoch: 1539   Training Loss: 0.34764153886315613 \n",
      "Epoch: 1539   Validation Loss: 12.265800273924919 \n",
      "Epoch: 1540   Training Loss: 0.34241399177677506 \n",
      "Epoch: 1540   Validation Loss: 12.152386462148357 \n",
      "Epoch: 1541   Training Loss: 0.3361412651234261 \n",
      "Epoch: 1541   Validation Loss: 12.318682953269576 \n",
      "Epoch: 1542   Training Loss: 0.3389912082700083 \n",
      "Epoch: 1542   Validation Loss: 12.081344536648688 \n",
      "Epoch: 1543   Training Loss: 0.3300319404439255 \n",
      "Epoch: 1543   Validation Loss: 12.182928198860454 \n",
      "Epoch: 1544   Training Loss: 0.328305180050686 \n",
      "Epoch: 1544   Validation Loss: 12.126107898849948 \n",
      "Epoch: 1545   Training Loss: 0.33846308264789504 \n",
      "Epoch: 1545   Validation Loss: 12.146953698418756 \n",
      "Epoch: 1546   Training Loss: 0.3296895462854778 \n",
      "Epoch: 1546   Validation Loss: 12.114279152625409 \n",
      "Epoch: 1547   Training Loss: 0.3273299395881398 \n",
      "Epoch: 1547   Validation Loss: 12.204822649421867 \n",
      "Epoch: 1548   Training Loss: 0.32061170277234396 \n",
      "Epoch: 1548   Validation Loss: 12.250805948478183 \n",
      "Epoch: 1549   Training Loss: 0.31996690080962203 \n",
      "Epoch: 1549   Validation Loss: 12.12777858294282 \n",
      "Epoch: 1550   Training Loss: 0.3422154960209062 \n",
      "Epoch: 1550   Validation Loss: 12.217106582348048 \n",
      "Epoch: 1551   Training Loss: 0.3399914527888097 \n",
      "Epoch: 1551   Validation Loss: 12.00392955648568 \n",
      "Epoch: 1552   Training Loss: 0.33818496028938283 \n",
      "Epoch: 1552   Validation Loss: 12.103648141748087 \n",
      "Epoch: 1553   Training Loss: 0.32588048445480156 \n",
      "Epoch: 1553   Validation Loss: 12.126395152754124 \n",
      "Epoch: 1554   Training Loss: 0.31822306590028143 \n",
      "Epoch: 1554   Validation Loss: 12.058206086407868 \n",
      "Epoch: 1555   Training Loss: 0.3021530672374052 \n",
      "Epoch: 1555   Validation Loss: 12.24682316039676 \n",
      "Epoch: 1556   Training Loss: 0.30067630451940336 \n",
      "Epoch: 1556   Validation Loss: 12.317719046422914 \n",
      "Epoch: 1557   Training Loss: 0.30967649878058173 \n",
      "Epoch: 1557   Validation Loss: 12.130917538035039 \n",
      "Epoch: 1558   Training Loss: 0.31545373486603817 \n",
      "Epoch: 1558   Validation Loss: 12.069906235518813 \n",
      "Epoch: 1559   Training Loss: 0.31671688833080286 \n",
      "Epoch: 1559   Validation Loss: 12.108320868634806 \n",
      "Epoch: 1560   Training Loss: 0.31051551492983925 \n",
      "Epoch: 1560   Validation Loss: 12.129897242821734 \n",
      "Epoch: 1561   Training Loss: 0.31938626887652344 \n",
      "Epoch: 1561   Validation Loss: 12.138648661843275 \n",
      "Epoch: 1562   Training Loss: 0.3309959270911402 \n",
      "Epoch: 1562   Validation Loss: 12.069659246325697 \n",
      "Epoch: 1563   Training Loss: 0.33250576029217915 \n",
      "Epoch: 1563   Validation Loss: 12.0275994879975 \n",
      "Epoch: 1564   Training Loss: 0.3392705152561055 \n",
      "Epoch: 1564   Validation Loss: 12.0381271506892 \n",
      "Epoch: 1565   Training Loss: 0.3299595901176536 \n",
      "Epoch: 1565   Validation Loss: 12.218083111793941 \n",
      "Epoch: 1566   Training Loss: 0.3302013327447617 \n",
      "Epoch: 1566   Validation Loss: 12.134414383165968 \n",
      "Epoch: 1567   Training Loss: 0.33152019081109724 \n",
      "Epoch: 1567   Validation Loss: 12.117290525063462 \n",
      "Epoch: 1568   Training Loss: 0.33313664293064266 \n",
      "Epoch: 1568   Validation Loss: 12.294008626354431 \n",
      "Epoch: 1569   Training Loss: 0.33567257615735596 \n",
      "Epoch: 1569   Validation Loss: 12.231411369268361 \n",
      "Epoch: 1570   Training Loss: 0.30402252862311 \n",
      "Epoch: 1570   Validation Loss: 12.35692896641104 \n",
      "Epoch: 1571   Training Loss: 0.307440803674258 \n",
      "Epoch: 1571   Validation Loss: 12.257501333633787 \n",
      "Epoch: 1572   Training Loss: 0.30408761839168286 \n",
      "Epoch: 1572   Validation Loss: 12.242440362083286 \n",
      "Epoch: 1573   Training Loss: 0.30284203132634463 \n",
      "Epoch: 1573   Validation Loss: 12.32173165798098 \n",
      "Epoch: 1574   Training Loss: 0.31519298601947443 \n",
      "Epoch: 1574   Validation Loss: 12.152260832513113 \n",
      "Epoch: 1575   Training Loss: 0.3232201407760401 \n",
      "Epoch: 1575   Validation Loss: 12.162969490205695 \n",
      "Epoch: 1576   Training Loss: 0.3190150415876936 \n",
      "Epoch: 1576   Validation Loss: 12.095482037497037 \n",
      "Epoch: 1577   Training Loss: 0.3257408406466712 \n",
      "Epoch: 1577   Validation Loss: 12.175160797842738 \n",
      "Epoch: 1578   Training Loss: 0.311606410354305 \n",
      "Epoch: 1578   Validation Loss: 12.256270228500442 \n",
      "Epoch: 1579   Training Loss: 0.3046194071448379 \n",
      "Epoch: 1579   Validation Loss: 12.134527820653544 \n",
      "Epoch: 1580   Training Loss: 0.30066689308401406 \n",
      "Epoch: 1580   Validation Loss: 12.28948479400866 \n",
      "Epoch: 1581   Training Loss: 0.30661836999276076 \n",
      "Epoch: 1581   Validation Loss: 12.184769394410742 \n",
      "Epoch: 1582   Training Loss: 0.29952083675910396 \n",
      "Epoch: 1582   Validation Loss: 12.149066670804848 \n",
      "Epoch: 1583   Training Loss: 0.31033014928446656 \n",
      "Epoch: 1583   Validation Loss: 12.114256653796213 \n",
      "Epoch: 1584   Training Loss: 0.310179534816554 \n",
      "Epoch: 1584   Validation Loss: 12.229449086402852 \n",
      "Epoch: 1585   Training Loss: 0.31294920903266304 \n",
      "Epoch: 1585   Validation Loss: 12.149528872052894 \n",
      "Epoch: 1586   Training Loss: 0.31065523344238805 \n",
      "Epoch: 1586   Validation Loss: 12.333920286827597 \n",
      "Epoch: 1587   Training Loss: 0.2949985925025489 \n",
      "Epoch: 1587   Validation Loss: 12.307446480564032 \n",
      "Epoch: 1588   Training Loss: 0.28992881424702166 \n",
      "Epoch: 1588   Validation Loss: 12.423931681851599 \n",
      "Epoch: 1589   Training Loss: 0.3000766395930211 \n",
      "Epoch: 1589   Validation Loss: 12.266328149621646 \n",
      "Epoch: 1590   Training Loss: 0.29794245776084777 \n",
      "Epoch: 1590   Validation Loss: 12.328537339619986 \n",
      "Epoch: 1591   Training Loss: 0.29899029158003787 \n",
      "Epoch: 1591   Validation Loss: 12.237754892441519 \n",
      "Epoch: 1592   Training Loss: 0.28816193751371755 \n",
      "Epoch: 1592   Validation Loss: 12.314480384698248 \n",
      "Epoch: 1593   Training Loss: 0.28162990776263924 \n",
      "Epoch: 1593   Validation Loss: 12.123861098378567 \n",
      "Epoch: 1594   Training Loss: 0.28863122311611655 \n",
      "Epoch: 1594   Validation Loss: 12.173193562113482 \n",
      "Epoch: 1595   Training Loss: 0.28846991927612486 \n",
      "Epoch: 1595   Validation Loss: 12.158010652927569 \n",
      "Epoch: 1596   Training Loss: 0.2847153947257987 \n",
      "Epoch: 1596   Validation Loss: 12.237916264455519 \n",
      "Epoch: 1597   Training Loss: 0.2851620370419958 \n",
      "Epoch: 1597   Validation Loss: 12.319622406356725 \n",
      "Epoch: 1598   Training Loss: 0.28196600550257644 \n",
      "Epoch: 1598   Validation Loss: 12.295904732112028 \n",
      "Epoch: 1599   Training Loss: 0.3103859123483865 \n",
      "Epoch: 1599   Validation Loss: 12.387318443448441 \n",
      "Epoch: 1600   Training Loss: 0.3229037026429083 \n",
      "Epoch: 1600   Validation Loss: 12.332033037553606 \n",
      "Epoch: 1601   Training Loss: 0.3125429776214839 \n",
      "Epoch: 1601   Validation Loss: 12.314243908577625 \n",
      "Epoch: 1602   Training Loss: 0.29754498062760415 \n",
      "Epoch: 1602   Validation Loss: 12.34976782097845 \n",
      "Epoch: 1603   Training Loss: 0.2919836968296515 \n",
      "Epoch: 1603   Validation Loss: 12.35789124805067 \n",
      "Epoch: 1604   Training Loss: 0.29323459241491234 \n",
      "Epoch: 1604   Validation Loss: 12.646108897008405 \n",
      "Epoch: 1605   Training Loss: 0.2834030090173987 \n",
      "Epoch: 1605   Validation Loss: 12.413622351308755 \n",
      "Epoch: 1606   Training Loss: 0.27455168776740063 \n",
      "Epoch: 1606   Validation Loss: 12.346810149489064 \n",
      "Epoch: 1607   Training Loss: 0.27091002940744136 \n",
      "Epoch: 1607   Validation Loss: 12.372698159073046 \n",
      "Epoch: 1608   Training Loss: 0.2775960245114396 \n",
      "Epoch: 1608   Validation Loss: 12.452330550937862 \n",
      "Epoch: 1609   Training Loss: 0.28030223732051784 \n",
      "Epoch: 1609   Validation Loss: 12.371530417311503 \n",
      "Epoch: 1610   Training Loss: 0.28322317289514914 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1610   Validation Loss: 12.469745966449922 \n",
      "Epoch: 1611   Training Loss: 0.28825734499294325 \n",
      "Epoch: 1611   Validation Loss: 12.349171962295186 \n",
      "Epoch: 1612   Training Loss: 0.2820212020792469 \n",
      "Epoch: 1612   Validation Loss: 12.41605303914902 \n",
      "Epoch: 1613   Training Loss: 0.277259376215688 \n",
      "Epoch: 1613   Validation Loss: 12.3794438856616 \n",
      "Epoch: 1614   Training Loss: 0.2723874760338431 \n",
      "Epoch: 1614   Validation Loss: 12.242389232532235 \n",
      "Epoch: 1615   Training Loss: 0.2762230148601055 \n",
      "Epoch: 1615   Validation Loss: 12.400231397728966 \n",
      "Epoch: 1616   Training Loss: 0.28120646491259244 \n",
      "Epoch: 1616   Validation Loss: 12.38506646922215 \n",
      "Epoch: 1617   Training Loss: 0.2782912820673328 \n",
      "Epoch: 1617   Validation Loss: 12.308610579036655 \n",
      "Epoch: 1618   Training Loss: 0.2741285418913279 \n",
      "Epoch: 1618   Validation Loss: 12.26929758985796 \n",
      "Epoch: 1619   Training Loss: 0.2758570576980141 \n",
      "Epoch: 1619   Validation Loss: 12.430051438291752 \n",
      "Epoch: 1620   Training Loss: 0.26834708292679627 \n",
      "Epoch: 1620   Validation Loss: 12.451419611144429 \n",
      "Epoch: 1621   Training Loss: 0.27384508125608575 \n",
      "Epoch: 1621   Validation Loss: 12.545220085822226 \n",
      "Epoch: 1622   Training Loss: 0.26409628342578956 \n",
      "Epoch: 1622   Validation Loss: 12.498110185466826 \n",
      "Epoch: 1623   Training Loss: 0.2871512054763536 \n",
      "Epoch: 1623   Validation Loss: 12.482087783306289 \n",
      "Epoch: 1624   Training Loss: 0.29541234912764663 \n",
      "Epoch: 1624   Validation Loss: 12.439304679264337 \n",
      "Epoch: 1625   Training Loss: 0.294201011087163 \n",
      "Epoch: 1625   Validation Loss: 12.566581367745194 \n",
      "Epoch: 1626   Training Loss: 0.2824493507196047 \n",
      "Epoch: 1626   Validation Loss: 12.504383645651322 \n",
      "Epoch: 1627   Training Loss: 0.26764675425939827 \n",
      "Epoch: 1627   Validation Loss: 12.53711605995805 \n",
      "Epoch: 1628   Training Loss: 0.25822594091290735 \n",
      "Epoch: 1628   Validation Loss: 12.573677566274322 \n",
      "Epoch: 1629   Training Loss: 0.26488055159286056 \n",
      "Epoch: 1629   Validation Loss: 12.517573716466917 \n",
      "Epoch: 1630   Training Loss: 0.27208700642012645 \n",
      "Epoch: 1630   Validation Loss: 12.503820823396298 \n",
      "Epoch: 1631   Training Loss: 0.2839113761337557 \n",
      "Epoch: 1631   Validation Loss: 12.586365548147175 \n",
      "Epoch: 1632   Training Loss: 0.2772818386396831 \n",
      "Epoch: 1632   Validation Loss: 12.491614835102409 \n",
      "Epoch: 1633   Training Loss: 0.28195544816119555 \n",
      "Epoch: 1633   Validation Loss: 12.522212025353097 \n",
      "Epoch: 1634   Training Loss: 0.28331313531914 \n",
      "Epoch: 1634   Validation Loss: 12.585124743451972 \n",
      "Epoch: 1635   Training Loss: 0.2807626098008241 \n",
      "Epoch: 1635   Validation Loss: 12.504787280433993 \n",
      "Epoch: 1636   Training Loss: 0.2778164753878944 \n",
      "Epoch: 1636   Validation Loss: 12.415356481293918 \n",
      "Epoch: 1637   Training Loss: 0.26942792455784786 \n",
      "Epoch: 1637   Validation Loss: 12.588970659819005 \n",
      "Epoch: 1638   Training Loss: 0.271592874498314 \n",
      "Epoch: 1638   Validation Loss: 12.585286045642746 \n",
      "Epoch: 1639   Training Loss: 0.26614452483326506 \n",
      "Epoch: 1639   Validation Loss: 12.435343840871251 \n",
      "Epoch: 1640   Training Loss: 0.2549839228612034 \n",
      "Epoch: 1640   Validation Loss: 12.59526422218287 \n",
      "Epoch: 1641   Training Loss: 0.260002255245966 \n",
      "Epoch: 1641   Validation Loss: 12.620741942961176 \n",
      "Epoch: 1642   Training Loss: 0.2558485770343121 \n",
      "Epoch: 1642   Validation Loss: 12.484510011459339 \n",
      "Epoch: 1643   Training Loss: 0.25917116540574037 \n",
      "Epoch: 1643   Validation Loss: 12.526664430215417 \n",
      "Epoch: 1644   Training Loss: 0.2654568176346517 \n",
      "Epoch: 1644   Validation Loss: 12.322999000882568 \n",
      "Epoch: 1645   Training Loss: 0.2738731808702019 \n",
      "Epoch: 1645   Validation Loss: 12.342522750702805 \n",
      "Epoch: 1646   Training Loss: 0.28938608490735956 \n",
      "Epoch: 1646   Validation Loss: 12.361831257008268 \n",
      "Epoch: 1647   Training Loss: 0.27712127311702645 \n",
      "Epoch: 1647   Validation Loss: 12.484132686253197 \n",
      "Epoch: 1648   Training Loss: 0.2639582592326766 \n",
      "Epoch: 1648   Validation Loss: 12.389361892101467 \n",
      "Epoch: 1649   Training Loss: 0.2636029065648651 \n",
      "Epoch: 1649   Validation Loss: 12.520273926075548 \n",
      "Epoch: 1650   Training Loss: 0.2659504864913615 \n",
      "Epoch: 1650   Validation Loss: 12.474867252877157 \n",
      "Epoch: 1651   Training Loss: 0.26518104382639157 \n",
      "Epoch: 1651   Validation Loss: 12.400460418783464 \n",
      "Epoch: 1652   Training Loss: 0.27644541730227795 \n",
      "Epoch: 1652   Validation Loss: 12.50676164242726 \n",
      "Epoch: 1653   Training Loss: 0.2518752495999964 \n",
      "Epoch: 1653   Validation Loss: 12.486812091010869 \n",
      "Epoch: 1654   Training Loss: 0.2414756222733427 \n",
      "Epoch: 1654   Validation Loss: 12.503921940159081 \n",
      "Epoch: 1655   Training Loss: 0.25212070221737165 \n",
      "Epoch: 1655   Validation Loss: 12.551881716060125 \n",
      "Epoch: 1656   Training Loss: 0.24703359597346677 \n",
      "Epoch: 1656   Validation Loss: 12.447125242582276 \n",
      "Epoch: 1657   Training Loss: 0.2445331575198832 \n",
      "Epoch: 1657   Validation Loss: 12.42895936208023 \n",
      "Epoch: 1658   Training Loss: 0.24487980238552437 \n",
      "Epoch: 1658   Validation Loss: 12.397670656366147 \n",
      "Epoch: 1659   Training Loss: 0.2543694550591508 \n",
      "Epoch: 1659   Validation Loss: 12.468157022307802 \n",
      "Epoch: 1660   Training Loss: 0.2555088396213827 \n",
      "Epoch: 1660   Validation Loss: 12.394165590308129 \n",
      "Epoch: 1661   Training Loss: 0.26740775622319635 \n",
      "Epoch: 1661   Validation Loss: 12.42777585159999 \n",
      "Epoch: 1662   Training Loss: 0.2610268061119823 \n",
      "Epoch: 1662   Validation Loss: 12.475499407746945 \n",
      "Epoch: 1663   Training Loss: 0.2514360733670431 \n",
      "Epoch: 1663   Validation Loss: 12.533558010189198 \n",
      "Epoch: 1664   Training Loss: 0.2639992022884762 \n",
      "Epoch: 1664   Validation Loss: 12.615182776526686 \n",
      "Epoch: 1665   Training Loss: 0.2605543044047516 \n",
      "Epoch: 1665   Validation Loss: 12.49415358007632 \n",
      "Epoch: 1666   Training Loss: 0.24706553134401596 \n",
      "Epoch: 1666   Validation Loss: 12.498187332132588 \n",
      "Epoch: 1667   Training Loss: 0.2517864307549248 \n",
      "Epoch: 1667   Validation Loss: 12.559463515656999 \n",
      "Epoch: 1668   Training Loss: 0.2448388021359305 \n",
      "Epoch: 1668   Validation Loss: 12.397397628609673 \n",
      "Epoch: 1669   Training Loss: 0.24351887365173683 \n",
      "Epoch: 1669   Validation Loss: 12.492353376956528 \n",
      "Epoch: 1670   Training Loss: 0.25450401734448386 \n",
      "Epoch: 1670   Validation Loss: 12.557336224634808 \n",
      "Epoch: 1671   Training Loss: 0.25181826150462 \n",
      "Epoch: 1671   Validation Loss: 12.62820698056498 \n",
      "Epoch: 1672   Training Loss: 0.23404863783299507 \n",
      "Epoch: 1672   Validation Loss: 12.653338080443055 \n",
      "Epoch: 1673   Training Loss: 0.2366735924816023 \n",
      "Epoch: 1673   Validation Loss: 12.511717909327707 \n",
      "Epoch: 1674   Training Loss: 0.24873144935961228 \n",
      "Epoch: 1674   Validation Loss: 12.576144520280701 \n",
      "Epoch: 1675   Training Loss: 0.24079032173988077 \n",
      "Epoch: 1675   Validation Loss: 12.364461974209485 \n",
      "Epoch: 1676   Training Loss: 0.24192884513762647 \n",
      "Epoch: 1676   Validation Loss: 12.432421608351294 \n",
      "Epoch: 1677   Training Loss: 0.24037286501525013 \n",
      "Epoch: 1677   Validation Loss: 12.503940987883336 \n",
      "Epoch: 1678   Training Loss: 0.2488519758793128 \n",
      "Epoch: 1678   Validation Loss: 12.483703940761137 \n",
      "Epoch: 1679   Training Loss: 0.25755217024965693 \n",
      "Epoch: 1679   Validation Loss: 12.446202180995114 \n",
      "Epoch: 1680   Training Loss: 0.25837919277805926 \n",
      "Epoch: 1680   Validation Loss: 12.667422149744107 \n",
      "Epoch: 1681   Training Loss: 0.2514830664028769 \n",
      "Epoch: 1681   Validation Loss: 12.596948618379603 \n",
      "Epoch: 1682   Training Loss: 0.24784843147948382 \n",
      "Epoch: 1682   Validation Loss: 12.410943729336335 \n",
      "Epoch: 1683   Training Loss: 0.24398312379012288 \n",
      "Epoch: 1683   Validation Loss: 12.556740358526614 \n",
      "Epoch: 1684   Training Loss: 0.24107590302165066 \n",
      "Epoch: 1684   Validation Loss: 12.543159593324082 \n",
      "Epoch: 1685   Training Loss: 0.24687142916588506 \n",
      "Epoch: 1685   Validation Loss: 12.54537269094699 \n",
      "Epoch: 1686   Training Loss: 0.25267132557128874 \n",
      "Epoch: 1686   Validation Loss: 12.599988099544742 \n",
      "Epoch: 1687   Training Loss: 0.24847730307517618 \n",
      "Epoch: 1687   Validation Loss: 12.585729064202868 \n",
      "Epoch: 1688   Training Loss: 0.24533636531487626 \n",
      "Epoch: 1688   Validation Loss: 12.541336273556384 \n",
      "Epoch: 1689   Training Loss: 0.24333006911138183 \n",
      "Epoch: 1689   Validation Loss: 12.576370487907733 \n",
      "Epoch: 1690   Training Loss: 0.23558953951208558 \n",
      "Epoch: 1690   Validation Loss: 12.530599402335007 \n",
      "Epoch: 1691   Training Loss: 0.23641578739976044 \n",
      "Epoch: 1691   Validation Loss: 12.615801959916137 \n",
      "Epoch: 1692   Training Loss: 0.23623581506551217 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1692   Validation Loss: 12.589330852126327 \n",
      "Epoch: 1693   Training Loss: 0.23992920040699375 \n",
      "Epoch: 1693   Validation Loss: 12.485188964991279 \n",
      "Epoch: 1694   Training Loss: 0.23252626678663324 \n",
      "Epoch: 1694   Validation Loss: 12.729429999080438 \n",
      "Epoch: 1695   Training Loss: 0.22690004904903147 \n",
      "Epoch: 1695   Validation Loss: 12.693118721023577 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     45\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 46\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared Cache\")\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# Create writer and profiler to analyze loss over each epoch\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "# Set device to CUDA if available, initialize model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "net = generate_model(18)\n",
    "net.to(device)\n",
    "\n",
    "# Set up optimizer and loss function, set number of epochs\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-2, momentum=0.9, weight_decay = 0)\n",
    "criterion = nn.MSELoss(reduction = 'mean')\n",
    "criterion.to(device)\n",
    "num_epochs = 2500\n",
    "\n",
    "# Iniitializing variables to show statistics\n",
    "iteration = 0\n",
    "test_iteration = 0\n",
    "loss_list = []\n",
    "test_loss_list = []\n",
    "epoch_loss_averages = []\n",
    "test_epoch_loss_averages = []\n",
    "\n",
    "# Iterates over dataset multiple times\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    test_epoch_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(trainset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(data[0]).to(device).float()), torch.from_numpy(data[1]).to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs).to(device)\n",
    "        loss = criterion(outputs, truths)\n",
    "        # writer.add_scalar(\"Loss / Train\", loss, epoch) # adds training loss scalar\n",
    "        loss_list.append(loss.cpu().detach().numpy())\n",
    "        epoch_loss += loss.cpu().detach().numpy()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration += 1\n",
    "        if iteration % trainset.shape[0] == 0:\n",
    "            epoch_loss_averages.append(epoch_loss / trainset.shape[0])\n",
    "            print('Epoch: {}   Training Loss:   {} '.format(epoch, epoch_loss / trainset.shape[0]))\n",
    "            \n",
    "    for i, test_data in enumerate(testset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(test_data[0]).to(device).float()), torch.from_numpy(test_data[1]).to(device).float()\n",
    "        outputs = net(inputs).to(device)\n",
    "        test_loss = criterion(outputs, truths)\n",
    "        \n",
    "        # writer.add_scalar(\"Loss / Test\", test_loss, epoch) # adds testing loss scalar\n",
    "        test_loss_list.append(test_loss.cpu().detach().numpy())\n",
    "        test_epoch_loss += test_loss.cpu().detach().numpy()\n",
    "        \n",
    "        test_iteration +=1\n",
    "        if test_iteration % testset.shape[0] == 0:\n",
    "            test_epoch_loss_averages.append(test_epoch_loss / testset.shape[0])\n",
    "            print('Epoch: {}   Validation Loss: {} '.format(epoch, test_epoch_loss / testset.shape[0]))\n",
    "            \n",
    "# writer.flush()\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cdd05ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot epoch loss to test for convergence\n",
    "plt.plot(epoch_loss_averages)\n",
    "plt.plot(test_epoch_loss_averages)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0987f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
