{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0695f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential packages\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import tomography and imaging packages\n",
    "import tomopy\n",
    "from skimage.transform import rotate, AffineTransform\n",
    "from skimage import transform as tf\n",
    "from scipy.fft import fft2, fftshift\n",
    "from scipy.signal import correlate\n",
    "\n",
    "# Import neural net packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.profiler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a21bf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Environment: pytorch\n",
      "Cuda Version: 11.8\n",
      "Cuda Availability: True\n",
      "/home/liam/Projects/Tomographic Alignment\r\n"
     ]
    }
   ],
   "source": [
    "# Checking to ensure environment and cuda are correct\n",
    "print(\"Working Environment: {}\".format(os.environ['CONDA_DEFAULT_ENV']))\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"Cuda Version: {}\".format(torch.version.cuda))\n",
    "print(\"Cuda Availability: {}\".format(torch.cuda.is_available()))\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a789b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr_reprojection(data, entries):\n",
    "    \n",
    "    ang = tomopy.angles(data[0][0][0, 0].shape[0])\n",
    "    _rec = 1e-12 * np.ones((data[0][0][0, 0].shape[1], data[0][0][0, 0].shape[2], data[0][0][0, 0].shape[2]))\n",
    "    data_copy = data.copy()\n",
    "    out_data = np.zeros((entries, 2), dtype = object)\n",
    "    \n",
    "    for i in range (entries):\n",
    "    \n",
    "        out_data[i, 0] = np.zeros((1, 1, data[0][0][0, 0].shape[0], data[0][0][0, 0].shape[1] * 2 - 1,\n",
    "                                      data[0][0][0, 0].shape[2] * 2 - 1))\n",
    "        out_data[i, 1] = data_copy[i, 1]\n",
    "        \n",
    "        rec = tomopy.recon(data_copy[i][0][0, 0], ang, center = None, \n",
    "                            algorithm = 'mlem', init_recon = _rec)\n",
    "        reproj = tomopy.project(rec, ang, center = None, pad = False)\n",
    "        \n",
    "        for j in range (data[0][0][0, 0].shape[0]):\n",
    "            \n",
    "            out_data[i, 0][0, 0, j] = correlate(data_copy[i][0][0, 0, j], reproj[j], method = 'fft')\n",
    "        \n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec321c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_projections(data, entries):\n",
    "    \n",
    "    projections = np.zeros((entries * data[0][0][0, 0].shape[0], 2), dtype = object)\n",
    "\n",
    "    for i in range (entries):\n",
    "\n",
    "        for j in range (data[0][0][0, 0].shape[0]):\n",
    "\n",
    "            projections[i * data[0][0][0, 0].shape[0] + j, 0] = data[i, 0][0, 0, j, :, :]\n",
    "            projections[i * data[0][0][0, 0].shape[0] + j, 1] = np.asarray([data[i, 1][0, 2 * j], \n",
    "                                                                            data[i, 1][0, 2 * j + 1]])\n",
    "            \n",
    "    return projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b959e373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Projections: 4500\n"
     ]
    }
   ],
   "source": [
    "# Loading data, 25 entries of 128 resolution shepp3ds\n",
    "res = 128\n",
    "entries = 25\n",
    "data = []\n",
    "\n",
    "for i in range(entries):\n",
    "    data.append(np.load('./shepp{}-{}/shepp{}-{}_{}.npy'.format(res, entries, res, entries, i), \n",
    "                        allow_pickle = True))\n",
    "    \n",
    "data = np.asarray(data)\n",
    "angles = entries * data[0][0][0, 0].shape[0]\n",
    "print(\"Total Projections: {}\".format(angles))\n",
    "crosscorr_reproj = crosscorr_reprojection(data, entries)\n",
    "crosscorr_data = to_projections(crosscorr_reproj, entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54daab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 2)\n",
      "(1, 1, 255, 367)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "for i in range (crosscorr_data.shape[0]):\n",
    "    \n",
    "    crosscorr_data[i, 0] = np.expand_dims(crosscorr_data[i, 0], axis = 0)\n",
    "    crosscorr_data[i, 0] = np.expand_dims(crosscorr_data[i, 0], axis = 0)\n",
    "    crosscorr_data[i, 1] = np.expand_dims(crosscorr_data[i, 1], axis = 0)\n",
    "    \n",
    "print(crosscorr_data.shape)\n",
    "print(crosscorr_data[0, 0].shape)\n",
    "print(crosscorr_data[0, 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2053f296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Dataset: (3600, 2)\n",
      "Shape of Testing Dataset: (900, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of training and testing splits\n",
    "trainset, testset = np.split(crosscorr_data, [int(angles * 4 / 5)])\n",
    "print(\"Shape of Training Dataset: {}\".format(trainset.shape))\n",
    "print(\"Shape of Testing Dataset: {}\".format(testset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6078e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def norm(proj):\n",
    "    proj = (proj - torch.min(proj)) / (torch.max(proj) - torch.min(proj))\n",
    "    return proj\n",
    "\n",
    "# Get inplanes for resnet\n",
    "def get_inplanes():\n",
    "    return [64, 128, 256, 512]\n",
    "\n",
    "\n",
    "# Preset for a 3x3 kernel convolution\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "# Preset for a 1x1 kernel convolution\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=1,\n",
    "                     stride=stride,\n",
    "                     bias=False)\n",
    "\n",
    "# Basic block for resnet\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# Bottleneck block for resnet\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv1x1(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Resnet structure\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 block_inplanes,\n",
    "                 n_input_channels=1,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 shortcut_type='B',\n",
    "                 widen_factor=1.0,\n",
    "                 n_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
    "\n",
    "        self.in_planes = block_inplanes[0]\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_input_channels,\n",
    "                               self.in_planes,\n",
    "                               kernel_size=(conv1_t_size, 7),\n",
    "                               stride=(conv1_t_stride, 2),\n",
    "                               padding=(conv1_t_size // 2, 3),\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n",
    "                                       shortcut_type)\n",
    "        self.layer2 = self._make_layer(block,\n",
    "                                       block_inplanes[1],\n",
    "                                       layers[1],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer3 = self._make_layer(block,\n",
    "                                       block_inplanes[2],\n",
    "                                       layers[2],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer4 = self._make_layer(block,\n",
    "                                       block_inplanes[3],\n",
    "                                       layers[3],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _downsample_basic_block(self, x, planes, stride):\n",
    "        out = F.avg_pool2d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n",
    "                                out.size(3), out.size(4))\n",
    "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "            zero_pads = zero_pads.cuda()\n",
    "\n",
    "        out = torch.cat([out.data, zero_pads], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # make layer helper function\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            \n",
    "                downsample = nn.Sequential(\n",
    "                    conv1x1(self.in_planes, planes * block.expansion, stride),\n",
    "                    nn.BatchNorm2d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(in_planes=self.in_planes,\n",
    "                  planes=planes,\n",
    "                  stride=stride,\n",
    "                  downsample=downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Generates form of resnet\n",
    "def generate_model(model_depth, **kwargs):\n",
    "    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
    "\n",
    "    if model_depth == 10:\n",
    "        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 18:\n",
    "        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 34:\n",
    "        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 50:\n",
    "        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 101:\n",
    "        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 152:\n",
    "        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 200:\n",
    "        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c36556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 2]                    --\n",
       "├─Conv2d: 1-1                            [1, 64, 255, 184]         3,136\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 255, 184]         128\n",
       "├─ReLU: 1-3                              [1, 64, 255, 184]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 128, 92]          --\n",
       "├─Sequential: 1-5                        [1, 64, 128, 92]          --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 128, 92]          36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 128, 92]          36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 128, 92]          --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 128, 92]          36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 128, 92]          36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 128, 92]          --\n",
       "├─Sequential: 1-6                        [1, 128, 64, 46]          --\n",
       "│    └─BasicBlock: 2-3                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 64, 46]          73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 64, 46]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 64, 46]          256\n",
       "│    │    └─Sequential: 3-18             [1, 128, 64, 46]          8,448\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 64, 46]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-20                 [1, 128, 64, 46]          147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-22                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 64, 46]          147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 64, 46]          --\n",
       "├─Sequential: 1-7                        [1, 256, 32, 23]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 32, 23]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 32, 23]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 256, 32, 23]          512\n",
       "│    │    └─Sequential: 3-31             [1, 256, 32, 23]          33,280\n",
       "│    │    └─ReLU: 3-32                   [1, 256, 32, 23]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-33                 [1, 256, 32, 23]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-35                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 32, 23]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-38                   [1, 256, 32, 23]          --\n",
       "├─Sequential: 1-8                        [1, 512, 16, 12]          --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-39                 [1, 512, 16, 12]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 512, 16, 12]          1,024\n",
       "│    │    └─ReLU: 3-41                   [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-42                 [1, 512, 16, 12]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 512, 16, 12]          1,024\n",
       "│    │    └─Sequential: 3-44             [1, 512, 16, 12]          132,096\n",
       "│    │    └─ReLU: 3-45                   [1, 512, 16, 12]          --\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-46                 [1, 512, 16, 12]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 512, 16, 12]          1,024\n",
       "│    │    └─ReLU: 3-48                   [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 16, 12]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 16, 12]          1,024\n",
       "│    │    └─ReLU: 3-51                   [1, 512, 16, 12]          --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [1, 2]                    1,026\n",
       "==========================================================================================\n",
       "Total params: 11,171,266\n",
       "Trainable params: 11,171,266\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 6.58\n",
       "==========================================================================================\n",
       "Input size (MB): 0.37\n",
       "Forward/backward pass size (MB): 149.36\n",
       "Params size (MB): 44.69\n",
       "Estimated Total Size (MB): 194.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = generate_model(18)\n",
    "summary(model, (1, 1, 255, 367))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b392f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared Cache\n",
      "Device: cuda:0\n",
      "Epoch: 0   Training Loss: 9.240803234628814 \n",
      "Epoch: 0   Validation Loss: 7.971414697913246 \n",
      "Epoch: 1   Training Loss: 8.964900368662367 \n",
      "Epoch: 1   Validation Loss: 7.971401303205639 \n",
      "Epoch: 2   Training Loss: 8.964852604303486 \n",
      "Epoch: 2   Validation Loss: 7.971389439401941 \n",
      "Epoch: 3   Training Loss: 8.96484365142682 \n",
      "Epoch: 3   Validation Loss: 7.971380442697555 \n",
      "Epoch: 4   Training Loss: 8.96483930239104 \n",
      "Epoch: 4   Validation Loss: 7.971373918820173 \n",
      "Epoch: 5   Training Loss: 8.964835091195015 \n",
      "Epoch: 5   Validation Loss: 7.971366627677861 \n",
      "Epoch: 6   Training Loss: 8.96483303369315 \n",
      "Epoch: 6   Validation Loss: 7.971361375873287 \n",
      "Epoch: 7   Training Loss: 8.96482993081446 \n",
      "Epoch: 7   Validation Loss: 7.971354872553299 \n",
      "Epoch: 8   Training Loss: 8.964828174851966 \n",
      "Epoch: 8   Validation Loss: 7.9713510549772115 \n",
      "Epoch: 9   Training Loss: 8.964826019524338 \n",
      "Epoch: 9   Validation Loss: 7.9713471342850895 \n",
      "Epoch: 10   Training Loss: 8.964824697805044 \n",
      "Epoch: 10   Validation Loss: 7.971342964015073 \n",
      "Epoch: 11   Training Loss: 8.964822857248429 \n",
      "Epoch: 11   Validation Loss: 7.971335075671474 \n",
      "Epoch: 12   Training Loss: 8.96482072637885 \n",
      "Epoch: 12   Validation Loss: 7.971326816485574 \n",
      "Epoch: 13   Training Loss: 8.964817072657633 \n",
      "Epoch: 13   Validation Loss: 7.971321760258741 \n",
      "Epoch: 14   Training Loss: 8.964814381160071 \n",
      "Epoch: 14   Validation Loss: 7.971315534448044 \n",
      "Epoch: 15   Training Loss: 8.964811597831309 \n",
      "Epoch: 15   Validation Loss: 7.971306460510111 \n",
      "Epoch: 16   Training Loss: 8.964809214346058 \n",
      "Epoch: 16   Validation Loss: 7.971298010090573 \n",
      "Epoch: 17   Training Loss: 8.964807813261427 \n",
      "Epoch: 17   Validation Loss: 7.971286118479653 \n",
      "Epoch: 18   Training Loss: 8.964808764684344 \n",
      "Epoch: 18   Validation Loss: 7.971283978145156 \n",
      "Epoch: 19   Training Loss: 8.964802512935206 \n",
      "Epoch: 19   Validation Loss: 7.971272112576084 \n",
      "Epoch: 20   Training Loss: 8.964803269168899 \n",
      "Epoch: 20   Validation Loss: 7.971265328257448 \n",
      "Epoch: 21   Training Loss: 8.964795590085048 \n",
      "Epoch: 21   Validation Loss: 7.971254080852701 \n",
      "Epoch: 22   Training Loss: 8.964796136187909 \n",
      "Epoch: 22   Validation Loss: 7.971243984076298 \n",
      "Epoch: 23   Training Loss: 8.964791170507212 \n",
      "Epoch: 23   Validation Loss: 7.971232791812056 \n",
      "Epoch: 24   Training Loss: 8.964791131900276 \n",
      "Epoch: 24   Validation Loss: 7.97122428768004 \n",
      "Epoch: 25   Training Loss: 8.96478699726436 \n",
      "Epoch: 25   Validation Loss: 7.971219457572119 \n",
      "Epoch: 26   Training Loss: 8.964776999175355 \n",
      "Epoch: 26   Validation Loss: 7.971193550353249 \n",
      "Epoch: 27   Training Loss: 8.964778384814386 \n",
      "Epoch: 27   Validation Loss: 7.97118707910594 \n",
      "Epoch: 28   Training Loss: 8.96477421683067 \n",
      "Epoch: 28   Validation Loss: 7.97117012636736 \n",
      "Epoch: 29   Training Loss: 8.964777972584121 \n",
      "Epoch: 29   Validation Loss: 7.971151744048629 \n",
      "Epoch: 30   Training Loss: 8.964757005270728 \n",
      "Epoch: 30   Validation Loss: 7.9711251507099306 \n",
      "Epoch: 31   Training Loss: 8.964752451869886 \n",
      "Epoch: 31   Validation Loss: 7.971102488022297 \n",
      "Epoch: 32   Training Loss: 8.964744716916009 \n",
      "Epoch: 32   Validation Loss: 7.9710829592578945 \n",
      "Epoch: 33   Training Loss: 8.964745190624582 \n",
      "Epoch: 33   Validation Loss: 7.971065601714783 \n",
      "Epoch: 34   Training Loss: 8.964741435653112 \n",
      "Epoch: 34   Validation Loss: 7.971033171634708 \n",
      "Epoch: 35   Training Loss: 8.964685330763144 \n",
      "Epoch: 35   Validation Loss: 7.970988876805123 \n",
      "Epoch: 36   Training Loss: 8.964723255303916 \n",
      "Epoch: 36   Validation Loss: 7.97095254159429 \n",
      "Epoch: 37   Training Loss: 8.96468411188826 \n",
      "Epoch: 37   Validation Loss: 7.970929127339687 \n",
      "Epoch: 38   Training Loss: 8.964703989943306 \n",
      "Epoch: 38   Validation Loss: 7.970944352365202 \n",
      "Epoch: 39   Training Loss: 8.964688080160235 \n",
      "Epoch: 39   Validation Loss: 7.970839776135981 \n",
      "Epoch: 40   Training Loss: 8.964659980933808 \n",
      "Epoch: 40   Validation Loss: 7.970779620796028 \n",
      "Epoch: 41   Training Loss: 8.964695921577968 \n",
      "Epoch: 41   Validation Loss: 7.970764675881299 \n",
      "Epoch: 42   Training Loss: 8.964731541349368 \n",
      "Epoch: 42   Validation Loss: 7.970789560526609 \n",
      "Epoch: 43   Training Loss: 8.964661017484481 \n",
      "Epoch: 43   Validation Loss: 7.970702440701425 \n",
      "Epoch: 44   Training Loss: 8.964610144987622 \n",
      "Epoch: 44   Validation Loss: 7.97052904592827 \n",
      "Epoch: 45   Training Loss: 8.964641330095045 \n",
      "Epoch: 45   Validation Loss: 7.97055816422527 \n",
      "Epoch: 46   Training Loss: 8.96464232584171 \n",
      "Epoch: 46   Validation Loss: 7.970498793121013 \n",
      "Epoch: 47   Training Loss: 8.964637941928613 \n",
      "Epoch: 47   Validation Loss: 7.970636873768849 \n",
      "Epoch: 48   Training Loss: 8.964794512871174 \n",
      "Epoch: 48   Validation Loss: 7.970983893637442 \n",
      "Epoch: 49   Training Loss: 8.964552190259063 \n",
      "Epoch: 49   Validation Loss: 7.970747770302825 \n",
      "Epoch: 50   Training Loss: 8.965303952845142 \n",
      "Epoch: 50   Validation Loss: 7.97092454444617 \n",
      "Epoch: 51   Training Loss: 8.964584992877585 \n",
      "Epoch: 51   Validation Loss: 7.970525078549981 \n",
      "Epoch: 52   Training Loss: 8.964378035335212 \n",
      "Epoch: 52   Validation Loss: 7.970416515587519 \n",
      "Epoch: 53   Training Loss: 8.964380961803926 \n",
      "Epoch: 53   Validation Loss: 7.970560805271897 \n",
      "Epoch: 54   Training Loss: 8.96450206321308 \n",
      "Epoch: 54   Validation Loss: 7.970600030014499 \n",
      "Epoch: 55   Training Loss: 8.96442306278421 \n",
      "Epoch: 55   Validation Loss: 7.9707211710678205 \n",
      "Epoch: 56   Training Loss: 8.964262805400875 \n",
      "Epoch: 56   Validation Loss: 7.970248604292671 \n",
      "Epoch: 57   Training Loss: 8.964427472835718 \n",
      "Epoch: 57   Validation Loss: 7.969988334692187 \n",
      "Epoch: 58   Training Loss: 8.964340806807506 \n",
      "Epoch: 58   Validation Loss: 7.970583909129103 \n",
      "Epoch: 59   Training Loss: 8.96433690840506 \n",
      "Epoch: 59   Validation Loss: 7.970269684123083 \n",
      "Epoch: 60   Training Loss: 8.9641132851841 \n",
      "Epoch: 60   Validation Loss: 7.9703795587395625 \n",
      "Epoch: 61   Training Loss: 8.965091613632023 \n",
      "Epoch: 61   Validation Loss: 7.970204453143395 \n",
      "Epoch: 62   Training Loss: 8.96415494645054 \n",
      "Epoch: 62   Validation Loss: 7.96963483911939 \n",
      "Epoch: 63   Training Loss: 8.964729361136762 \n",
      "Epoch: 63   Validation Loss: 7.970240938570351 \n",
      "Epoch: 64   Training Loss: 8.964140560501004 \n",
      "Epoch: 64   Validation Loss: 7.969622049479642 \n",
      "Epoch: 65   Training Loss: 8.964217750928785 \n",
      "Epoch: 65   Validation Loss: 7.97011787279716 \n",
      "Epoch: 66   Training Loss: 8.964155078443484 \n",
      "Epoch: 66   Validation Loss: 7.9699568228599516 \n",
      "Epoch: 67   Training Loss: 8.96451358908734 \n",
      "Epoch: 67   Validation Loss: 7.970581286194631 \n",
      "Epoch: 68   Training Loss: 8.964251259596187 \n",
      "Epoch: 68   Validation Loss: 7.9703959906660025 \n",
      "Epoch: 69   Training Loss: 8.964415471635569 \n",
      "Epoch: 69   Validation Loss: 7.96980703964519 \n",
      "Epoch: 70   Training Loss: 8.96442634884083 \n",
      "Epoch: 70   Validation Loss: 7.970641169982652 \n",
      "Epoch: 71   Training Loss: 8.965154427295202 \n",
      "Epoch: 71   Validation Loss: 7.970661424185253 \n",
      "Epoch: 72   Training Loss: 8.964212824503848 \n",
      "Epoch: 72   Validation Loss: 7.970401845642676 \n",
      "Epoch: 73   Training Loss: 8.964711953310875 \n",
      "Epoch: 73   Validation Loss: 7.970458504018477 \n",
      "Epoch: 74   Training Loss: 8.962613206094586 \n",
      "Epoch: 74   Validation Loss: 7.969702479458517 \n",
      "Epoch: 75   Training Loss: 8.96402332713694 \n",
      "Epoch: 75   Validation Loss: 7.9702280568041735 \n",
      "Epoch: 76   Training Loss: 8.964040812367676 \n",
      "Epoch: 76   Validation Loss: 7.969680122899719 \n",
      "Epoch: 77   Training Loss: 8.964332903753457 \n",
      "Epoch: 77   Validation Loss: 7.970342238105626 \n",
      "Epoch: 78   Training Loss: 8.964346017385534 \n",
      "Epoch: 78   Validation Loss: 7.9698132108917665 \n",
      "Epoch: 79   Training Loss: 8.963945553542363 \n",
      "Epoch: 79   Validation Loss: 7.970105214285561 \n",
      "Epoch: 80   Training Loss: 8.964106639735077 \n",
      "Epoch: 80   Validation Loss: 7.969915918331179 \n",
      "Epoch: 81   Training Loss: 8.964015691885127 \n",
      "Epoch: 81   Validation Loss: 7.969725025177209 \n",
      "Epoch: 82   Training Loss: 8.963952043642347 \n",
      "Epoch: 82   Validation Loss: 7.968671404122271 \n",
      "Epoch: 83   Training Loss: 8.964124848116239 \n",
      "Epoch: 83   Validation Loss: 7.96973584237198 \n",
      "Epoch: 84   Training Loss: 8.963935587608447 \n",
      "Epoch: 84   Validation Loss: 7.969938265104882 \n",
      "Epoch: 85   Training Loss: 8.963825537414781 \n",
      "Epoch: 85   Validation Loss: 7.9696536636352535 \n",
      "Epoch: 86   Training Loss: 8.963783684287046 \n",
      "Epoch: 86   Validation Loss: 7.969931291372825 \n",
      "Epoch: 87   Training Loss: 8.963533424014944 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87   Validation Loss: 7.969192136795156 \n",
      "Epoch: 88   Training Loss: 8.963627031908901 \n",
      "Epoch: 88   Validation Loss: 7.969343194746309 \n",
      "Epoch: 89   Training Loss: 8.963620647866435 \n",
      "Epoch: 89   Validation Loss: 7.969079024090121 \n",
      "Epoch: 90   Training Loss: 8.963519303480732 \n",
      "Epoch: 90   Validation Loss: 7.969173980582919 \n",
      "Epoch: 91   Training Loss: 8.963693568915785 \n",
      "Epoch: 91   Validation Loss: 7.968799661640078 \n",
      "Epoch: 92   Training Loss: 8.963324936939058 \n",
      "Epoch: 92   Validation Loss: 7.968986876767335 \n",
      "Epoch: 93   Training Loss: 8.964549310307792 \n",
      "Epoch: 93   Validation Loss: 7.97153124311318 \n",
      "Epoch: 94   Training Loss: 8.963913773849717 \n",
      "Epoch: 94   Validation Loss: 7.971597902309667 \n",
      "Epoch: 95   Training Loss: 8.963504309119582 \n",
      "Epoch: 95   Validation Loss: 7.970633585225377 \n",
      "Epoch: 96   Training Loss: 8.963526762259102 \n",
      "Epoch: 96   Validation Loss: 7.972467625142696 \n",
      "Epoch: 97   Training Loss: 8.963503910681142 \n",
      "Epoch: 97   Validation Loss: 7.973120816560048 \n",
      "Epoch: 98   Training Loss: 8.964904790863852 \n",
      "Epoch: 98   Validation Loss: 7.967679065084085 \n",
      "Epoch: 99   Training Loss: 8.962361928726605 \n",
      "Epoch: 99   Validation Loss: 7.969259290234703 \n",
      "Epoch: 100   Training Loss: 8.963229299626748 \n",
      "Epoch: 100   Validation Loss: 7.970202067593734 \n",
      "Epoch: 101   Training Loss: 8.962984833547816 \n",
      "Epoch: 101   Validation Loss: 7.970584197651802 \n",
      "Epoch: 102   Training Loss: 8.963293004458311 \n",
      "Epoch: 102   Validation Loss: 7.970949742855607 \n",
      "Epoch: 103   Training Loss: 8.962715852173542 \n",
      "Epoch: 103   Validation Loss: 7.9691085303740365 \n",
      "Epoch: 104   Training Loss: 8.962498068939317 \n",
      "Epoch: 104   Validation Loss: 7.971710970283279 \n",
      "Epoch: 105   Training Loss: 8.962870548998545 \n",
      "Epoch: 105   Validation Loss: 7.96740866650672 \n",
      "Epoch: 106   Training Loss: 8.96193651086899 \n",
      "Epoch: 106   Validation Loss: 7.968343904307112 \n",
      "Epoch: 107   Training Loss: 8.962501124049174 \n",
      "Epoch: 107   Validation Loss: 7.967513594938856 \n",
      "Epoch: 108   Training Loss: 8.962484007819125 \n",
      "Epoch: 108   Validation Loss: 7.97000173331109 \n",
      "Epoch: 109   Training Loss: 8.962834953078815 \n",
      "Epoch: 109   Validation Loss: 7.970409394714257 \n",
      "Epoch: 110   Training Loss: 8.962507505634857 \n",
      "Epoch: 110   Validation Loss: 7.972308702037359 \n",
      "Epoch: 111   Training Loss: 8.963048088751206 \n",
      "Epoch: 111   Validation Loss: 7.968954162231336 \n",
      "Epoch: 112   Training Loss: 8.962330364276019 \n",
      "Epoch: 112   Validation Loss: 7.968474064966819 \n",
      "Epoch: 113   Training Loss: 8.96353941733674 \n",
      "Epoch: 113   Validation Loss: 7.9697830777398 \n",
      "Epoch: 114   Training Loss: 8.96555638778211 \n",
      "Epoch: 114   Validation Loss: 7.966556894688143 \n",
      "Epoch: 115   Training Loss: 8.964077869536737 \n",
      "Epoch: 115   Validation Loss: 7.9662319567163165 \n",
      "Epoch: 116   Training Loss: 8.964267265011829 \n",
      "Epoch: 116   Validation Loss: 7.966450935701012 \n",
      "Epoch: 117   Training Loss: 8.96321777540086 \n",
      "Epoch: 117   Validation Loss: 7.966498027028412 \n",
      "Epoch: 118   Training Loss: 8.962883443156187 \n",
      "Epoch: 118   Validation Loss: 7.966071232691304 \n",
      "Epoch: 119   Training Loss: 8.96234090679629 \n",
      "Epoch: 119   Validation Loss: 7.964248636273616 \n",
      "Epoch: 120   Training Loss: 8.961721474054649 \n",
      "Epoch: 120   Validation Loss: 7.964533484920911 \n",
      "Epoch: 121   Training Loss: 8.961072073163628 \n",
      "Epoch: 121   Validation Loss: 7.96320940358228 \n",
      "Epoch: 122   Training Loss: 8.961547291511295 \n",
      "Epoch: 122   Validation Loss: 7.963027036539279 \n",
      "Epoch: 123   Training Loss: 8.961031548744636 \n",
      "Epoch: 123   Validation Loss: 7.965437042386685 \n",
      "Epoch: 124   Training Loss: 8.961932446369458 \n",
      "Epoch: 124   Validation Loss: 7.96780279786326 \n",
      "Epoch: 125   Training Loss: 8.961345771268194 \n",
      "Epoch: 125   Validation Loss: 7.966779849823409 \n",
      "Epoch: 126   Training Loss: 8.96079458620063 \n",
      "Epoch: 126   Validation Loss: 7.966982161730476 \n",
      "Epoch: 127   Training Loss: 8.961684761324468 \n",
      "Epoch: 127   Validation Loss: 7.968712407519731 \n",
      "Epoch: 128   Training Loss: 8.961288731112372 \n",
      "Epoch: 128   Validation Loss: 7.967833304006813 \n",
      "Epoch: 129   Training Loss: 8.961674491224937 \n",
      "Epoch: 129   Validation Loss: 7.96838370312471 \n",
      "Epoch: 130   Training Loss: 8.961934401835396 \n",
      "Epoch: 130   Validation Loss: 7.965746337555401 \n",
      "Epoch: 131   Training Loss: 8.962217229510223 \n",
      "Epoch: 131   Validation Loss: 7.967863952189995 \n",
      "Epoch: 132   Training Loss: 8.960474721172892 \n",
      "Epoch: 132   Validation Loss: 7.9714023087403945 \n",
      "Epoch: 133   Training Loss: 8.961921609559456 \n",
      "Epoch: 133   Validation Loss: 7.970659999024356 \n",
      "Epoch: 134   Training Loss: 8.96051116467557 \n",
      "Epoch: 134   Validation Loss: 7.972622779319645 \n",
      "Epoch: 135   Training Loss: 8.960739297746105 \n",
      "Epoch: 135   Validation Loss: 7.969531342083162 \n",
      "Epoch: 136   Training Loss: 8.961155248601337 \n",
      "Epoch: 136   Validation Loss: 7.972600259355968 \n",
      "Epoch: 137   Training Loss: 8.95994747763456 \n",
      "Epoch: 137   Validation Loss: 7.971184806568684 \n",
      "Epoch: 138   Training Loss: 8.961861382752897 \n",
      "Epoch: 138   Validation Loss: 7.9697772493324655 \n",
      "Epoch: 139   Training Loss: 8.959883310608728 \n",
      "Epoch: 139   Validation Loss: 7.971895615087835 \n",
      "Epoch: 140   Training Loss: 8.960421343997053 \n",
      "Epoch: 140   Validation Loss: 7.9730340842163905 \n",
      "Epoch: 141   Training Loss: 8.960941568176963 \n",
      "Epoch: 141   Validation Loss: 7.972548461906845 \n",
      "Epoch: 142   Training Loss: 8.959349591037617 \n",
      "Epoch: 142   Validation Loss: 7.97267055984736 \n",
      "Epoch: 143   Training Loss: 8.959621473882208 \n",
      "Epoch: 143   Validation Loss: 7.9731318634349275 \n",
      "Epoch: 144   Training Loss: 8.957871894847194 \n",
      "Epoch: 144   Validation Loss: 7.975378355421902 \n",
      "Epoch: 145   Training Loss: 8.96146035661979 \n",
      "Epoch: 145   Validation Loss: 7.97309629534216 \n",
      "Epoch: 146   Training Loss: 8.959675426546811 \n",
      "Epoch: 146   Validation Loss: 7.973515222557148 \n",
      "Epoch: 147   Training Loss: 8.957989940768877 \n",
      "Epoch: 147   Validation Loss: 7.976021063852952 \n",
      "Epoch: 148   Training Loss: 8.95754089668626 \n",
      "Epoch: 148   Validation Loss: 7.978795685263144 \n",
      "Epoch: 149   Training Loss: 8.959073663098987 \n",
      "Epoch: 149   Validation Loss: 7.978304658911268 \n",
      "Epoch: 150   Training Loss: 8.955523016458448 \n",
      "Epoch: 150   Validation Loss: 7.981429634543343 \n",
      "Epoch: 151   Training Loss: 8.958272369339328 \n",
      "Epoch: 151   Validation Loss: 7.97433898116145 \n",
      "Epoch: 152   Training Loss: 8.958483527114085 \n",
      "Epoch: 152   Validation Loss: 7.977737319202473 \n",
      "Epoch: 153   Training Loss: 8.959918493653369 \n",
      "Epoch: 153   Validation Loss: 7.970915263442116 \n",
      "Epoch: 154   Training Loss: 8.966339351788928 \n",
      "Epoch: 154   Validation Loss: 7.964702484468516 \n",
      "Epoch: 155   Training Loss: 8.962322503896786 \n",
      "Epoch: 155   Validation Loss: 7.959976822341689 \n",
      "Epoch: 156   Training Loss: 8.960806947830813 \n",
      "Epoch: 156   Validation Loss: 7.956852406366314 \n",
      "Epoch: 157   Training Loss: 8.959606393471217 \n",
      "Epoch: 157   Validation Loss: 7.9584519721339975 \n",
      "Epoch: 158   Training Loss: 8.962084908637884 \n",
      "Epoch: 158   Validation Loss: 7.958486734433617 \n",
      "Epoch: 159   Training Loss: 8.962571104731357 \n",
      "Epoch: 159   Validation Loss: 7.963257317231037 \n",
      "Epoch: 160   Training Loss: 8.961192848883284 \n",
      "Epoch: 160   Validation Loss: 7.961519568133064 \n",
      "Epoch: 161   Training Loss: 8.96090043113014 \n",
      "Epoch: 161   Validation Loss: 7.963557306413746 \n",
      "Epoch: 162   Training Loss: 8.96153224625838 \n",
      "Epoch: 162   Validation Loss: 7.9612314482374735 \n",
      "Epoch: 163   Training Loss: 8.959901776526758 \n",
      "Epoch: 163   Validation Loss: 7.961886681745398 \n",
      "Epoch: 164   Training Loss: 8.95976563707518 \n",
      "Epoch: 164   Validation Loss: 7.962125866051712 \n",
      "Epoch: 165   Training Loss: 8.958032675774982 \n",
      "Epoch: 165   Validation Loss: 7.96382382464684 \n",
      "Epoch: 166   Training Loss: 8.957768492805029 \n",
      "Epoch: 166   Validation Loss: 7.968293106257689 \n",
      "Epoch: 167   Training Loss: 8.96081297217325 \n",
      "Epoch: 167   Validation Loss: 7.969030819993705 \n",
      "Epoch: 168   Training Loss: 8.95942361652109 \n",
      "Epoch: 168   Validation Loss: 7.969924682258214 \n",
      "Epoch: 169   Training Loss: 8.958040203281287 \n",
      "Epoch: 169   Validation Loss: 7.965324989277223 \n",
      "Epoch: 170   Training Loss: 8.957992747917128 \n",
      "Epoch: 170   Validation Loss: 7.963381128535273 \n",
      "Epoch: 171   Training Loss: 8.956353536289454 \n",
      "Epoch: 171   Validation Loss: 7.959493850505775 \n",
      "Epoch: 172   Training Loss: 8.955922507318787 \n",
      "Epoch: 172   Validation Loss: 7.962933592326007 \n",
      "Epoch: 173   Training Loss: 8.958885110654098 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 173   Validation Loss: 7.964006282478157 \n",
      "Epoch: 174   Training Loss: 8.956573476295 \n",
      "Epoch: 174   Validation Loss: 7.960960548007085 \n",
      "Epoch: 175   Training Loss: 8.955215833051172 \n",
      "Epoch: 175   Validation Loss: 7.964648287434959 \n",
      "Epoch: 176   Training Loss: 8.955221718289863 \n",
      "Epoch: 176   Validation Loss: 7.967397678071737 \n",
      "Epoch: 177   Training Loss: 8.955881865969827 \n",
      "Epoch: 177   Validation Loss: 7.958489487439187 \n",
      "Epoch: 178   Training Loss: 8.953696906149593 \n",
      "Epoch: 178   Validation Loss: 7.961642695889523 \n",
      "Epoch: 179   Training Loss: 8.954264124152639 \n",
      "Epoch: 179   Validation Loss: 7.9586089707036605 \n",
      "Epoch: 180   Training Loss: 8.952327148203185 \n",
      "Epoch: 180   Validation Loss: 7.961215146230275 \n",
      "Epoch: 181   Training Loss: 8.952514264255685 \n",
      "Epoch: 181   Validation Loss: 7.955574011982212 \n",
      "Epoch: 182   Training Loss: 8.954062790695236 \n",
      "Epoch: 182   Validation Loss: 7.955806107110499 \n",
      "Epoch: 183   Training Loss: 8.956366416558094 \n",
      "Epoch: 183   Validation Loss: 7.956544552357195 \n",
      "Epoch: 184   Training Loss: 8.952717648184011 \n",
      "Epoch: 184   Validation Loss: 7.951940004877332 \n",
      "Epoch: 185   Training Loss: 8.952646021342872 \n",
      "Epoch: 185   Validation Loss: 7.955851859710852 \n",
      "Epoch: 186   Training Loss: 8.955272793534816 \n",
      "Epoch: 186   Validation Loss: 7.961838002197393 \n",
      "Epoch: 187   Training Loss: 8.952404106659444 \n",
      "Epoch: 187   Validation Loss: 7.959454736185984 \n",
      "Epoch: 188   Training Loss: 8.950402837948415 \n",
      "Epoch: 188   Validation Loss: 7.956773094443294 \n",
      "Epoch: 189   Training Loss: 8.946090585140386 \n",
      "Epoch: 189   Validation Loss: 7.959237866109341 \n",
      "Epoch: 190   Training Loss: 8.947423054471955 \n",
      "Epoch: 190   Validation Loss: 7.9567353752775425 \n",
      "Epoch: 191   Training Loss: 8.948605437593425 \n",
      "Epoch: 191   Validation Loss: 7.956464674851805 \n",
      "Epoch: 192   Training Loss: 8.945386401796842 \n",
      "Epoch: 192   Validation Loss: 7.956930355303307 \n",
      "Epoch: 193   Training Loss: 8.948404773397375 \n",
      "Epoch: 193   Validation Loss: 7.9629767359580566 \n",
      "Epoch: 194   Training Loss: 8.945696838671347 \n",
      "Epoch: 194   Validation Loss: 7.962954080418632 \n",
      "Epoch: 195   Training Loss: 8.943140218899329 \n",
      "Epoch: 195   Validation Loss: 7.963838988016359 \n",
      "Epoch: 196   Training Loss: 8.961157470993315 \n",
      "Epoch: 196   Validation Loss: 7.967459702023512 \n",
      "Epoch: 197   Training Loss: 8.960426314978184 \n",
      "Epoch: 197   Validation Loss: 7.965344819316589 \n",
      "Epoch: 198   Training Loss: 8.950405533348697 \n",
      "Epoch: 198   Validation Loss: 7.970529029351116 \n",
      "Epoch: 199   Training Loss: 8.95638729124212 \n",
      "Epoch: 199   Validation Loss: 7.9681870256328775 \n",
      "Epoch: 200   Training Loss: 8.956057371724972 \n",
      "Epoch: 200   Validation Loss: 7.959550210985019 \n",
      "Epoch: 201   Training Loss: 8.956654809486256 \n",
      "Epoch: 201   Validation Loss: 7.979292480747746 \n",
      "Epoch: 202   Training Loss: 8.9585836748226 \n",
      "Epoch: 202   Validation Loss: 7.982840136056248 \n",
      "Epoch: 203   Training Loss: 8.954955063847205 \n",
      "Epoch: 203   Validation Loss: 7.978349797329413 \n",
      "Epoch: 204   Training Loss: 8.952199032743454 \n",
      "Epoch: 204   Validation Loss: 7.9868146935853614 \n",
      "Epoch: 205   Training Loss: 8.95023793060942 \n",
      "Epoch: 205   Validation Loss: 7.9893005072811825 \n",
      "Epoch: 206   Training Loss: 8.951530162528668 \n",
      "Epoch: 206   Validation Loss: 7.98698518065477 \n",
      "Epoch: 207   Training Loss: 8.950448571817768 \n",
      "Epoch: 207   Validation Loss: 7.98440191531473 \n",
      "Epoch: 208   Training Loss: 8.949349854977983 \n",
      "Epoch: 208   Validation Loss: 7.981198594178002 \n",
      "Epoch: 209   Training Loss: 8.946912721781732 \n",
      "Epoch: 209   Validation Loss: 7.98177258357535 \n",
      "Epoch: 210   Training Loss: 8.946344254598209 \n",
      "Epoch: 210   Validation Loss: 7.983521532304843 \n",
      "Epoch: 211   Training Loss: 8.947712237214356 \n",
      "Epoch: 211   Validation Loss: 7.984779657179671 \n",
      "Epoch: 212   Training Loss: 8.947635085956296 \n",
      "Epoch: 212   Validation Loss: 7.972718330823506 \n",
      "Epoch: 213   Training Loss: 8.945774261824853 \n",
      "Epoch: 213   Validation Loss: 7.968407313912499 \n",
      "Epoch: 214   Training Loss: 8.9468412015222 \n",
      "Epoch: 214   Validation Loss: 7.965091934447281 \n",
      "Epoch: 215   Training Loss: 8.943054833014006 \n",
      "Epoch: 215   Validation Loss: 7.966717047980314 \n",
      "Epoch: 216   Training Loss: 8.938964084158878 \n",
      "Epoch: 216   Validation Loss: 7.97576788574095 \n",
      "Epoch: 217   Training Loss: 8.940115654680776 \n",
      "Epoch: 217   Validation Loss: 7.971022345349192 \n",
      "Epoch: 218   Training Loss: 8.938962180593231 \n",
      "Epoch: 218   Validation Loss: 7.984886585609832 \n",
      "Epoch: 219   Training Loss: 8.937379581914769 \n",
      "Epoch: 219   Validation Loss: 7.96869631312904 \n",
      "Epoch: 220   Training Loss: 8.934682315042592 \n",
      "Epoch: 220   Validation Loss: 7.96313224093594 \n",
      "Epoch: 221   Training Loss: 8.935902629899307 \n",
      "Epoch: 221   Validation Loss: 7.959731738107164 \n",
      "Epoch: 222   Training Loss: 8.938668686773616 \n",
      "Epoch: 222   Validation Loss: 7.991061554683579 \n",
      "Epoch: 223   Training Loss: 8.939708258653878 \n",
      "Epoch: 223   Validation Loss: 7.9542667388100465 \n",
      "Epoch: 224   Training Loss: 8.933283030975339 \n",
      "Epoch: 224   Validation Loss: 7.972330314527369 \n",
      "Epoch: 225   Training Loss: 8.937287275759944 \n",
      "Epoch: 225   Validation Loss: 7.95724319259455 \n",
      "Epoch: 226   Training Loss: 8.93090943750783 \n",
      "Epoch: 226   Validation Loss: 7.9549928040730045 \n",
      "Epoch: 227   Training Loss: 8.928579212106367 \n",
      "Epoch: 227   Validation Loss: 7.962305474212238 \n",
      "Epoch: 228   Training Loss: 8.92623467929823 \n",
      "Epoch: 228   Validation Loss: 7.954204995585864 \n",
      "Epoch: 229   Training Loss: 8.927621984419659 \n",
      "Epoch: 229   Validation Loss: 7.942516626629254 \n",
      "Epoch: 230   Training Loss: 8.956547727613588 \n",
      "Epoch: 230   Validation Loss: 7.964195303642612 \n",
      "Epoch: 231   Training Loss: 8.953403040729366 \n",
      "Epoch: 231   Validation Loss: 7.965963436019633 \n",
      "Epoch: 232   Training Loss: 8.959212100559057 \n",
      "Epoch: 232   Validation Loss: 7.970244832697014 \n",
      "Epoch: 233   Training Loss: 8.959829380864466 \n",
      "Epoch: 233   Validation Loss: 7.970398200249425 \n",
      "Epoch: 234   Training Loss: 8.956123470874347 \n",
      "Epoch: 234   Validation Loss: 7.965570274052087 \n",
      "Epoch: 235   Training Loss: 8.957644300053564 \n",
      "Epoch: 235   Validation Loss: 7.971126697948202 \n",
      "Epoch: 236   Training Loss: 8.958547920237212 \n",
      "Epoch: 236   Validation Loss: 7.9712594443663125 \n",
      "Epoch: 237   Training Loss: 8.952163984133957 \n",
      "Epoch: 237   Validation Loss: 7.9762907982933235 \n",
      "Epoch: 238   Training Loss: 8.946868981729446 \n",
      "Epoch: 238   Validation Loss: 7.978920101248659 \n",
      "Epoch: 239   Training Loss: 8.942502587821036 \n",
      "Epoch: 239   Validation Loss: 7.9780671761071105 \n",
      "Epoch: 240   Training Loss: 8.941948153215925 \n",
      "Epoch: 240   Validation Loss: 7.987890750914812 \n",
      "Epoch: 241   Training Loss: 8.936687483319417 \n",
      "Epoch: 241   Validation Loss: 7.983584501812649 \n",
      "Epoch: 242   Training Loss: 8.932187431656187 \n",
      "Epoch: 242   Validation Loss: 7.976383117306719 \n",
      "Epoch: 243   Training Loss: 8.933123103103554 \n",
      "Epoch: 243   Validation Loss: 7.976325239149543 \n",
      "Epoch: 244   Training Loss: 8.927002441267833 \n",
      "Epoch: 244   Validation Loss: 7.991655507398306 \n",
      "Epoch: 245   Training Loss: 8.926689147651365 \n",
      "Epoch: 245   Validation Loss: 7.986866691538857 \n",
      "Epoch: 246   Training Loss: 8.924369122989818 \n",
      "Epoch: 246   Validation Loss: 7.9960108406280375 \n",
      "Epoch: 247   Training Loss: 8.927642618076579 \n",
      "Epoch: 247   Validation Loss: 7.983761971968536 \n",
      "Epoch: 248   Training Loss: 8.9258392062136 \n",
      "Epoch: 248   Validation Loss: 8.005431427582064 \n",
      "Epoch: 249   Training Loss: 8.92433495684957 \n",
      "Epoch: 249   Validation Loss: 7.996544013857687 \n",
      "Epoch: 250   Training Loss: 8.933080414724099 \n",
      "Epoch: 250   Validation Loss: 7.986575664005553 \n",
      "Epoch: 251   Training Loss: 8.930940232309384 \n",
      "Epoch: 251   Validation Loss: 8.001674164899935 \n",
      "Epoch: 252   Training Loss: 8.930818142040701 \n",
      "Epoch: 252   Validation Loss: 8.01614552237973 \n",
      "Epoch: 253   Training Loss: 8.945246506061231 \n",
      "Epoch: 253   Validation Loss: 7.982512502851378 \n",
      "Epoch: 254   Training Loss: 8.960193399057461 \n",
      "Epoch: 254   Validation Loss: 8.003169190790246 \n",
      "Epoch: 255   Training Loss: 8.932478889591552 \n",
      "Epoch: 255   Validation Loss: 8.003370726000544 \n",
      "Epoch: 256   Training Loss: 8.932886234782183 \n",
      "Epoch: 256   Validation Loss: 8.002427536274578 \n",
      "Epoch: 257   Training Loss: 8.931033339566332 \n",
      "Epoch: 257   Validation Loss: 8.003693782501763 \n",
      "Epoch: 258   Training Loss: 8.923370797360937 \n",
      "Epoch: 258   Validation Loss: 7.998677457755613 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 259   Training Loss: 8.924754045706658 \n",
      "Epoch: 259   Validation Loss: 7.992992564126293 \n",
      "Epoch: 260   Training Loss: 8.923339026973638 \n",
      "Epoch: 260   Validation Loss: 8.004443848559168 \n",
      "Epoch: 261   Training Loss: 8.951063028142025 \n",
      "Epoch: 261   Validation Loss: 7.989003085090113 \n",
      "Epoch: 262   Training Loss: 8.947555794249872 \n",
      "Epoch: 262   Validation Loss: 7.99608802741581 \n",
      "Epoch: 263   Training Loss: 8.93528153611336 \n",
      "Epoch: 263   Validation Loss: 8.006893566239192 \n",
      "Epoch: 264   Training Loss: 8.93863078866445 \n",
      "Epoch: 264   Validation Loss: 7.987270003818234 \n",
      "Epoch: 265   Training Loss: 8.943397084468485 \n",
      "Epoch: 265   Validation Loss: 7.981693161450481 \n",
      "Epoch: 266   Training Loss: 8.938404146405382 \n",
      "Epoch: 266   Validation Loss: 7.9802500978804245 \n",
      "Epoch: 267   Training Loss: 8.931432784193085 \n",
      "Epoch: 267   Validation Loss: 7.994579231672817 \n",
      "Epoch: 268   Training Loss: 8.933773420125387 \n",
      "Epoch: 268   Validation Loss: 7.976627403627305 \n",
      "Epoch: 269   Training Loss: 8.940115182636251 \n",
      "Epoch: 269   Validation Loss: 7.977994766184453 \n",
      "Epoch: 270   Training Loss: 8.938717740826187 \n",
      "Epoch: 270   Validation Loss: 7.971264170381861 \n",
      "Epoch: 271   Training Loss: 8.938617419186777 \n",
      "Epoch: 271   Validation Loss: 7.981319073380695 \n",
      "Epoch: 272   Training Loss: 8.945094717064203 \n",
      "Epoch: 272   Validation Loss: 7.984056190176246 \n",
      "Epoch: 273   Training Loss: 8.9374624482134 \n",
      "Epoch: 273   Validation Loss: 7.977336064337546 \n",
      "Epoch: 274   Training Loss: 8.932955062887526 \n",
      "Epoch: 274   Validation Loss: 7.975278737243886 \n",
      "Epoch: 275   Training Loss: 8.941807959562931 \n",
      "Epoch: 275   Validation Loss: 7.9744883326322045 \n",
      "Epoch: 276   Training Loss: 8.926330684881913 \n",
      "Epoch: 276   Validation Loss: 7.961224383514459 \n",
      "Epoch: 277   Training Loss: 8.930282353924413 \n",
      "Epoch: 277   Validation Loss: 7.987024278030731 \n",
      "Epoch: 278   Training Loss: 8.934660089940314 \n",
      "Epoch: 278   Validation Loss: 7.9819308949283245 \n",
      "Epoch: 279   Training Loss: 8.944746252164155 \n",
      "Epoch: 279   Validation Loss: 7.989141268866564 \n",
      "Epoch: 280   Training Loss: 8.942490910310687 \n",
      "Epoch: 280   Validation Loss: 7.992795775432605 \n",
      "Epoch: 281   Training Loss: 8.936088580286984 \n",
      "Epoch: 281   Validation Loss: 7.990151244813525 \n",
      "Epoch: 282   Training Loss: 8.944180983947234 \n",
      "Epoch: 282   Validation Loss: 7.978135436800205 \n",
      "Epoch: 283   Training Loss: 8.941744110163482 \n",
      "Epoch: 283   Validation Loss: 7.977953551714568 \n",
      "Epoch: 284   Training Loss: 8.932467551714717 \n",
      "Epoch: 284   Validation Loss: 7.981260213337632 \n",
      "Epoch: 285   Training Loss: 8.92484105620441 \n",
      "Epoch: 285   Validation Loss: 7.970341630682298 \n",
      "Epoch: 286   Training Loss: 8.919676605433674 \n",
      "Epoch: 286   Validation Loss: 7.975004575357307 \n",
      "Epoch: 287   Training Loss: 8.912197618799837 \n",
      "Epoch: 287   Validation Loss: 7.969738653258213 \n",
      "Epoch: 288   Training Loss: 8.905355976861028 \n",
      "Epoch: 288   Validation Loss: 7.975208043323687 \n",
      "Epoch: 289   Training Loss: 8.902110227993205 \n",
      "Epoch: 289   Validation Loss: 7.981502067246216 \n",
      "Epoch: 290   Training Loss: 8.916789129560145 \n",
      "Epoch: 290   Validation Loss: 7.9847521852607475 \n",
      "Epoch: 291   Training Loss: 8.910647931451926 \n",
      "Epoch: 291   Validation Loss: 7.997735682443599 \n",
      "Epoch: 292   Training Loss: 8.9503007156814 \n",
      "Epoch: 292   Validation Loss: 7.973249727877944 \n",
      "Epoch: 293   Training Loss: 8.924983341397667 \n",
      "Epoch: 293   Validation Loss: 8.004722730653011 \n",
      "Epoch: 294   Training Loss: 8.945706454432463 \n",
      "Epoch: 294   Validation Loss: 7.974688842329714 \n",
      "Epoch: 295   Training Loss: 8.940993637201593 \n",
      "Epoch: 295   Validation Loss: 8.002593520374761 \n",
      "Epoch: 296   Training Loss: 8.925188736909638 \n",
      "Epoch: 296   Validation Loss: 8.012837895608584 \n",
      "Epoch: 297   Training Loss: 8.924208871033173 \n",
      "Epoch: 297   Validation Loss: 7.993652220955701 \n",
      "Epoch: 298   Training Loss: 8.912841752617185 \n",
      "Epoch: 298   Validation Loss: 7.975034651106495 \n",
      "Epoch: 299   Training Loss: 8.911678451872948 \n",
      "Epoch: 299   Validation Loss: 7.987318659687492 \n",
      "Epoch: 300   Training Loss: 8.90125713402701 \n",
      "Epoch: 300   Validation Loss: 7.991018606282111 \n",
      "Epoch: 301   Training Loss: 8.89101759164865 \n",
      "Epoch: 301   Validation Loss: 7.995458573758689 \n",
      "Epoch: 302   Training Loss: 8.900685320357896 \n",
      "Epoch: 302   Validation Loss: 8.00648331806193 \n",
      "Epoch: 303   Training Loss: 8.89178806974365 \n",
      "Epoch: 303   Validation Loss: 8.013379875694065 \n",
      "Epoch: 304   Training Loss: 8.882020835516936 \n",
      "Epoch: 304   Validation Loss: 8.02793168537849 \n",
      "Epoch: 305   Training Loss: 8.877978336084036 \n",
      "Epoch: 305   Validation Loss: 8.04417875520497 \n",
      "Epoch: 306   Training Loss: 8.869671983122911 \n",
      "Epoch: 306   Validation Loss: 8.027744953754397 \n",
      "Epoch: 307   Training Loss: 8.883305491293513 \n",
      "Epoch: 307   Validation Loss: 8.021888994624508 \n",
      "Epoch: 308   Training Loss: 8.87289776888258 \n",
      "Epoch: 308   Validation Loss: 8.03227493680341 \n",
      "Epoch: 309   Training Loss: 8.860530283442131 \n",
      "Epoch: 309   Validation Loss: 8.026003218888397 \n",
      "Epoch: 310   Training Loss: 8.859410423851543 \n",
      "Epoch: 310   Validation Loss: 8.039410730079593 \n",
      "Epoch: 311   Training Loss: 8.852668038157507 \n",
      "Epoch: 311   Validation Loss: 8.011223792188135 \n",
      "Epoch: 312   Training Loss: 8.861430998835228 \n",
      "Epoch: 312   Validation Loss: 8.012583021965629 \n",
      "Epoch: 313   Training Loss: 8.85402645320347 \n",
      "Epoch: 313   Validation Loss: 8.024402169356245 \n",
      "Epoch: 314   Training Loss: 8.835191924632591 \n",
      "Epoch: 314   Validation Loss: 7.992478386476157 \n",
      "Epoch: 315   Training Loss: 8.825473358337675 \n",
      "Epoch: 315   Validation Loss: 8.013196335771742 \n",
      "Epoch: 316   Training Loss: 8.81956007290929 \n",
      "Epoch: 316   Validation Loss: 7.987157461083326 \n",
      "Epoch: 317   Training Loss: 8.834759879912276 \n",
      "Epoch: 317   Validation Loss: 7.992476989081544 \n",
      "Epoch: 318   Training Loss: 8.821589415713776 \n",
      "Epoch: 318   Validation Loss: 7.9914347841673425 \n",
      "Epoch: 319   Training Loss: 8.820008775427285 \n",
      "Epoch: 319   Validation Loss: 7.993202963489004 \n",
      "Epoch: 320   Training Loss: 8.818906343384976 \n",
      "Epoch: 320   Validation Loss: 7.97570104706929 \n",
      "Epoch: 321   Training Loss: 8.813665490009678 \n",
      "Epoch: 321   Validation Loss: 7.993561528595941 \n",
      "Epoch: 322   Training Loss: 8.788448498008705 \n",
      "Epoch: 322   Validation Loss: 8.004121727807085 \n",
      "Epoch: 323   Training Loss: 8.77937638577001 \n",
      "Epoch: 323   Validation Loss: 8.03271601293029 \n",
      "Epoch: 324   Training Loss: 8.84365138421165 \n",
      "Epoch: 324   Validation Loss: 7.975341857355605 \n",
      "Epoch: 325   Training Loss: 8.853899879144285 \n",
      "Epoch: 325   Validation Loss: 7.940939944739867 \n",
      "Epoch: 326   Training Loss: 8.87912490615517 \n",
      "Epoch: 326   Validation Loss: 7.946187643582622 \n",
      "Epoch: 327   Training Loss: 8.873580645297382 \n",
      "Epoch: 327   Validation Loss: 7.954842462974032 \n",
      "Epoch: 328   Training Loss: 8.849144030660549 \n",
      "Epoch: 328   Validation Loss: 7.956980127942128 \n",
      "Epoch: 329   Training Loss: 8.843052402443233 \n",
      "Epoch: 329   Validation Loss: 7.964561096160259 \n",
      "Epoch: 330   Training Loss: 8.811544250086865 \n",
      "Epoch: 330   Validation Loss: 7.955318613269159 \n",
      "Epoch: 331   Training Loss: 8.80718087272594 \n",
      "Epoch: 331   Validation Loss: 7.965625008084915 \n",
      "Epoch: 332   Training Loss: 8.774793578221953 \n",
      "Epoch: 332   Validation Loss: 7.959770144287937 \n",
      "Epoch: 333   Training Loss: 8.753963765620234 \n",
      "Epoch: 333   Validation Loss: 7.986784769854146 \n",
      "Epoch: 334   Training Loss: 8.82101919796106 \n",
      "Epoch: 334   Validation Loss: 7.982781613365482 \n",
      "Epoch: 335   Training Loss: 8.96600395485607 \n",
      "Epoch: 335   Validation Loss: 7.971097137371285 \n",
      "Epoch: 336   Training Loss: 8.940292004488706 \n",
      "Epoch: 336   Validation Loss: 7.977167672098156 \n",
      "Epoch: 337   Training Loss: 8.8372290268152 \n",
      "Epoch: 337   Validation Loss: 8.00912451462762 \n",
      "Epoch: 338   Training Loss: 8.96427717677375 \n",
      "Epoch: 338   Validation Loss: 8.010081104624097 \n",
      "Epoch: 339   Training Loss: 8.921950305338557 \n",
      "Epoch: 339   Validation Loss: 7.98092092228426 \n",
      "Epoch: 340   Training Loss: 8.90408147811895 \n",
      "Epoch: 340   Validation Loss: 7.974289250966782 \n",
      "Epoch: 341   Training Loss: 8.886370294506714 \n",
      "Epoch: 341   Validation Loss: 7.951566323956164 \n",
      "Epoch: 342   Training Loss: 8.909196243699261 \n",
      "Epoch: 342   Validation Loss: 7.954531977990862 \n",
      "Epoch: 343   Training Loss: 8.885811154887111 \n",
      "Epoch: 343   Validation Loss: 7.93761892541226 \n",
      "Epoch: 344   Training Loss: 8.869741451981996 \n",
      "Epoch: 344   Validation Loss: 7.942099966113649 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 345   Training Loss: 8.855420705784542 \n",
      "Epoch: 345   Validation Loss: 7.940552575212205 \n",
      "Epoch: 346   Training Loss: 8.85189791140111 \n",
      "Epoch: 346   Validation Loss: 7.9450338394667455 \n",
      "Epoch: 347   Training Loss: 8.847073488406863 \n",
      "Epoch: 347   Validation Loss: 7.952494457086238 \n",
      "Epoch: 348   Training Loss: 8.835480332371857 \n",
      "Epoch: 348   Validation Loss: 7.958038565236364 \n",
      "Epoch: 349   Training Loss: 8.869549189933377 \n",
      "Epoch: 349   Validation Loss: 7.976885374194179 \n",
      "Epoch: 350   Training Loss: 8.90357335514832 \n",
      "Epoch: 350   Validation Loss: 7.974188301239111 \n",
      "Epoch: 351   Training Loss: 8.90077919668454 \n",
      "Epoch: 351   Validation Loss: 7.961484642053515 \n",
      "Epoch: 352   Training Loss: 8.882128242143452 \n",
      "Epoch: 352   Validation Loss: 7.969282296348829 \n",
      "Epoch: 353   Training Loss: 8.85870627965086 \n",
      "Epoch: 353   Validation Loss: 7.957329519695712 \n",
      "Epoch: 354   Training Loss: 8.84542573032532 \n",
      "Epoch: 354   Validation Loss: 7.991027231883672 \n",
      "Epoch: 355   Training Loss: 8.819910958893937 \n",
      "Epoch: 355   Validation Loss: 7.958973250506808 \n",
      "Epoch: 356   Training Loss: 8.80303094748343 \n",
      "Epoch: 356   Validation Loss: 7.961429682868208 \n",
      "Epoch: 357   Training Loss: 8.831462068024978 \n",
      "Epoch: 357   Validation Loss: 7.999296964869006 \n",
      "Epoch: 358   Training Loss: 8.832774518799512 \n",
      "Epoch: 358   Validation Loss: 8.010202273398981 \n",
      "Epoch: 359   Training Loss: 8.851471131537336 \n",
      "Epoch: 359   Validation Loss: 7.961936709738543 \n",
      "Epoch: 360   Training Loss: 8.847103044973316 \n",
      "Epoch: 360   Validation Loss: 7.985624390428226 \n",
      "Epoch: 361   Training Loss: 8.857797348035367 \n",
      "Epoch: 361   Validation Loss: 8.007221363808203 \n",
      "Epoch: 362   Training Loss: 8.96292759718949 \n",
      "Epoch: 362   Validation Loss: 8.016704616823441 \n",
      "Epoch: 363   Training Loss: 8.86648921714495 \n",
      "Epoch: 363   Validation Loss: 7.985938633888598 \n",
      "Epoch: 364   Training Loss: 8.982094656176658 \n",
      "Epoch: 364   Validation Loss: 7.958466391242626 \n",
      "Epoch: 365   Training Loss: 8.956663923139583 \n",
      "Epoch: 365   Validation Loss: 7.943757743947434 \n",
      "Epoch: 366   Training Loss: 8.949427506710547 \n",
      "Epoch: 366   Validation Loss: 7.941809125636873 \n",
      "Epoch: 367   Training Loss: 8.940121173227455 \n",
      "Epoch: 367   Validation Loss: 7.93964148938229 \n",
      "Epoch: 368   Training Loss: 8.934096276655088 \n",
      "Epoch: 368   Validation Loss: 7.946220150796127 \n",
      "Epoch: 369   Training Loss: 8.926245055666335 \n",
      "Epoch: 369   Validation Loss: 7.954254959137308 \n",
      "Epoch: 370   Training Loss: 8.914740768639506 \n",
      "Epoch: 370   Validation Loss: 7.958445339609478 \n",
      "Epoch: 371   Training Loss: 8.906042695225418 \n",
      "Epoch: 371   Validation Loss: 7.9764112615541345 \n",
      "Epoch: 372   Training Loss: 8.904538324436475 \n",
      "Epoch: 372   Validation Loss: 7.9924888009668535 \n",
      "Epoch: 373   Training Loss: 8.903174111731659 \n",
      "Epoch: 373   Validation Loss: 8.012098886702054 \n",
      "Epoch: 374   Training Loss: 8.895947089244364 \n",
      "Epoch: 374   Validation Loss: 8.001132703552463 \n",
      "Epoch: 375   Training Loss: 8.895296381056703 \n",
      "Epoch: 375   Validation Loss: 8.002640054252616 \n",
      "Epoch: 376   Training Loss: 8.880168265393861 \n",
      "Epoch: 376   Validation Loss: 8.002815393979512 \n",
      "Epoch: 377   Training Loss: 8.864835875365188 \n",
      "Epoch: 377   Validation Loss: 7.992507910302148 \n",
      "Epoch: 378   Training Loss: 8.845585977057263 \n",
      "Epoch: 378   Validation Loss: 8.002438991424407 \n",
      "Epoch: 379   Training Loss: 8.823687073855641 \n",
      "Epoch: 379   Validation Loss: 8.048323367367459 \n",
      "Epoch: 380   Training Loss: 8.80666553544191 \n",
      "Epoch: 380   Validation Loss: 8.032616141833067 \n",
      "Epoch: 381   Training Loss: 8.803190947795002 \n",
      "Epoch: 381   Validation Loss: 8.035938213981895 \n",
      "Epoch: 382   Training Loss: 8.760486011144215 \n",
      "Epoch: 382   Validation Loss: 8.069544714560422 \n",
      "Epoch: 383   Training Loss: 8.752232292408735 \n",
      "Epoch: 383   Validation Loss: 8.081897064858284 \n",
      "Epoch: 384   Training Loss: 8.728563348524657 \n",
      "Epoch: 384   Validation Loss: 8.090873890889343 \n",
      "Epoch: 385   Training Loss: 8.755991291665826 \n",
      "Epoch: 385   Validation Loss: 8.165972932379615 \n",
      "Epoch: 386   Training Loss: 8.75494469621804 \n",
      "Epoch: 386   Validation Loss: 8.101902308401995 \n",
      "Epoch: 387   Training Loss: 8.742351975739657 \n",
      "Epoch: 387   Validation Loss: 8.124201667952358 \n",
      "Epoch: 388   Training Loss: 8.841486231444069 \n",
      "Epoch: 388   Validation Loss: 7.97633642011736 \n",
      "Epoch: 389   Training Loss: 8.848289792552968 \n",
      "Epoch: 389   Validation Loss: 8.128205384308158 \n",
      "Epoch: 390   Training Loss: 8.75365858786541 \n",
      "Epoch: 390   Validation Loss: 8.116348172013273 \n",
      "Epoch: 391   Training Loss: 8.746398090021449 \n",
      "Epoch: 391   Validation Loss: 8.004060879605046 \n",
      "Epoch: 392   Training Loss: 8.879178481001937 \n",
      "Epoch: 392   Validation Loss: 7.969816954589656 \n",
      "Epoch: 393   Training Loss: 8.881464043199319 \n",
      "Epoch: 393   Validation Loss: 8.098769624643674 \n",
      "Epoch: 394   Training Loss: 8.736644808680163 \n",
      "Epoch: 394   Validation Loss: 8.129256427846332 \n",
      "Epoch: 395   Training Loss: 8.798049687394757 \n",
      "Epoch: 395   Validation Loss: 7.998058038501961 \n",
      "Epoch: 396   Training Loss: 8.772675585686683 \n",
      "Epoch: 396   Validation Loss: 8.049376402513008 \n",
      "Epoch: 397   Training Loss: 8.708245815834404 \n",
      "Epoch: 397   Validation Loss: 8.082287648861271 \n",
      "Epoch: 398   Training Loss: 8.681508948996496 \n",
      "Epoch: 398   Validation Loss: 8.046348350860386 \n",
      "Epoch: 399   Training Loss: 8.658666729423624 \n",
      "Epoch: 399   Validation Loss: 8.152887716044464 \n",
      "Epoch: 400   Training Loss: 8.733947444953147 \n",
      "Epoch: 400   Validation Loss: 7.9962059489379556 \n",
      "Epoch: 401   Training Loss: 8.973020866500867 \n",
      "Epoch: 401   Validation Loss: 7.962597012867013 \n",
      "Epoch: 402   Training Loss: 8.94816950160816 \n",
      "Epoch: 402   Validation Loss: 7.953416633305347 \n",
      "Epoch: 403   Training Loss: 8.937478544865032 \n",
      "Epoch: 403   Validation Loss: 7.960326415660998 \n",
      "Epoch: 404   Training Loss: 8.934864073539858 \n",
      "Epoch: 404   Validation Loss: 7.954395282575084 \n",
      "Epoch: 405   Training Loss: 8.918457681694239 \n",
      "Epoch: 405   Validation Loss: 7.954976179855455 \n",
      "Epoch: 406   Training Loss: 8.906221686017219 \n",
      "Epoch: 406   Validation Loss: 7.9690870514958325 \n",
      "Epoch: 407   Training Loss: 8.895954970914026 \n",
      "Epoch: 407   Validation Loss: 7.952622598238651 \n",
      "Epoch: 408   Training Loss: 8.891064681833248 \n",
      "Epoch: 408   Validation Loss: 7.958782928773564 \n",
      "Epoch: 409   Training Loss: 8.877180287597454 \n",
      "Epoch: 409   Validation Loss: 7.9659381881552855 \n",
      "Epoch: 410   Training Loss: 8.886999384396065 \n",
      "Epoch: 410   Validation Loss: 7.998388574697278 \n",
      "Epoch: 411   Training Loss: 8.87653187322959 \n",
      "Epoch: 411   Validation Loss: 7.996494496068723 \n",
      "Epoch: 412   Training Loss: 8.860117106815904 \n",
      "Epoch: 412   Validation Loss: 7.970463259404577 \n",
      "Epoch: 413   Training Loss: 8.841718653517633 \n",
      "Epoch: 413   Validation Loss: 7.992948149360503 \n",
      "Epoch: 414   Training Loss: 8.828692764699 \n",
      "Epoch: 414   Validation Loss: 8.006943170205535 \n",
      "Epoch: 415   Training Loss: 8.850196810347617 \n",
      "Epoch: 415   Validation Loss: 8.006365918455412 \n",
      "Epoch: 416   Training Loss: 8.87097930153527 \n",
      "Epoch: 416   Validation Loss: 8.011496368211487 \n",
      "Epoch: 417   Training Loss: 8.82562198214891 \n",
      "Epoch: 417   Validation Loss: 8.016679954974387 \n",
      "Epoch: 418   Training Loss: 8.867595391829028 \n",
      "Epoch: 418   Validation Loss: 8.0219771357463 \n",
      "Epoch: 419   Training Loss: 8.859369785649582 \n",
      "Epoch: 419   Validation Loss: 7.989329766531009 \n",
      "Epoch: 420   Training Loss: 8.81114668620393 \n",
      "Epoch: 420   Validation Loss: 8.017567844143292 \n",
      "Epoch: 421   Training Loss: 8.784211551776146 \n",
      "Epoch: 421   Validation Loss: 8.006259118653722 \n",
      "Epoch: 422   Training Loss: 8.777022436720687 \n",
      "Epoch: 422   Validation Loss: 8.032839315664024 \n",
      "Epoch: 423   Training Loss: 8.742527738902007 \n",
      "Epoch: 423   Validation Loss: 8.021613813744496 \n",
      "Epoch: 424   Training Loss: 8.722454328869912 \n",
      "Epoch: 424   Validation Loss: 8.038652511357213 \n",
      "Epoch: 425   Training Loss: 8.699702797021668 \n",
      "Epoch: 425   Validation Loss: 8.044887086584673 \n",
      "Epoch: 426   Training Loss: 8.720898035445797 \n",
      "Epoch: 426   Validation Loss: 8.056314477809808 \n",
      "Epoch: 427   Training Loss: 8.698522848433624 \n",
      "Epoch: 427   Validation Loss: 7.988992331450572 \n",
      "Epoch: 428   Training Loss: 8.742945978329436 \n",
      "Epoch: 428   Validation Loss: 8.012649299730029 \n",
      "Epoch: 429   Training Loss: 8.781835333694552 \n",
      "Epoch: 429   Validation Loss: 8.015963357653366 \n",
      "Epoch: 430   Training Loss: 8.700977874014853 \n",
      "Epoch: 430   Validation Loss: 8.08947251706591 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 431   Training Loss: 8.628571062054576 \n",
      "Epoch: 431   Validation Loss: 8.114314519470403 \n",
      "Epoch: 432   Training Loss: 8.662710923213627 \n",
      "Epoch: 432   Validation Loss: 8.135803086086979 \n",
      "Epoch: 433   Training Loss: 8.859503531498728 \n",
      "Epoch: 433   Validation Loss: 7.9978047853467675 \n",
      "Epoch: 434   Training Loss: 8.857712310622478 \n",
      "Epoch: 434   Validation Loss: 8.047552755385016 \n",
      "Epoch: 435   Training Loss: 8.971725117496163 \n",
      "Epoch: 435   Validation Loss: 7.954227244625541 \n",
      "Epoch: 436   Training Loss: 8.922816186748063 \n",
      "Epoch: 436   Validation Loss: 7.98424946219324 \n",
      "Epoch: 437   Training Loss: 8.941295373671526 \n",
      "Epoch: 437   Validation Loss: 7.9782139408335855 \n",
      "Epoch: 438   Training Loss: 8.92879944525655 \n",
      "Epoch: 438   Validation Loss: 8.016834487537013 \n",
      "Epoch: 439   Training Loss: 8.922742632348292 \n",
      "Epoch: 439   Validation Loss: 8.020634691490947 \n",
      "Epoch: 440   Training Loss: 8.885272884830826 \n",
      "Epoch: 440   Validation Loss: 8.044412443811437 \n",
      "Epoch: 441   Training Loss: 8.87570116575568 \n",
      "Epoch: 441   Validation Loss: 8.036754462538132 \n",
      "Epoch: 442   Training Loss: 8.8334779773963 \n",
      "Epoch: 442   Validation Loss: 8.083990321881009 \n",
      "Epoch: 443   Training Loss: 8.78574596383882 \n",
      "Epoch: 443   Validation Loss: 8.107353182536446 \n",
      "Epoch: 444   Training Loss: 8.727038050530384 \n",
      "Epoch: 444   Validation Loss: 8.161423132378195 \n",
      "Epoch: 445   Training Loss: 8.682729563922585 \n",
      "Epoch: 445   Validation Loss: 8.161342963726364 \n",
      "Epoch: 446   Training Loss: 8.656034467664673 \n",
      "Epoch: 446   Validation Loss: 8.267031238516275 \n",
      "Epoch: 447   Training Loss: 8.638751959117087 \n",
      "Epoch: 447   Validation Loss: 8.245382172668405 \n",
      "Epoch: 448   Training Loss: 8.551642288330719 \n",
      "Epoch: 448   Validation Loss: 8.240842200700039 \n",
      "Epoch: 449   Training Loss: 8.504252300797768 \n",
      "Epoch: 449   Validation Loss: 8.318175911787483 \n",
      "Epoch: 450   Training Loss: 8.513898814066327 \n",
      "Epoch: 450   Validation Loss: 8.306603191585486 \n",
      "Epoch: 451   Training Loss: 8.990551345405414 \n",
      "Epoch: 451   Validation Loss: 7.960617847073203 \n",
      "Epoch: 452   Training Loss: 8.95174779527751 \n",
      "Epoch: 452   Validation Loss: 7.984065499289945 \n",
      "Epoch: 453   Training Loss: 8.919455392999657 \n",
      "Epoch: 453   Validation Loss: 7.980396507547314 \n",
      "Epoch: 454   Training Loss: 8.898058690460449 \n",
      "Epoch: 454   Validation Loss: 7.958103811379038 \n",
      "Epoch: 455   Training Loss: 8.861283554666839 \n",
      "Epoch: 455   Validation Loss: 7.9649294451272326 \n",
      "Epoch: 456   Training Loss: 8.798983648454014 \n",
      "Epoch: 456   Validation Loss: 8.005582242651531 \n",
      "Epoch: 457   Training Loss: 8.772297774217472 \n",
      "Epoch: 457   Validation Loss: 8.027393149360755 \n",
      "Epoch: 458   Training Loss: 8.736278687978754 \n",
      "Epoch: 458   Validation Loss: 8.026592001513345 \n",
      "Epoch: 459   Training Loss: 8.712177542766229 \n",
      "Epoch: 459   Validation Loss: 8.033970240470437 \n",
      "Epoch: 460   Training Loss: 8.668618207128166 \n",
      "Epoch: 460   Validation Loss: 8.096601675339157 \n",
      "Epoch: 461   Training Loss: 8.684209705179407 \n",
      "Epoch: 461   Validation Loss: 8.077193218474923 \n",
      "Epoch: 462   Training Loss: 8.655300202556665 \n",
      "Epoch: 462   Validation Loss: 8.079800662024969 \n",
      "Epoch: 463   Training Loss: 8.614601723492944 \n",
      "Epoch: 463   Validation Loss: 8.046718731150937 \n",
      "Epoch: 464   Training Loss: 8.545672257616307 \n",
      "Epoch: 464   Validation Loss: 8.082648069114269 \n",
      "Epoch: 465   Training Loss: 8.792592211258153 \n",
      "Epoch: 465   Validation Loss: 7.961105069072089 \n",
      "Epoch: 466   Training Loss: 8.950818793049645 \n",
      "Epoch: 466   Validation Loss: 7.9877702908841375 \n",
      "Epoch: 467   Training Loss: 8.907489868845571 \n",
      "Epoch: 467   Validation Loss: 8.033179098937868 \n",
      "Epoch: 468   Training Loss: 8.856456469939085 \n",
      "Epoch: 468   Validation Loss: 8.011215654191975 \n",
      "Epoch: 469   Training Loss: 8.823425291985801 \n",
      "Epoch: 469   Validation Loss: 8.013179830957217 \n",
      "Epoch: 470   Training Loss: 8.761372159255844 \n",
      "Epoch: 470   Validation Loss: 8.043831757340167 \n",
      "Epoch: 471   Training Loss: 8.727724302364546 \n",
      "Epoch: 471   Validation Loss: 8.060316001841306 \n",
      "Epoch: 472   Training Loss: 8.665125386320627 \n",
      "Epoch: 472   Validation Loss: 8.088613050743524 \n",
      "Epoch: 473   Training Loss: 8.6241987635934 \n",
      "Epoch: 473   Validation Loss: 8.107690091489477 \n",
      "Epoch: 474   Training Loss: 8.589748717240791 \n",
      "Epoch: 474   Validation Loss: 8.101242772187817 \n",
      "Epoch: 475   Training Loss: 8.519833903009616 \n",
      "Epoch: 475   Validation Loss: 8.134685606539326 \n",
      "Epoch: 476   Training Loss: 8.522429175100859 \n",
      "Epoch: 476   Validation Loss: 8.14778957999001 \n",
      "Epoch: 477   Training Loss: 8.592154611372626 \n",
      "Epoch: 477   Validation Loss: 8.243569300761447 \n",
      "Epoch: 478   Training Loss: 8.517629330663071 \n",
      "Epoch: 478   Validation Loss: 8.403444825981019 \n",
      "Epoch: 479   Training Loss: 8.438018716431882 \n",
      "Epoch: 479   Validation Loss: 8.312648917271549 \n",
      "Epoch: 480   Training Loss: 8.430382899187139 \n",
      "Epoch: 480   Validation Loss: 8.433664379474212 \n",
      "Epoch: 481   Training Loss: 8.352180108696832 \n",
      "Epoch: 481   Validation Loss: 8.415218555387627 \n",
      "Epoch: 482   Training Loss: 8.815430464063704 \n",
      "Epoch: 482   Validation Loss: 7.977739269125223 \n",
      "Epoch: 483   Training Loss: 8.943360092364694 \n",
      "Epoch: 483   Validation Loss: 8.01041933819962 \n",
      "Epoch: 484   Training Loss: 8.94152071817558 \n",
      "Epoch: 484   Validation Loss: 8.013286394001916 \n",
      "Epoch: 485   Training Loss: 8.922274082746238 \n",
      "Epoch: 485   Validation Loss: 8.02781868826209 \n",
      "Epoch: 486   Training Loss: 8.90556315170352 \n",
      "Epoch: 486   Validation Loss: 7.996526577971979 \n",
      "Epoch: 487   Training Loss: 8.8947988717265 \n",
      "Epoch: 487   Validation Loss: 8.011336769965581 \n",
      "Epoch: 488   Training Loss: 8.857909234778667 \n",
      "Epoch: 488   Validation Loss: 7.98586878814631 \n",
      "Epoch: 489   Training Loss: 8.844013511048406 \n",
      "Epoch: 489   Validation Loss: 7.994236336895782 \n",
      "Epoch: 490   Training Loss: 8.830831104107464 \n",
      "Epoch: 490   Validation Loss: 8.024271571017389 \n",
      "Epoch: 491   Training Loss: 8.812325914479798 \n",
      "Epoch: 491   Validation Loss: 7.996264601658849 \n",
      "Epoch: 492   Training Loss: 8.810425347697763 \n",
      "Epoch: 492   Validation Loss: 7.991179190756213 \n",
      "Epoch: 493   Training Loss: 8.776036157726388 \n",
      "Epoch: 493   Validation Loss: 8.005749321406086 \n",
      "Epoch: 494   Training Loss: 8.788646809064973 \n",
      "Epoch: 494   Validation Loss: 8.037385481574262 \n",
      "Epoch: 495   Training Loss: 8.738295243492821 \n",
      "Epoch: 495   Validation Loss: 8.051917167582886 \n",
      "Epoch: 496   Training Loss: 8.703960075074654 \n",
      "Epoch: 496   Validation Loss: 8.042481418632798 \n",
      "Epoch: 497   Training Loss: 8.79311774589167 \n",
      "Epoch: 497   Validation Loss: 8.08670383387892 \n",
      "Epoch: 498   Training Loss: 8.686257993050296 \n",
      "Epoch: 498   Validation Loss: 8.080559038700562 \n",
      "Epoch: 499   Training Loss: 8.598438612287605 \n",
      "Epoch: 499   Validation Loss: 8.092781468626976 \n",
      "Epoch: 500   Training Loss: 8.637909582851053 \n",
      "Epoch: 500   Validation Loss: 8.159198905573641 \n",
      "Epoch: 501   Training Loss: 8.553682372994574 \n",
      "Epoch: 501   Validation Loss: 8.231227486522661 \n",
      "Epoch: 502   Training Loss: 8.455841780136996 \n",
      "Epoch: 502   Validation Loss: 8.299498550997944 \n",
      "Epoch: 503   Training Loss: 8.427177544211723 \n",
      "Epoch: 503   Validation Loss: 8.194772861879319 \n",
      "Epoch: 504   Training Loss: 8.464150893518267 \n",
      "Epoch: 504   Validation Loss: 8.22093824466587 \n",
      "Epoch: 505   Training Loss: 8.358705344206433 \n",
      "Epoch: 505   Validation Loss: 8.275161955387384 \n",
      "Epoch: 506   Training Loss: 8.790537817908302 \n",
      "Epoch: 506   Validation Loss: 7.971397104098255 \n",
      "Epoch: 507   Training Loss: 8.653369682038496 \n",
      "Epoch: 507   Validation Loss: 8.177707390505045 \n",
      "Epoch: 508   Training Loss: 8.540246137056917 \n",
      "Epoch: 508   Validation Loss: 8.272521543907674 \n",
      "Epoch: 509   Training Loss: 8.489213298344913 \n",
      "Epoch: 509   Validation Loss: 8.343230600549528 \n",
      "Epoch: 510   Training Loss: 8.464526159056161 \n",
      "Epoch: 510   Validation Loss: 8.59308623648572 \n",
      "Epoch: 511   Training Loss: 8.410554419118109 \n",
      "Epoch: 511   Validation Loss: 8.366535425008172 \n",
      "Epoch: 512   Training Loss: 8.479624557684309 \n",
      "Epoch: 512   Validation Loss: 8.37043331323709 \n",
      "Epoch: 513   Training Loss: 8.41037617690027 \n",
      "Epoch: 513   Validation Loss: 8.26183509074824 \n",
      "Epoch: 514   Training Loss: 8.349173263881816 \n",
      "Epoch: 514   Validation Loss: 8.39792602053326 \n",
      "Epoch: 515   Training Loss: 8.296013144569919 \n",
      "Epoch: 515   Validation Loss: 8.412078958734249 \n",
      "Epoch: 516   Training Loss: 8.415946583626372 \n",
      "Epoch: 516   Validation Loss: 7.994930589596172 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 517   Training Loss: 8.972833305342535 \n",
      "Epoch: 517   Validation Loss: 7.971459266138586 \n",
      "Epoch: 518   Training Loss: 8.949703449241238 \n",
      "Epoch: 518   Validation Loss: 7.9679273067735545 \n",
      "Epoch: 519   Training Loss: 8.928111251295963 \n",
      "Epoch: 519   Validation Loss: 7.971539766204336 \n",
      "Epoch: 520   Training Loss: 8.923274184527912 \n",
      "Epoch: 520   Validation Loss: 7.986496067234936 \n",
      "Epoch: 521   Training Loss: 8.904993258145774 \n",
      "Epoch: 521   Validation Loss: 8.015886353183951 \n",
      "Epoch: 522   Training Loss: 8.893437891084197 \n",
      "Epoch: 522   Validation Loss: 8.04069709660589 \n",
      "Epoch: 523   Training Loss: 8.86769794317053 \n",
      "Epoch: 523   Validation Loss: 8.081581752825942 \n",
      "Epoch: 524   Training Loss: 8.83358141633572 \n",
      "Epoch: 524   Validation Loss: 8.102196480038886 \n",
      "Epoch: 525   Training Loss: 8.821417425725349 \n",
      "Epoch: 525   Validation Loss: 8.1465930811709 \n",
      "Epoch: 526   Training Loss: 8.80418453933309 \n",
      "Epoch: 526   Validation Loss: 8.136449986772302 \n",
      "Epoch: 527   Training Loss: 8.766382313580124 \n",
      "Epoch: 527   Validation Loss: 8.201157337910391 \n",
      "Epoch: 528   Training Loss: 8.744357328173177 \n",
      "Epoch: 528   Validation Loss: 8.25846290244792 \n",
      "Epoch: 529   Training Loss: 8.72588168260761 \n",
      "Epoch: 529   Validation Loss: 8.321229556101995 \n",
      "Epoch: 530   Training Loss: 8.705345409154026 \n",
      "Epoch: 530   Validation Loss: 8.399722187892637 \n",
      "Epoch: 531   Training Loss: 8.683728397463808 \n",
      "Epoch: 531   Validation Loss: 8.411245039775968 \n",
      "Epoch: 532   Training Loss: 8.656823923322268 \n",
      "Epoch: 532   Validation Loss: 8.449209723023749 \n",
      "Epoch: 533   Training Loss: 8.875351865337644 \n",
      "Epoch: 533   Validation Loss: 7.969123668926768 \n",
      "Epoch: 534   Training Loss: 8.933309647155294 \n",
      "Epoch: 534   Validation Loss: 7.972240606528681 \n",
      "Epoch: 535   Training Loss: 8.903093656158308 \n",
      "Epoch: 535   Validation Loss: 8.007216219908118 \n",
      "Epoch: 536   Training Loss: 8.882448529191418 \n",
      "Epoch: 536   Validation Loss: 8.00532364355633 \n",
      "Epoch: 537   Training Loss: 8.862020607489038 \n",
      "Epoch: 537   Validation Loss: 7.990905942948432 \n",
      "Epoch: 538   Training Loss: 8.835341709074282 \n",
      "Epoch: 538   Validation Loss: 7.985493521997834 \n",
      "Epoch: 539   Training Loss: 8.812197180471994 \n",
      "Epoch: 539   Validation Loss: 7.964488336071129 \n",
      "Epoch: 540   Training Loss: 8.803157284499061 \n",
      "Epoch: 540   Validation Loss: 7.978587476838665 \n",
      "Epoch: 541   Training Loss: 8.760527237841185 \n",
      "Epoch: 541   Validation Loss: 8.01731958286899 \n",
      "Epoch: 542   Training Loss: 8.756071799016928 \n",
      "Epoch: 542   Validation Loss: 8.042071420885266 \n",
      "Epoch: 543   Training Loss: 8.764997364407495 \n",
      "Epoch: 543   Validation Loss: 8.05745533158579 \n",
      "Epoch: 544   Training Loss: 8.697821783599979 \n",
      "Epoch: 544   Validation Loss: 8.229668461255109 \n",
      "Epoch: 545   Training Loss: 8.677241841866312 \n",
      "Epoch: 545   Validation Loss: 8.248077641158469 \n",
      "Epoch: 546   Training Loss: 8.647835815836572 \n",
      "Epoch: 546   Validation Loss: 8.186901823725137 \n",
      "Epoch: 547   Training Loss: 8.632751139600392 \n",
      "Epoch: 547   Validation Loss: 8.280322180499788 \n",
      "Epoch: 548   Training Loss: 8.592364318268814 \n",
      "Epoch: 548   Validation Loss: 8.380499878293937 \n",
      "Epoch: 549   Training Loss: 8.567074167013892 \n",
      "Epoch: 549   Validation Loss: 8.296304907944156 \n",
      "Epoch: 550   Training Loss: 8.827844799575288 \n",
      "Epoch: 550   Validation Loss: 7.979502036467764 \n",
      "Epoch: 551   Training Loss: 8.92326084354277 \n",
      "Epoch: 551   Validation Loss: 7.966292877730189 \n",
      "Epoch: 552   Training Loss: 8.897952275339678 \n",
      "Epoch: 552   Validation Loss: 7.985702040028603 \n",
      "Epoch: 553   Training Loss: 8.88021165533195 \n",
      "Epoch: 553   Validation Loss: 7.967169548560761 \n",
      "Epoch: 554   Training Loss: 8.856806331222424 \n",
      "Epoch: 554   Validation Loss: 7.945219207915426 \n",
      "Epoch: 555   Training Loss: 8.828548016376928 \n",
      "Epoch: 555   Validation Loss: 7.942592732754226 \n",
      "Epoch: 556   Training Loss: 8.799184611322913 \n",
      "Epoch: 556   Validation Loss: 7.965901630201552 \n",
      "Epoch: 557   Training Loss: 8.76963050829239 \n",
      "Epoch: 557   Validation Loss: 7.95786127555184 \n",
      "Epoch: 558   Training Loss: 8.760974160748159 \n",
      "Epoch: 558   Validation Loss: 7.9768179849581795 \n",
      "Epoch: 559   Training Loss: 8.7303482708427 \n",
      "Epoch: 559   Validation Loss: 8.009435576972448 \n",
      "Epoch: 560   Training Loss: 8.706766382288123 \n",
      "Epoch: 560   Validation Loss: 7.995923618025974 \n",
      "Epoch: 561   Training Loss: 8.723789377652938 \n",
      "Epoch: 561   Validation Loss: 8.03649097223197 \n",
      "Epoch: 562   Training Loss: 8.711721614471932 \n",
      "Epoch: 562   Validation Loss: 8.01467210265035 \n",
      "Epoch: 563   Training Loss: 8.68745130581057 \n",
      "Epoch: 563   Validation Loss: 8.01857783778717 \n",
      "Epoch: 564   Training Loss: 8.682506997008362 \n",
      "Epoch: 564   Validation Loss: 8.009024977258846 \n",
      "Epoch: 565   Training Loss: 8.648848284691613 \n",
      "Epoch: 565   Validation Loss: 8.073478930047527 \n",
      "Epoch: 566   Training Loss: 8.61395045409765 \n",
      "Epoch: 566   Validation Loss: 8.118267351614259 \n",
      "Epoch: 567   Training Loss: 8.551161471107205 \n",
      "Epoch: 567   Validation Loss: 8.153423161117178 \n",
      "Epoch: 568   Training Loss: 8.466311408269037 \n",
      "Epoch: 568   Validation Loss: 8.236625120856075 \n",
      "Epoch: 569   Training Loss: 8.409556436881992 \n",
      "Epoch: 569   Validation Loss: 8.343402278004215 \n",
      "Epoch: 570   Training Loss: 8.335684316567235 \n",
      "Epoch: 570   Validation Loss: 8.537108533171574 \n",
      "Epoch: 571   Training Loss: 8.288413416927797 \n",
      "Epoch: 571   Validation Loss: 8.462034537613444 \n",
      "Epoch: 572   Training Loss: 8.219368551612092 \n",
      "Epoch: 572   Validation Loss: 8.28504386143914 \n",
      "Epoch: 573   Training Loss: 8.296235534082015 \n",
      "Epoch: 573   Validation Loss: 8.23667923973691 \n",
      "Epoch: 574   Training Loss: 8.218439694625493 \n",
      "Epoch: 574   Validation Loss: 8.214255315276484 \n",
      "Epoch: 575   Training Loss: 8.111679410368216 \n",
      "Epoch: 575   Validation Loss: 8.69495807227032 \n",
      "Epoch: 576   Training Loss: 8.705281483049966 \n",
      "Epoch: 576   Validation Loss: 7.956223094425708 \n",
      "Epoch: 577   Training Loss: 8.958010858014733 \n",
      "Epoch: 577   Validation Loss: 7.963815177167869 \n",
      "Epoch: 578   Training Loss: 8.92358310156005 \n",
      "Epoch: 578   Validation Loss: 7.947019495424918 \n",
      "Epoch: 579   Training Loss: 8.8979659625689 \n",
      "Epoch: 579   Validation Loss: 7.969314998653718 \n",
      "Epoch: 580   Training Loss: 8.862334869461888 \n",
      "Epoch: 580   Validation Loss: 7.960497614260659 \n",
      "Epoch: 581   Training Loss: 8.831954373419284 \n",
      "Epoch: 581   Validation Loss: 7.983070982044123 \n",
      "Epoch: 582   Training Loss: 8.79663653913245 \n",
      "Epoch: 582   Validation Loss: 8.015040614496037 \n",
      "Epoch: 583   Training Loss: 8.769820466002344 \n",
      "Epoch: 583   Validation Loss: 8.048161218034414 \n",
      "Epoch: 584   Training Loss: 8.705887770179737 \n",
      "Epoch: 584   Validation Loss: 8.033349537242385 \n",
      "Epoch: 585   Training Loss: 8.636056986042595 \n",
      "Epoch: 585   Validation Loss: 8.07944457629618 \n",
      "Epoch: 586   Training Loss: 8.603629552276583 \n",
      "Epoch: 586   Validation Loss: 8.104959686332537 \n",
      "Epoch: 587   Training Loss: 8.536844597261775 \n",
      "Epoch: 587   Validation Loss: 8.031722053594681 \n",
      "Epoch: 588   Training Loss: 8.570364972557904 \n",
      "Epoch: 588   Validation Loss: 8.398825730073472 \n",
      "Epoch: 589   Training Loss: 8.518840933010473 \n",
      "Epoch: 589   Validation Loss: 8.375741600789544 \n",
      "Epoch: 590   Training Loss: 8.56398317677274 \n",
      "Epoch: 590   Validation Loss: 8.338820653970922 \n",
      "Epoch: 591   Training Loss: 8.470301529250703 \n",
      "Epoch: 591   Validation Loss: 8.282039829215824 \n",
      "Epoch: 592   Training Loss: 8.345080308362336 \n",
      "Epoch: 592   Validation Loss: 8.350663526078893 \n",
      "Epoch: 593   Training Loss: 8.4007378063896 \n",
      "Epoch: 593   Validation Loss: 8.558770263982328 \n",
      "Epoch: 594   Training Loss: 8.251359215989172 \n",
      "Epoch: 594   Validation Loss: 8.386735840561126 \n",
      "Epoch: 595   Training Loss: 8.233851419755503 \n",
      "Epoch: 595   Validation Loss: 8.286771522580336 \n",
      "Epoch: 596   Training Loss: 8.396656427144416 \n",
      "Epoch: 596   Validation Loss: 8.458278588527918 \n",
      "Epoch: 597   Training Loss: 8.527788554886635 \n",
      "Epoch: 597   Validation Loss: 8.111131579534073 \n",
      "Epoch: 598   Training Loss: 8.726888182775825 \n",
      "Epoch: 598   Validation Loss: 8.354483878581991 \n",
      "Epoch: 599   Training Loss: 8.580532329311131 \n",
      "Epoch: 599   Validation Loss: 8.443953493344209 \n",
      "Epoch: 600   Training Loss: 8.531789292082474 \n",
      "Epoch: 600   Validation Loss: 8.681381401528293 \n",
      "Epoch: 601   Training Loss: 8.638795813941849 \n",
      "Epoch: 601   Validation Loss: 8.696829520720316 \n",
      "Epoch: 602   Training Loss: 8.405900179281517 \n",
      "Epoch: 602   Validation Loss: 8.575228437603348 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 603   Training Loss: 8.2985965213112 \n",
      "Epoch: 603   Validation Loss: 8.565871568241985 \n",
      "Epoch: 604   Training Loss: 8.291274769363065 \n",
      "Epoch: 604   Validation Loss: 8.863749456700559 \n",
      "Epoch: 605   Training Loss: 8.29133388624748 \n",
      "Epoch: 605   Validation Loss: 8.530384405882408 \n",
      "Epoch: 606   Training Loss: 8.097230738245287 \n",
      "Epoch: 606   Validation Loss: 8.375620662283358 \n",
      "Epoch: 607   Training Loss: 8.250327407221128 \n",
      "Epoch: 607   Validation Loss: 8.23284300910461 \n",
      "Epoch: 608   Training Loss: 7.878256709138902 \n",
      "Epoch: 608   Validation Loss: 8.784300558661215 \n",
      "Epoch: 609   Training Loss: 7.873372659520327 \n",
      "Epoch: 609   Validation Loss: 8.446984154554084 \n",
      "Epoch: 610   Training Loss: 7.738713520105011 \n",
      "Epoch: 610   Validation Loss: 8.506682863615634 \n",
      "Epoch: 611   Training Loss: 7.642858307618978 \n",
      "Epoch: 611   Validation Loss: 8.537008507168956 \n",
      "Epoch: 612   Training Loss: 7.467420961181989 \n",
      "Epoch: 612   Validation Loss: 8.149905760992164 \n",
      "Epoch: 613   Training Loss: 9.002355722921607 \n",
      "Epoch: 613   Validation Loss: 8.01493323262548 \n",
      "Epoch: 614   Training Loss: 8.944744400613146 \n",
      "Epoch: 614   Validation Loss: 7.98666815425321 \n",
      "Epoch: 615   Training Loss: 8.923798132778687 \n",
      "Epoch: 615   Validation Loss: 8.013818443466702 \n",
      "Epoch: 616   Training Loss: 8.903872893128428 \n",
      "Epoch: 616   Validation Loss: 8.005979086800233 \n",
      "Epoch: 617   Training Loss: 8.859199488251923 \n",
      "Epoch: 617   Validation Loss: 7.999797194809394 \n",
      "Epoch: 618   Training Loss: 8.82488138901738 \n",
      "Epoch: 618   Validation Loss: 8.033472197683425 \n",
      "Epoch: 619   Training Loss: 8.785811684753426 \n",
      "Epoch: 619   Validation Loss: 8.020549194651135 \n",
      "Epoch: 620   Training Loss: 8.719533975397628 \n",
      "Epoch: 620   Validation Loss: 8.002283877509957 \n",
      "Epoch: 621   Training Loss: 8.679834543680577 \n",
      "Epoch: 621   Validation Loss: 8.050036341176083 \n",
      "Epoch: 622   Training Loss: 8.626454172550442 \n",
      "Epoch: 622   Validation Loss: 8.044627613953004 \n",
      "Epoch: 623   Training Loss: 8.543444743061523 \n",
      "Epoch: 623   Validation Loss: 8.058456190308206 \n",
      "Epoch: 624   Training Loss: 8.465595136637099 \n",
      "Epoch: 624   Validation Loss: 8.08804833369071 \n",
      "Epoch: 625   Training Loss: 8.439357481733305 \n",
      "Epoch: 625   Validation Loss: 8.091602593392341 \n",
      "Epoch: 626   Training Loss: 8.358817075258873 \n",
      "Epoch: 626   Validation Loss: 8.060054495976413 \n",
      "Epoch: 627   Training Loss: 8.331587785085462 \n",
      "Epoch: 627   Validation Loss: 8.135021666631413 \n",
      "Epoch: 628   Training Loss: 8.401478901004658 \n",
      "Epoch: 628   Validation Loss: 8.100651074768038 \n",
      "Epoch: 629   Training Loss: 8.477247775307752 \n",
      "Epoch: 629   Validation Loss: 8.160174766477818 \n",
      "Epoch: 630   Training Loss: 8.246261338558138 \n",
      "Epoch: 630   Validation Loss: 8.190813099589302 \n",
      "Epoch: 631   Training Loss: 8.117650691773104 \n",
      "Epoch: 631   Validation Loss: 8.228857798038257 \n",
      "Epoch: 632   Training Loss: 8.016706865251617 \n",
      "Epoch: 632   Validation Loss: 8.220630776292333 \n",
      "Epoch: 633   Training Loss: 8.202153917162958 \n",
      "Epoch: 633   Validation Loss: 8.20659234967825 \n",
      "Epoch: 634   Training Loss: 8.514672358966674 \n",
      "Epoch: 634   Validation Loss: 8.00431261321478 \n",
      "Epoch: 635   Training Loss: 8.379584143103841 \n",
      "Epoch: 635   Validation Loss: 8.345475671345161 \n",
      "Epoch: 636   Training Loss: 8.304534676040705 \n",
      "Epoch: 636   Validation Loss: 8.354554626237498 \n",
      "Epoch: 637   Training Loss: 8.15984522087865 \n",
      "Epoch: 637   Validation Loss: 8.380734481905174 \n",
      "Epoch: 638   Training Loss: 8.004981104410112 \n",
      "Epoch: 638   Validation Loss: 8.643577269234488 \n",
      "Epoch: 639   Training Loss: 7.726244997687528 \n",
      "Epoch: 639   Validation Loss: 8.63161687531612 \n",
      "Epoch: 640   Training Loss: 8.504273387603721 \n",
      "Epoch: 640   Validation Loss: 7.979229138184908 \n",
      "Epoch: 641   Training Loss: 8.283101033400873 \n",
      "Epoch: 641   Validation Loss: 8.945272721968747 \n",
      "Epoch: 642   Training Loss: 7.627308705101152 \n",
      "Epoch: 642   Validation Loss: 9.105107729916668 \n",
      "Epoch: 643   Training Loss: 7.445795463326827 \n",
      "Epoch: 643   Validation Loss: 8.80229130765475 \n",
      "Epoch: 644   Training Loss: 7.509188892891335 \n",
      "Epoch: 644   Validation Loss: 8.887306437971645 \n",
      "Epoch: 645   Training Loss: 7.442871118290609 \n",
      "Epoch: 645   Validation Loss: 8.490606657592192 \n",
      "Epoch: 646   Training Loss: 7.549687815343948 \n",
      "Epoch: 646   Validation Loss: 8.918588715319832 \n",
      "Epoch: 647   Training Loss: 7.5256304370847324 \n",
      "Epoch: 647   Validation Loss: 8.900467036278892 \n",
      "Epoch: 648   Training Loss: 7.506866349698474 \n",
      "Epoch: 648   Validation Loss: 8.098846311789627 \n",
      "Epoch: 649   Training Loss: 8.968130063513255 \n",
      "Epoch: 649   Validation Loss: 8.088475301043687 \n",
      "Epoch: 650   Training Loss: 7.710275301607067 \n",
      "Epoch: 650   Validation Loss: 8.579291602977449 \n",
      "Epoch: 651   Training Loss: 7.361019996273555 \n",
      "Epoch: 651   Validation Loss: 8.848416654316631 \n",
      "Epoch: 652   Training Loss: 7.51362431506324 \n",
      "Epoch: 652   Validation Loss: 8.042648893423385 \n",
      "Epoch: 653   Training Loss: 8.993930443838737 \n",
      "Epoch: 653   Validation Loss: 7.981173648583548 \n",
      "Epoch: 654   Training Loss: 8.923256581220631 \n",
      "Epoch: 654   Validation Loss: 7.974152783951722 \n",
      "Epoch: 655   Training Loss: 8.947435437686552 \n",
      "Epoch: 655   Validation Loss: 7.994047530700143 \n",
      "Epoch: 656   Training Loss: 8.915502368352707 \n",
      "Epoch: 656   Validation Loss: 8.014822515280297 \n",
      "Epoch: 657   Training Loss: 8.87247986724031 \n",
      "Epoch: 657   Validation Loss: 8.042706459675813 \n",
      "Epoch: 658   Training Loss: 8.848699680592302 \n",
      "Epoch: 658   Validation Loss: 8.06773355343379 \n",
      "Epoch: 659   Training Loss: 8.808233614725662 \n",
      "Epoch: 659   Validation Loss: 8.128146150317043 \n",
      "Epoch: 660   Training Loss: 8.751424762203614 \n",
      "Epoch: 660   Validation Loss: 8.154512019901691 \n",
      "Epoch: 661   Training Loss: 8.6980373728263 \n",
      "Epoch: 661   Validation Loss: 8.220216424575904 \n",
      "Epoch: 662   Training Loss: 8.6158370870773 \n",
      "Epoch: 662   Validation Loss: 8.230529002308806 \n",
      "Epoch: 663   Training Loss: 8.543454818975912 \n",
      "Epoch: 663   Validation Loss: 8.226449405361459 \n",
      "Epoch: 664   Training Loss: 8.470847472382456 \n",
      "Epoch: 664   Validation Loss: 8.321670917458315 \n",
      "Epoch: 665   Training Loss: 8.402040957924005 \n",
      "Epoch: 665   Validation Loss: 8.361090142805672 \n",
      "Epoch: 666   Training Loss: 8.426378244205269 \n",
      "Epoch: 666   Validation Loss: 8.39171715611579 \n",
      "Epoch: 667   Training Loss: 8.327833526870956 \n",
      "Epoch: 667   Validation Loss: 8.707219486909711 \n",
      "Epoch: 668   Training Loss: 8.187402775855375 \n",
      "Epoch: 668   Validation Loss: 8.836633403404202 \n",
      "Epoch: 669   Training Loss: 8.163656517281222 \n",
      "Epoch: 669   Validation Loss: 8.496603958938374 \n",
      "Epoch: 670   Training Loss: 8.048388983931938 \n",
      "Epoch: 670   Validation Loss: 8.68239875070961 \n",
      "Epoch: 671   Training Loss: 8.027672564815957 \n",
      "Epoch: 671   Validation Loss: 8.715461441522024 \n",
      "Epoch: 672   Training Loss: 7.850739257933965 \n",
      "Epoch: 672   Validation Loss: 8.84717484304675 \n",
      "Epoch: 673   Training Loss: 7.9308343993555255 \n",
      "Epoch: 673   Validation Loss: 9.145660742707292 \n",
      "Epoch: 674   Training Loss: 7.822135239730706 \n",
      "Epoch: 674   Validation Loss: 9.464240934473928 \n",
      "Epoch: 675   Training Loss: 8.866348643724292 \n",
      "Epoch: 675   Validation Loss: 7.948547804621275 \n",
      "Epoch: 676   Training Loss: 8.158414444005675 \n",
      "Epoch: 676   Validation Loss: 8.887086677289982 \n",
      "Epoch: 677   Training Loss: 8.267211006760917 \n",
      "Epoch: 677   Validation Loss: 8.678645054364784 \n",
      "Epoch: 678   Training Loss: 7.952515596468261 \n",
      "Epoch: 678   Validation Loss: 8.3195762158453 \n",
      "Epoch: 679   Training Loss: 7.742953025949666 \n",
      "Epoch: 679   Validation Loss: 8.39911382088955 \n",
      "Epoch: 680   Training Loss: 8.066084396022557 \n",
      "Epoch: 680   Validation Loss: 8.389774165774773 \n",
      "Epoch: 681   Training Loss: 7.885872776532851 \n",
      "Epoch: 681   Validation Loss: 8.457141769507693 \n",
      "Epoch: 682   Training Loss: 7.693295149292373 \n",
      "Epoch: 682   Validation Loss: 8.659034483019624 \n",
      "Epoch: 683   Training Loss: 8.062884949861456 \n",
      "Epoch: 683   Validation Loss: 8.024724580611412 \n",
      "Epoch: 684   Training Loss: 8.612634072227488 \n",
      "Epoch: 684   Validation Loss: 7.991400923785857 \n",
      "Epoch: 685   Training Loss: 8.82525585738917 \n",
      "Epoch: 685   Validation Loss: 8.214547343103009 \n",
      "Epoch: 686   Training Loss: 8.5511500777976 \n",
      "Epoch: 686   Validation Loss: 8.116777137882696 \n",
      "Epoch: 687   Training Loss: 8.771478636965298 \n",
      "Epoch: 687   Validation Loss: 8.154870714816369 \n",
      "Epoch: 688   Training Loss: 8.605202319646414 \n",
      "Epoch: 688   Validation Loss: 8.353925450663988 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 689   Training Loss: 8.397100675695789 \n",
      "Epoch: 689   Validation Loss: 8.448954177054627 \n",
      "Epoch: 690   Training Loss: 8.242507278336156 \n",
      "Epoch: 690   Validation Loss: 8.455726723877403 \n",
      "Epoch: 691   Training Loss: 8.127625343315978 \n",
      "Epoch: 691   Validation Loss: 8.481616777291315 \n",
      "Epoch: 692   Training Loss: 7.959941431357333 \n",
      "Epoch: 692   Validation Loss: 8.635552485166118 \n",
      "Epoch: 693   Training Loss: 8.207530578437995 \n",
      "Epoch: 693   Validation Loss: 8.136254728154487 \n",
      "Epoch: 694   Training Loss: 8.729662541312186 \n",
      "Epoch: 694   Validation Loss: 8.260385480561915 \n",
      "Epoch: 695   Training Loss: 8.15520393832876 \n",
      "Epoch: 695   Validation Loss: 8.594867693719765 \n",
      "Epoch: 696   Training Loss: 7.994836888461917 \n",
      "Epoch: 696   Validation Loss: 8.390302015222712 \n",
      "Epoch: 697   Training Loss: 7.8867910879047285 \n",
      "Epoch: 697   Validation Loss: 8.416291562283618 \n",
      "Epoch: 698   Training Loss: 7.577005043587172 \n",
      "Epoch: 698   Validation Loss: 8.429237586616848 \n",
      "Epoch: 699   Training Loss: 8.668697821775988 \n",
      "Epoch: 699   Validation Loss: 8.054801524634629 \n",
      "Epoch: 700   Training Loss: 7.9640061321123765 \n",
      "Epoch: 700   Validation Loss: 8.737365643104162 \n",
      "Epoch: 701   Training Loss: 7.543873431413566 \n",
      "Epoch: 701   Validation Loss: 8.758766340735471 \n",
      "Epoch: 702   Training Loss: 7.529551738795449 \n",
      "Epoch: 702   Validation Loss: 8.453958143847995 \n",
      "Epoch: 703   Training Loss: 8.06999424221324 \n",
      "Epoch: 703   Validation Loss: 8.235679155170349 \n",
      "Epoch: 704   Training Loss: 8.964131361167416 \n",
      "Epoch: 704   Validation Loss: 8.03652857940049 \n",
      "Epoch: 705   Training Loss: 8.85539794158814 \n",
      "Epoch: 705   Validation Loss: 8.122158288134573 \n",
      "Epoch: 706   Training Loss: 8.77506850569041 \n",
      "Epoch: 706   Validation Loss: 8.25269743710808 \n",
      "Epoch: 707   Training Loss: 8.716743226390228 \n",
      "Epoch: 707   Validation Loss: 8.33402643277103 \n",
      "Epoch: 708   Training Loss: 8.59733134517869 \n",
      "Epoch: 708   Validation Loss: 8.509156058005885 \n",
      "Epoch: 709   Training Loss: 8.558067210521694 \n",
      "Epoch: 709   Validation Loss: 8.66252120118446 \n",
      "Epoch: 710   Training Loss: 8.395497877098904 \n",
      "Epoch: 710   Validation Loss: 8.868116729816215 \n",
      "Epoch: 711   Training Loss: 8.245391448348935 \n",
      "Epoch: 711   Validation Loss: 9.521385335441316 \n",
      "Epoch: 712   Training Loss: 8.104435093023147 \n",
      "Epoch: 712   Validation Loss: 8.922177183149486 \n",
      "Epoch: 713   Training Loss: 8.059867380212616 \n",
      "Epoch: 713   Validation Loss: 9.316692433536778 \n",
      "Epoch: 714   Training Loss: 7.8753639850594155 \n",
      "Epoch: 714   Validation Loss: 10.17574160965651 \n",
      "Epoch: 715   Training Loss: 7.814373996714581 \n",
      "Epoch: 715   Validation Loss: 10.462758521733987 \n",
      "Epoch: 716   Training Loss: 8.011149716992998 \n",
      "Epoch: 716   Validation Loss: 12.000628036019496 \n",
      "Epoch: 717   Training Loss: 7.929006162412123 \n",
      "Epoch: 717   Validation Loss: 9.763035665859158 \n",
      "Epoch: 718   Training Loss: 7.808524444385944 \n",
      "Epoch: 718   Validation Loss: 8.943338985736741 \n",
      "Epoch: 719   Training Loss: 7.786523906702221 \n",
      "Epoch: 719   Validation Loss: 8.3717553582133 \n",
      "Epoch: 720   Training Loss: 7.50604546437468 \n",
      "Epoch: 720   Validation Loss: 8.523464381224604 \n",
      "Epoch: 721   Training Loss: 7.250600789864855 \n",
      "Epoch: 721   Validation Loss: 8.802895735453607 \n",
      "Epoch: 722   Training Loss: 9.029032999732921 \n",
      "Epoch: 722   Validation Loss: 7.985793213831364 \n",
      "Epoch: 723   Training Loss: 8.90225021897014 \n",
      "Epoch: 723   Validation Loss: 8.05900402468267 \n",
      "Epoch: 724   Training Loss: 8.847101528896859 \n",
      "Epoch: 724   Validation Loss: 8.10643344479406 \n",
      "Epoch: 725   Training Loss: 8.809236514977302 \n",
      "Epoch: 725   Validation Loss: 8.134585765276311 \n",
      "Epoch: 726   Training Loss: 8.804520611318292 \n",
      "Epoch: 726   Validation Loss: 8.140627721839248 \n",
      "Epoch: 727   Training Loss: 8.691712256725813 \n",
      "Epoch: 727   Validation Loss: 8.255296143166875 \n",
      "Epoch: 728   Training Loss: 8.532973597727251 \n",
      "Epoch: 728   Validation Loss: 8.273651275936928 \n",
      "Epoch: 729   Training Loss: 8.441145387949424 \n",
      "Epoch: 729   Validation Loss: 8.213373170634215 \n",
      "Epoch: 730   Training Loss: 8.319164019100846 \n",
      "Epoch: 730   Validation Loss: 8.30195273717322 \n",
      "Epoch: 731   Training Loss: 8.171522491987577 \n",
      "Epoch: 731   Validation Loss: 8.296506003911944 \n",
      "Epoch: 732   Training Loss: 8.039464988083479 \n",
      "Epoch: 732   Validation Loss: 8.366687614171662 \n",
      "Epoch: 733   Training Loss: 7.92518926034253 \n",
      "Epoch: 733   Validation Loss: 8.458649464278974 \n",
      "Epoch: 734   Training Loss: 7.8889332250363156 \n",
      "Epoch: 734   Validation Loss: 8.52029038820985 \n",
      "Epoch: 735   Training Loss: 8.139738216177737 \n",
      "Epoch: 735   Validation Loss: 8.717434343870263 \n",
      "Epoch: 736   Training Loss: 8.51980042019732 \n",
      "Epoch: 736   Validation Loss: 8.122303422958487 \n",
      "Epoch: 737   Training Loss: 8.887080068824346 \n",
      "Epoch: 737   Validation Loss: 8.21018104674191 \n",
      "Epoch: 738   Training Loss: 8.74046087194095 \n",
      "Epoch: 738   Validation Loss: 8.313186005147614 \n",
      "Epoch: 739   Training Loss: 8.536406317517015 \n",
      "Epoch: 739   Validation Loss: 8.22585533224667 \n",
      "Epoch: 740   Training Loss: 8.593180597939872 \n",
      "Epoch: 740   Validation Loss: 8.512393656863521 \n",
      "Epoch: 741   Training Loss: 8.378753740687907 \n",
      "Epoch: 741   Validation Loss: 8.105992034078161 \n",
      "Epoch: 742   Training Loss: 8.454646787812559 \n",
      "Epoch: 742   Validation Loss: 8.61818901650297 \n",
      "Epoch: 743   Training Loss: 8.243775317722054 \n",
      "Epoch: 743   Validation Loss: 8.142920097120934 \n",
      "Epoch: 744   Training Loss: 8.261694700157856 \n",
      "Epoch: 744   Validation Loss: 8.7054311637539 \n",
      "Epoch: 745   Training Loss: 7.948064556423698 \n",
      "Epoch: 745   Validation Loss: 8.64423829289231 \n",
      "Epoch: 746   Training Loss: 7.656775578518152 \n",
      "Epoch: 746   Validation Loss: 8.833622505585662 \n",
      "Epoch: 747   Training Loss: 8.289318081563051 \n",
      "Epoch: 747   Validation Loss: 8.28833434695973 \n",
      "Epoch: 748   Training Loss: 8.151627933747724 \n",
      "Epoch: 748   Validation Loss: 9.238846031468775 \n",
      "Epoch: 749   Training Loss: 7.38329450341349 \n",
      "Epoch: 749   Validation Loss: 9.197045275113325 \n",
      "Epoch: 750   Training Loss: 7.179783510395413 \n",
      "Epoch: 750   Validation Loss: 9.248341838447894 \n",
      "Epoch: 751   Training Loss: 7.225874022448657 \n",
      "Epoch: 751   Validation Loss: 9.440098743302126 \n",
      "Epoch: 752   Training Loss: 7.356638223814375 \n",
      "Epoch: 752   Validation Loss: 8.631119761524152 \n",
      "Epoch: 753   Training Loss: 7.382370117653304 \n",
      "Epoch: 753   Validation Loss: 8.24136605871531 \n",
      "Epoch: 754   Training Loss: 7.363790375974172 \n",
      "Epoch: 754   Validation Loss: 8.584140439909598 \n",
      "Epoch: 755   Training Loss: 7.267954137128852 \n",
      "Epoch: 755   Validation Loss: 8.59039322095768 \n",
      "Epoch: 756   Training Loss: 7.151459200771116 \n",
      "Epoch: 756   Validation Loss: 8.567947579434678 \n",
      "Epoch: 757   Training Loss: 6.8724137586790635 \n",
      "Epoch: 757   Validation Loss: 8.916177494763396 \n",
      "Epoch: 758   Training Loss: 6.765662138910912 \n",
      "Epoch: 758   Validation Loss: 8.855756122015364 \n",
      "Epoch: 759   Training Loss: 7.184946337876108 \n",
      "Epoch: 759   Validation Loss: 9.106327600681947 \n",
      "Epoch: 760   Training Loss: 7.858474362457384 \n",
      "Epoch: 760   Validation Loss: 8.010950538825968 \n",
      "Epoch: 761   Training Loss: 8.918276900126685 \n",
      "Epoch: 761   Validation Loss: 8.065874140979723 \n",
      "Epoch: 762   Training Loss: 8.831129006090745 \n",
      "Epoch: 762   Validation Loss: 8.068202726386694 \n",
      "Epoch: 763   Training Loss: 8.743722742126549 \n",
      "Epoch: 763   Validation Loss: 8.09206855435804 \n",
      "Epoch: 764   Training Loss: 8.64281907247936 \n",
      "Epoch: 764   Validation Loss: 8.233326095686191 \n",
      "Epoch: 765   Training Loss: 8.540122075724423 \n",
      "Epoch: 765   Validation Loss: 8.444827903664361 \n",
      "Epoch: 766   Training Loss: 8.387426100704793 \n",
      "Epoch: 766   Validation Loss: 8.493267695618432 \n",
      "Epoch: 767   Training Loss: 8.239379698302564 \n",
      "Epoch: 767   Validation Loss: 9.151702691904372 \n",
      "Epoch: 768   Training Loss: 8.093880094399644 \n",
      "Epoch: 768   Validation Loss: 9.120918316993242 \n",
      "Epoch: 769   Training Loss: 7.946039209059446 \n",
      "Epoch: 769   Validation Loss: 9.48059446368693 \n",
      "Epoch: 770   Training Loss: 7.823095994369813 \n",
      "Epoch: 770   Validation Loss: 9.883560018048543 \n",
      "Epoch: 771   Training Loss: 7.620732691015 \n",
      "Epoch: 771   Validation Loss: 9.308870652920255 \n",
      "Epoch: 772   Training Loss: 7.551662799948762 \n",
      "Epoch: 772   Validation Loss: 9.49931676039912 \n",
      "Epoch: 773   Training Loss: 7.581517489705843 \n",
      "Epoch: 773   Validation Loss: 9.255308166404461 \n",
      "Epoch: 774   Training Loss: 7.510011567401412 \n",
      "Epoch: 774   Validation Loss: 9.51108378642659 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 775   Training Loss: 7.215762417130981 \n",
      "Epoch: 775   Validation Loss: 9.834297890997389 \n",
      "Epoch: 776   Training Loss: 7.070482763633611 \n",
      "Epoch: 776   Validation Loss: 9.35634221323423 \n",
      "Epoch: 777   Training Loss: 6.918762207923994 \n",
      "Epoch: 777   Validation Loss: 9.736223378608573 \n",
      "Epoch: 778   Training Loss: 7.833765087975371 \n",
      "Epoch: 778   Validation Loss: 8.1800840837793 \n",
      "Epoch: 779   Training Loss: 8.217268046329547 \n",
      "Epoch: 779   Validation Loss: 9.431155832424347 \n",
      "Epoch: 780   Training Loss: 6.69024487110107 \n",
      "Epoch: 780   Validation Loss: 9.735088174091652 \n",
      "Epoch: 781   Training Loss: 6.435493433882657 \n",
      "Epoch: 781   Validation Loss: 10.05950081429496 \n",
      "Epoch: 782   Training Loss: 7.544801487838834 \n",
      "Epoch: 782   Validation Loss: 9.372794355696985 \n",
      "Epoch: 783   Training Loss: 7.751121728160328 \n",
      "Epoch: 783   Validation Loss: 9.652785238111909 \n",
      "Epoch: 784   Training Loss: 7.083519052748573 \n",
      "Epoch: 784   Validation Loss: 9.20474838113468 \n",
      "Epoch: 785   Training Loss: 6.64206113236911 \n",
      "Epoch: 785   Validation Loss: 9.276188916135256 \n",
      "Epoch: 786   Training Loss: 6.336154743755186 \n",
      "Epoch: 786   Validation Loss: 9.75854586570834 \n",
      "Epoch: 787   Training Loss: 6.176008105487077 \n",
      "Epoch: 787   Validation Loss: 9.857078219680115 \n",
      "Epoch: 788   Training Loss: 5.872586265960709 \n",
      "Epoch: 788   Validation Loss: 9.593138278927395 \n",
      "Epoch: 789   Training Loss: 5.779263078151335 \n",
      "Epoch: 789   Validation Loss: 9.231778824190455 \n",
      "Epoch: 790   Training Loss: 9.0287590752543 \n",
      "Epoch: 790   Validation Loss: 8.09766569487423 \n",
      "Epoch: 791   Training Loss: 8.817373283251136 \n",
      "Epoch: 791   Validation Loss: 8.222794842475817 \n",
      "Epoch: 792   Training Loss: 8.73785423867052 \n",
      "Epoch: 792   Validation Loss: 8.55829304261327 \n",
      "Epoch: 793   Training Loss: 8.59536309539819 \n",
      "Epoch: 793   Validation Loss: 8.729041417586721 \n",
      "Epoch: 794   Training Loss: 8.465846359623248 \n",
      "Epoch: 794   Validation Loss: 9.015169649150403 \n",
      "Epoch: 795   Training Loss: 8.334578106093685 \n",
      "Epoch: 795   Validation Loss: 9.084628824212123 \n",
      "Epoch: 796   Training Loss: 8.158543885316531 \n",
      "Epoch: 796   Validation Loss: 9.437155475659917 \n",
      "Epoch: 797   Training Loss: 8.027786785987308 \n",
      "Epoch: 797   Validation Loss: 9.449402242975516 \n",
      "Epoch: 798   Training Loss: 7.830798830711784 \n",
      "Epoch: 798   Validation Loss: 9.819863896706245 \n",
      "Epoch: 799   Training Loss: 7.659093182079855 \n",
      "Epoch: 799   Validation Loss: 9.483279945934191 \n",
      "Epoch: 800   Training Loss: 7.455685217253663 \n",
      "Epoch: 800   Validation Loss: 9.454280905559298 \n",
      "Epoch: 801   Training Loss: 7.371708432042005 \n",
      "Epoch: 801   Validation Loss: 9.400370105746099 \n",
      "Epoch: 802   Training Loss: 7.258185179401076 \n",
      "Epoch: 802   Validation Loss: 9.780281865885998 \n",
      "Epoch: 803   Training Loss: 7.405575120382612 \n",
      "Epoch: 803   Validation Loss: 9.917079636269474 \n",
      "Epoch: 804   Training Loss: 7.226882570237342 \n",
      "Epoch: 804   Validation Loss: 9.763462338263375 \n",
      "Epoch: 805   Training Loss: 7.04192998517822 \n",
      "Epoch: 805   Validation Loss: 9.969475782150402 \n",
      "Epoch: 806   Training Loss: 6.9672991159199835 \n",
      "Epoch: 806   Validation Loss: 10.522062993544257 \n",
      "Epoch: 807   Training Loss: 6.6600330854978855 \n",
      "Epoch: 807   Validation Loss: 9.952382011830858 \n",
      "Epoch: 808   Training Loss: 6.6188557530453584 \n",
      "Epoch: 808   Validation Loss: 9.60970932994219 \n",
      "Epoch: 809   Training Loss: 6.364257983413149 \n",
      "Epoch: 809   Validation Loss: 10.389778750298493 \n",
      "Epoch: 810   Training Loss: 6.184239626082288 \n",
      "Epoch: 810   Validation Loss: 10.869755478487868 \n",
      "Epoch: 811   Training Loss: 5.878459000597456 \n",
      "Epoch: 811   Validation Loss: 10.79835196852309 \n",
      "Epoch: 812   Training Loss: 5.850659757031891 \n",
      "Epoch: 812   Validation Loss: 9.61023190387654 \n",
      "Epoch: 813   Training Loss: 5.9133327080596105 \n",
      "Epoch: 813   Validation Loss: 9.900725776359646 \n",
      "Epoch: 814   Training Loss: 5.5857492184430315 \n",
      "Epoch: 814   Validation Loss: 9.932294599950547 \n",
      "Epoch: 815   Training Loss: 5.460398711021709 \n",
      "Epoch: 815   Validation Loss: 10.264570198501978 \n",
      "Epoch: 816   Training Loss: 5.734999816289282 \n",
      "Epoch: 816   Validation Loss: 9.983745294593067 \n",
      "Epoch: 817   Training Loss: 6.261212076141318 \n",
      "Epoch: 817   Validation Loss: 9.439459840473202 \n",
      "Epoch: 818   Training Loss: 6.174460202993812 \n",
      "Epoch: 818   Validation Loss: 9.375685226507485 \n",
      "Epoch: 819   Training Loss: 5.906489641260155 \n",
      "Epoch: 819   Validation Loss: 9.418427059083609 \n",
      "Epoch: 820   Training Loss: 5.509858435806754 \n",
      "Epoch: 820   Validation Loss: 10.751531053009677 \n",
      "Epoch: 821   Training Loss: 5.848078494968229 \n",
      "Epoch: 821   Validation Loss: 9.793877551041337 \n",
      "Epoch: 822   Training Loss: 6.027474120875089 \n",
      "Epoch: 822   Validation Loss: 10.126555554811688 \n",
      "Epoch: 823   Training Loss: 6.022046366444038 \n",
      "Epoch: 823   Validation Loss: 9.738681820881709 \n",
      "Epoch: 824   Training Loss: 5.588176904104265 \n",
      "Epoch: 824   Validation Loss: 9.606227046445323 \n",
      "Epoch: 825   Training Loss: 5.632644139564478 \n",
      "Epoch: 825   Validation Loss: 9.891865277958827 \n",
      "Epoch: 826   Training Loss: 9.081099465061754 \n",
      "Epoch: 826   Validation Loss: 8.178814479511898 \n",
      "Epoch: 827   Training Loss: 8.735610010159876 \n",
      "Epoch: 827   Validation Loss: 8.331172833562208 \n",
      "Epoch: 828   Training Loss: 8.477982768271143 \n",
      "Epoch: 828   Validation Loss: 8.673572405183998 \n",
      "Epoch: 829   Training Loss: 8.20233027531134 \n",
      "Epoch: 829   Validation Loss: 9.024705064588279 \n",
      "Epoch: 830   Training Loss: 7.8890094160188164 \n",
      "Epoch: 830   Validation Loss: 9.299112297894123 \n",
      "Epoch: 831   Training Loss: 7.634307711692947 \n",
      "Epoch: 831   Validation Loss: 9.719179551590576 \n",
      "Epoch: 832   Training Loss: 7.302163596459658 \n",
      "Epoch: 832   Validation Loss: 9.742750900633013 \n",
      "Epoch: 833   Training Loss: 6.941237534174789 \n",
      "Epoch: 833   Validation Loss: 10.176855463164394 \n",
      "Epoch: 834   Training Loss: 6.515923410576419 \n",
      "Epoch: 834   Validation Loss: 9.975118514492253 \n",
      "Epoch: 835   Training Loss: 6.095124225089522 \n",
      "Epoch: 835   Validation Loss: 10.19591020226168 \n",
      "Epoch: 836   Training Loss: 6.11550606963401 \n",
      "Epoch: 836   Validation Loss: 10.20258426696062 \n",
      "Epoch: 837   Training Loss: 5.8884592978908845 \n",
      "Epoch: 837   Validation Loss: 10.482445065121476 \n",
      "Epoch: 838   Training Loss: 5.878861341289493 \n",
      "Epoch: 838   Validation Loss: 10.198744339948236 \n",
      "Epoch: 839   Training Loss: 5.655045343083329 \n",
      "Epoch: 839   Validation Loss: 11.014494858541681 \n",
      "Epoch: 840   Training Loss: 5.472469538865456 \n",
      "Epoch: 840   Validation Loss: 11.212499966749714 \n",
      "Epoch: 841   Training Loss: 5.898542235258113 \n",
      "Epoch: 841   Validation Loss: 11.177340652452937 \n",
      "Epoch: 842   Training Loss: 6.35032229723868 \n",
      "Epoch: 842   Validation Loss: 10.31998576593275 \n",
      "Epoch: 843   Training Loss: 5.972018599666702 \n",
      "Epoch: 843   Validation Loss: 10.362863187589165 \n",
      "Epoch: 844   Training Loss: 5.5011837743757575 \n",
      "Epoch: 844   Validation Loss: 10.599099553690758 \n",
      "Epoch: 845   Training Loss: 5.368917610866451 \n",
      "Epoch: 845   Validation Loss: 11.087255353097701 \n",
      "Epoch: 846   Training Loss: 4.9175260275952555 \n",
      "Epoch: 846   Validation Loss: 10.241992822856798 \n",
      "Epoch: 847   Training Loss: 5.0876277220384996 \n",
      "Epoch: 847   Validation Loss: 10.501009841942125 \n",
      "Epoch: 848   Training Loss: 4.8568246406451605 \n",
      "Epoch: 848   Validation Loss: 11.156664809694178 \n",
      "Epoch: 849   Training Loss: 4.514284637183826 \n",
      "Epoch: 849   Validation Loss: 10.33884002143931 \n",
      "Epoch: 850   Training Loss: 4.484217972194828 \n",
      "Epoch: 850   Validation Loss: 10.56356675569175 \n",
      "Epoch: 851   Training Loss: 4.257729503678986 \n",
      "Epoch: 851   Validation Loss: 10.486697123937516 \n",
      "Epoch: 852   Training Loss: 4.294359526756703 \n",
      "Epoch: 852   Validation Loss: 10.878119436710193 \n",
      "Epoch: 853   Training Loss: 4.486418692054156 \n",
      "Epoch: 853   Validation Loss: 10.087349150168398 \n",
      "Epoch: 854   Training Loss: 4.309782979956039 \n",
      "Epoch: 854   Validation Loss: 10.595958657100176 \n",
      "Epoch: 855   Training Loss: 4.37732508364216 \n",
      "Epoch: 855   Validation Loss: 10.65366555040935 \n",
      "Epoch: 856   Training Loss: 4.086882928328107 \n",
      "Epoch: 856   Validation Loss: 10.25878326149554 \n",
      "Epoch: 857   Training Loss: 4.50807036920791 \n",
      "Epoch: 857   Validation Loss: 10.383653617046463 \n",
      "Epoch: 858   Training Loss: 4.450304713596044 \n",
      "Epoch: 858   Validation Loss: 9.887360017315691 \n",
      "Epoch: 859   Training Loss: 8.184530247491052 \n",
      "Epoch: 859   Validation Loss: 8.001450975307316 \n",
      "Epoch: 860   Training Loss: 8.916560927259969 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 860   Validation Loss: 7.982275622870074 \n",
      "Epoch: 861   Training Loss: 8.852028583480601 \n",
      "Epoch: 861   Validation Loss: 8.069880749433974 \n",
      "Epoch: 862   Training Loss: 8.784415720312666 \n",
      "Epoch: 862   Validation Loss: 8.06780714410965 \n",
      "Epoch: 863   Training Loss: 8.693239134857011 \n",
      "Epoch: 863   Validation Loss: 8.064395244266036 \n",
      "Epoch: 864   Training Loss: 8.583372731771977 \n",
      "Epoch: 864   Validation Loss: 8.31666623294664 \n",
      "Epoch: 865   Training Loss: 8.417945380986072 \n",
      "Epoch: 865   Validation Loss: 8.317148148360817 \n",
      "Epoch: 866   Training Loss: 8.23944936607425 \n",
      "Epoch: 866   Validation Loss: 8.534124282456938 \n",
      "Epoch: 867   Training Loss: 7.998793069905239 \n",
      "Epoch: 867   Validation Loss: 8.82585634978569 \n",
      "Epoch: 868   Training Loss: 7.7452655884981825 \n",
      "Epoch: 868   Validation Loss: 8.754511472837637 \n",
      "Epoch: 869   Training Loss: 7.4216540557862185 \n",
      "Epoch: 869   Validation Loss: 8.650334246846226 \n",
      "Epoch: 870   Training Loss: 7.156588298068964 \n",
      "Epoch: 870   Validation Loss: 8.763200393555033 \n",
      "Epoch: 871   Training Loss: 7.031699162768039 \n",
      "Epoch: 871   Validation Loss: 8.685693723035252 \n",
      "Epoch: 872   Training Loss: 6.775311071115838 \n",
      "Epoch: 872   Validation Loss: 8.72094981332413 \n",
      "Epoch: 873   Training Loss: 6.666919594115793 \n",
      "Epoch: 873   Validation Loss: 9.803404756113887 \n",
      "Epoch: 874   Training Loss: 6.267747590272144 \n",
      "Epoch: 874   Validation Loss: 8.982569681887204 \n",
      "Epoch: 875   Training Loss: 6.263119137380483 \n",
      "Epoch: 875   Validation Loss: 9.988373132538982 \n",
      "Epoch: 876   Training Loss: 6.134547523414916 \n",
      "Epoch: 876   Validation Loss: 9.756363285209776 \n",
      "Epoch: 877   Training Loss: 5.917703775374652 \n",
      "Epoch: 877   Validation Loss: 10.929041140040434 \n",
      "Epoch: 878   Training Loss: 5.870755877453723 \n",
      "Epoch: 878   Validation Loss: 9.994896717457774 \n",
      "Epoch: 879   Training Loss: 6.245060869839802 \n",
      "Epoch: 879   Validation Loss: 9.802312737594669 \n",
      "Epoch: 880   Training Loss: 5.88141551831569 \n",
      "Epoch: 880   Validation Loss: 10.004544087918992 \n",
      "Epoch: 881   Training Loss: 5.455419164245249 \n",
      "Epoch: 881   Validation Loss: 9.990558139431911 \n",
      "Epoch: 882   Training Loss: 5.54838335418401 \n",
      "Epoch: 882   Validation Loss: 10.24778708884111 \n",
      "Epoch: 883   Training Loss: 7.426684887993635 \n",
      "Epoch: 883   Validation Loss: 8.054103423227659 \n",
      "Epoch: 884   Training Loss: 8.018768120626014 \n",
      "Epoch: 884   Validation Loss: 10.964873716947105 \n",
      "Epoch: 885   Training Loss: 6.170294045574924 \n",
      "Epoch: 885   Validation Loss: 9.804731033773471 \n",
      "Epoch: 886   Training Loss: 5.6102254487130425 \n",
      "Epoch: 886   Validation Loss: 9.932928954349158 \n",
      "Epoch: 887   Training Loss: 5.255684138338822 \n",
      "Epoch: 887   Validation Loss: 10.392481957299946 \n",
      "Epoch: 888   Training Loss: 4.759346617392536 \n",
      "Epoch: 888   Validation Loss: 10.553204721511445 \n",
      "Epoch: 889   Training Loss: 4.668513940347111 \n",
      "Epoch: 889   Validation Loss: 10.110944642173644 \n",
      "Epoch: 890   Training Loss: 4.988963650706509 \n",
      "Epoch: 890   Validation Loss: 11.122269593661962 \n",
      "Epoch: 891   Training Loss: 4.381762700229596 \n",
      "Epoch: 891   Validation Loss: 10.599773014585177 \n",
      "Epoch: 892   Training Loss: 4.182527968552903 \n",
      "Epoch: 892   Validation Loss: 10.32889537254426 \n",
      "Epoch: 893   Training Loss: 4.206367935363475 \n",
      "Epoch: 893   Validation Loss: 11.155101955035288 \n",
      "Epoch: 894   Training Loss: 4.138886671663462 \n",
      "Epoch: 894   Validation Loss: 11.774564221922516 \n",
      "Epoch: 895   Training Loss: 4.361566934776286 \n",
      "Epoch: 895   Validation Loss: 10.929108282689315 \n",
      "Epoch: 896   Training Loss: 4.400990850763757 \n",
      "Epoch: 896   Validation Loss: 11.505416573784832 \n",
      "Epoch: 897   Training Loss: 9.141470330253352 \n",
      "Epoch: 897   Validation Loss: 8.100778193671463 \n",
      "Epoch: 898   Training Loss: 8.833650968851398 \n",
      "Epoch: 898   Validation Loss: 8.227163379806123 \n",
      "Epoch: 899   Training Loss: 8.725084443409061 \n",
      "Epoch: 899   Validation Loss: 8.542568175317719 \n",
      "Epoch: 900   Training Loss: 5.920518979957392 \n",
      "Epoch: 900   Validation Loss: 9.719805339039304 \n",
      "Epoch: 901   Training Loss: 7.020090663471977 \n",
      "Epoch: 901   Validation Loss: 10.797718006213481 \n",
      "Epoch: 902   Training Loss: 6.007023996540811 \n",
      "Epoch: 902   Validation Loss: 9.845991133027807 \n",
      "Epoch: 903   Training Loss: 5.981054704028979 \n",
      "Epoch: 903   Validation Loss: 10.653224249597212 \n",
      "Epoch: 904   Training Loss: 5.4302119947377 \n",
      "Epoch: 904   Validation Loss: 10.747210914179062 \n",
      "Epoch: 905   Training Loss: 4.923941922637104 \n",
      "Epoch: 905   Validation Loss: 11.818517852234137 \n",
      "Epoch: 906   Training Loss: 4.536897635502521 \n",
      "Epoch: 906   Validation Loss: 12.267197432097618 \n",
      "Epoch: 907   Training Loss: 4.732845933127165 \n",
      "Epoch: 907   Validation Loss: 10.509259586965976 \n",
      "Epoch: 908   Training Loss: 4.84295895678699 \n",
      "Epoch: 908   Validation Loss: 11.91279709622471 \n",
      "Epoch: 909   Training Loss: 4.471659413176191 \n",
      "Epoch: 909   Validation Loss: 10.4743356984088 \n",
      "Epoch: 910   Training Loss: 4.443493434147903 \n",
      "Epoch: 910   Validation Loss: 10.756523904571207 \n",
      "Epoch: 911   Training Loss: 4.435174011108142 \n",
      "Epoch: 911   Validation Loss: 10.84993845045857 \n",
      "Epoch: 912   Training Loss: 4.460527678123132 \n",
      "Epoch: 912   Validation Loss: 10.746585933203265 \n",
      "Epoch: 913   Training Loss: 4.055997383959507 \n",
      "Epoch: 913   Validation Loss: 10.170179656500824 \n",
      "Epoch: 914   Training Loss: 9.059990638754114 \n",
      "Epoch: 914   Validation Loss: 7.9747473088237975 \n",
      "Epoch: 915   Training Loss: 8.882092454730858 \n",
      "Epoch: 915   Validation Loss: 8.070685182758737 \n",
      "Epoch: 916   Training Loss: 8.75386508600463 \n",
      "Epoch: 916   Validation Loss: 8.084263740210009 \n",
      "Epoch: 917   Training Loss: 8.623654103142131 \n",
      "Epoch: 917   Validation Loss: 8.192938349994366 \n",
      "Epoch: 918   Training Loss: 8.42099953777603 \n",
      "Epoch: 918   Validation Loss: 8.30151082659554 \n",
      "Epoch: 919   Training Loss: 8.19673281808759 \n",
      "Epoch: 919   Validation Loss: 8.354678018696916 \n",
      "Epoch: 920   Training Loss: 7.851224350968566 \n",
      "Epoch: 920   Validation Loss: 8.641768888253589 \n",
      "Epoch: 921   Training Loss: 7.51473050128844 \n",
      "Epoch: 921   Validation Loss: 8.738007963168558 \n",
      "Epoch: 922   Training Loss: 7.175296377482349 \n",
      "Epoch: 922   Validation Loss: 9.053351558538722 \n",
      "Epoch: 923   Training Loss: 6.838152179309396 \n",
      "Epoch: 923   Validation Loss: 9.396640433321396 \n",
      "Epoch: 924   Training Loss: 6.657549504865189 \n",
      "Epoch: 924   Validation Loss: 9.57731141759807 \n",
      "Epoch: 925   Training Loss: 6.474935332968994 \n",
      "Epoch: 925   Validation Loss: 9.61737684534966 \n",
      "Epoch: 926   Training Loss: 6.115039905488892 \n",
      "Epoch: 926   Validation Loss: 9.617080989977241 \n",
      "Epoch: 927   Training Loss: 5.705610147018775 \n",
      "Epoch: 927   Validation Loss: 10.444994743122823 \n",
      "Epoch: 928   Training Loss: 5.7021421202491345 \n",
      "Epoch: 928   Validation Loss: 11.313421981752747 \n",
      "Epoch: 929   Training Loss: 5.5274644665572445 \n",
      "Epoch: 929   Validation Loss: 11.056196449073937 \n",
      "Epoch: 930   Training Loss: 5.180823608788649 \n",
      "Epoch: 930   Validation Loss: 11.868042239467613 \n",
      "Epoch: 931   Training Loss: 5.174672602034891 \n",
      "Epoch: 931   Validation Loss: 12.598197126607928 \n",
      "Epoch: 932   Training Loss: 5.266087170423513 \n",
      "Epoch: 932   Validation Loss: 11.374772952576167 \n",
      "Epoch: 933   Training Loss: 5.016328430029842 \n",
      "Epoch: 933   Validation Loss: 11.309436900854731 \n",
      "Epoch: 934   Training Loss: 6.738484215477397 \n",
      "Epoch: 934   Validation Loss: 8.109694305049617 \n",
      "Epoch: 935   Training Loss: 8.9076947997211 \n",
      "Epoch: 935   Validation Loss: 8.089647387122321 \n",
      "Epoch: 936   Training Loss: 8.737076018543478 \n",
      "Epoch: 936   Validation Loss: 8.193502406875034 \n",
      "Epoch: 937   Training Loss: 8.547976561244433 \n",
      "Epoch: 937   Validation Loss: 8.284420049224929 \n",
      "Epoch: 938   Training Loss: 8.345331910176311 \n",
      "Epoch: 938   Validation Loss: 8.473883297762109 \n",
      "Epoch: 939   Training Loss: 8.090947146514752 \n",
      "Epoch: 939   Validation Loss: 8.834144130504379 \n",
      "Epoch: 940   Training Loss: 7.771975573930755 \n",
      "Epoch: 940   Validation Loss: 9.122609456210501 \n",
      "Epoch: 941   Training Loss: 7.361002385133955 \n",
      "Epoch: 941   Validation Loss: 9.522037321992944 \n",
      "Epoch: 942   Training Loss: 6.9391362506235925 \n",
      "Epoch: 942   Validation Loss: 9.948253593182761 \n",
      "Epoch: 943   Training Loss: 6.502627912111089 \n",
      "Epoch: 943   Validation Loss: 10.469431062427919 \n",
      "Epoch: 944   Training Loss: 6.0446551190762285 \n",
      "Epoch: 944   Validation Loss: 10.507989013898705 \n",
      "Epoch: 945   Training Loss: 5.673700587362039 \n",
      "Epoch: 945   Validation Loss: 11.178107197277455 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 946   Training Loss: 5.3139052837443 \n",
      "Epoch: 946   Validation Loss: 10.568911020589164 \n",
      "Epoch: 947   Training Loss: 5.06991686065365 \n",
      "Epoch: 947   Validation Loss: 10.860345400113923 \n",
      "Epoch: 948   Training Loss: 5.13493608993405 \n",
      "Epoch: 948   Validation Loss: 11.352784692559961 \n",
      "Epoch: 949   Training Loss: 4.917036545639599 \n",
      "Epoch: 949   Validation Loss: 11.414989348426978 \n",
      "Epoch: 950   Training Loss: 4.484607281385019 \n",
      "Epoch: 950   Validation Loss: 11.273111570672029 \n",
      "Epoch: 951   Training Loss: 4.455954083712875 \n",
      "Epoch: 951   Validation Loss: 11.3093341026807 \n",
      "Epoch: 952   Training Loss: 7.608833608557503 \n",
      "Epoch: 952   Validation Loss: 8.28567190156079 \n",
      "Epoch: 953   Training Loss: 7.193918357086239 \n",
      "Epoch: 953   Validation Loss: 10.796449430983824 \n",
      "Epoch: 954   Training Loss: 6.046752243204586 \n",
      "Epoch: 954   Validation Loss: 11.576173755279225 \n",
      "Epoch: 955   Training Loss: 5.405814565598719 \n",
      "Epoch: 955   Validation Loss: 11.910871657841943 \n",
      "Epoch: 956   Training Loss: 4.919196248417591 \n",
      "Epoch: 956   Validation Loss: 11.80605959362703 \n",
      "Epoch: 957   Training Loss: 4.692645045885357 \n",
      "Epoch: 957   Validation Loss: 11.09861394596118 \n",
      "Epoch: 958   Training Loss: 4.388810368285154 \n",
      "Epoch: 958   Validation Loss: 11.144687135789233 \n",
      "Epoch: 959   Training Loss: 4.051007422820484 \n",
      "Epoch: 959   Validation Loss: 11.052211416149511 \n",
      "Epoch: 960   Training Loss: 3.8702703520171537 \n",
      "Epoch: 960   Validation Loss: 12.036543801718443 \n",
      "Epoch: 961   Training Loss: 3.590862573818045 \n",
      "Epoch: 961   Validation Loss: 11.417009602101105 \n",
      "Epoch: 962   Training Loss: 3.6998935185734685 \n",
      "Epoch: 962   Validation Loss: 12.899832899622206 \n",
      "Epoch: 963   Training Loss: 3.804487443753557 \n",
      "Epoch: 963   Validation Loss: 11.721917955906958 \n",
      "Epoch: 964   Training Loss: 3.686274185831927 \n",
      "Epoch: 964   Validation Loss: 11.915571271549496 \n",
      "Epoch: 965   Training Loss: 3.4475049750088753 \n",
      "Epoch: 965   Validation Loss: 12.881208530180157 \n",
      "Epoch: 966   Training Loss: 3.331211152323198 \n",
      "Epoch: 966   Validation Loss: 13.13139495652169 \n",
      "Epoch: 967   Training Loss: 3.0921728398802872 \n",
      "Epoch: 967   Validation Loss: 11.871888684035383 \n",
      "Epoch: 968   Training Loss: 2.8983937991593405 \n",
      "Epoch: 968   Validation Loss: 12.470731254019775 \n",
      "Epoch: 969   Training Loss: 3.0646664890994564 \n",
      "Epoch: 969   Validation Loss: 13.768932511582971 \n",
      "Epoch: 970   Training Loss: 2.7801552129384395 \n",
      "Epoch: 970   Validation Loss: 12.696354064142538 \n",
      "Epoch: 971   Training Loss: 2.8938682154274686 \n",
      "Epoch: 971   Validation Loss: 13.020257861545414 \n",
      "Epoch: 972   Training Loss: 2.9195182815545317 \n",
      "Epoch: 972   Validation Loss: 12.281449742036477 \n",
      "Epoch: 973   Training Loss: 2.6379257516708456 \n",
      "Epoch: 973   Validation Loss: 12.119553200061329 \n",
      "Epoch: 974   Training Loss: 2.65243202562741 \n",
      "Epoch: 974   Validation Loss: 11.7463597489063 \n",
      "Epoch: 975   Training Loss: 2.479322893871835 \n",
      "Epoch: 975   Validation Loss: 11.979621102206357 \n",
      "Epoch: 976   Training Loss: 2.7842698971107973 \n",
      "Epoch: 976   Validation Loss: 12.720768205711307 \n",
      "Epoch: 977   Training Loss: 2.7684213744671182 \n",
      "Epoch: 977   Validation Loss: 12.460322922702051 \n",
      "Epoch: 978   Training Loss: 2.7096031768773616 \n",
      "Epoch: 978   Validation Loss: 12.048554621081955 \n",
      "Epoch: 979   Training Loss: 2.7375529753950345 \n",
      "Epoch: 979   Validation Loss: 11.089550802288578 \n",
      "Epoch: 980   Training Loss: 2.636421086635261 \n",
      "Epoch: 980   Validation Loss: 11.392175841031388 \n",
      "Epoch: 981   Training Loss: 2.574480025045167 \n",
      "Epoch: 981   Validation Loss: 11.564526137150306 \n",
      "Epoch: 982   Training Loss: 2.53467378723579 \n",
      "Epoch: 982   Validation Loss: 11.8621204933788 \n",
      "Epoch: 983   Training Loss: 2.4414159213464397 \n",
      "Epoch: 983   Validation Loss: 12.256897801636821 \n",
      "Epoch: 984   Training Loss: 2.253391997607153 \n",
      "Epoch: 984   Validation Loss: 11.364278257195839 \n",
      "Epoch: 985   Training Loss: 2.2390166719474673 \n",
      "Epoch: 985   Validation Loss: 11.108587102655115 \n",
      "Epoch: 986   Training Loss: 2.183325456074494 \n",
      "Epoch: 986   Validation Loss: 11.292895414564862 \n",
      "Epoch: 987   Training Loss: 2.1472460864722476 \n",
      "Epoch: 987   Validation Loss: 11.752379175014276 \n",
      "Epoch: 988   Training Loss: 2.099278561977036 \n",
      "Epoch: 988   Validation Loss: 12.033261313383118 \n",
      "Epoch: 989   Training Loss: 2.050244572596065 \n",
      "Epoch: 989   Validation Loss: 11.646969463623698 \n",
      "Epoch: 990   Training Loss: 2.0991209306960568 \n",
      "Epoch: 990   Validation Loss: 12.092476713072585 \n",
      "Epoch: 991   Training Loss: 2.0515828713592943 \n",
      "Epoch: 991   Validation Loss: 11.193668348909252 \n",
      "Epoch: 992   Training Loss: 2.0505749900096997 \n",
      "Epoch: 992   Validation Loss: 10.83362251761654 \n",
      "Epoch: 993   Training Loss: 1.8880828993326173 \n",
      "Epoch: 993   Validation Loss: 11.714476188290346 \n",
      "Epoch: 994   Training Loss: 1.9540767192494686 \n",
      "Epoch: 994   Validation Loss: 11.546113947284821 \n",
      "Epoch: 995   Training Loss: 2.0939228313306497 \n",
      "Epoch: 995   Validation Loss: 12.464577974058274 \n",
      "Epoch: 996   Training Loss: 2.053275031083752 \n",
      "Epoch: 996   Validation Loss: 12.392796399492719 \n",
      "Epoch: 997   Training Loss: 2.016668153375524 \n",
      "Epoch: 997   Validation Loss: 10.778004811376126 \n",
      "Epoch: 998   Training Loss: 1.8569326970290951 \n",
      "Epoch: 998   Validation Loss: 11.482467664403634 \n",
      "Epoch: 999   Training Loss: 1.787176671360616 \n",
      "Epoch: 999   Validation Loss: 11.917424218183797 \n"
     ]
    }
   ],
   "source": [
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared Cache\")\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# Create writer and profiler to analyze loss over each epoch\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "# Set device to CUDA if available, initialize model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "net = generate_model(18)\n",
    "net.to(device)\n",
    "\n",
    "# Set up optimizer and loss function, set number of epochs\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-2, momentum=0.9, weight_decay = 0)\n",
    "criterion = nn.MSELoss(reduction = 'mean')\n",
    "criterion.to(device)\n",
    "num_epochs = 1000\n",
    "\n",
    "# Iniitializing variables to show statistics\n",
    "iteration = 0\n",
    "test_iteration = 0\n",
    "loss_list = []\n",
    "test_loss_list = []\n",
    "epoch_loss_averages = []\n",
    "test_epoch_loss_averages = []\n",
    "\n",
    "# Iterates over dataset multiple times\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    test_epoch_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(trainset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(data[0]).to(device).float()), torch.from_numpy(data[1]).to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs).to(device)\n",
    "        loss = criterion(outputs, truths)\n",
    "        # writer.add_scalar(\"Loss / Train\", loss, epoch) # adds training loss scalar\n",
    "        loss_list.append(loss.cpu().detach().numpy())\n",
    "        epoch_loss += loss.cpu().detach().numpy()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration += 1\n",
    "        if iteration % trainset.shape[0] == 0:\n",
    "            epoch_loss_averages.append(epoch_loss / trainset.shape[0])\n",
    "            print('Epoch: {}   Training Loss: {} '.format(epoch, epoch_loss / trainset.shape[0]))\n",
    "            \n",
    "    for i, test_data in enumerate(testset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(test_data[0]).to(device).float()), torch.from_numpy(test_data[1]).to(device).float()\n",
    "        outputs = net(inputs).to(device)\n",
    "        test_loss = criterion(outputs, truths)\n",
    "        \n",
    "        # writer.add_scalar(\"Loss / Test\", test_loss, epoch) # adds testing loss scalar\n",
    "        test_loss_list.append(test_loss.cpu().detach().numpy())\n",
    "        test_epoch_loss += test_loss.cpu().detach().numpy()\n",
    "        \n",
    "        test_iteration +=1\n",
    "        if test_iteration % testset.shape[0] == 0:\n",
    "            test_epoch_loss_averages.append(test_epoch_loss / testset.shape[0])\n",
    "            print('Epoch: {}   Validation Loss: {} '.format(epoch, test_epoch_loss / testset.shape[0]))\n",
    "            \n",
    "# writer.flush()\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e3eda99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAABKSElEQVR4nO2dd5wdVfn/3+f2rdmSTe+dQCCEBBICkSpVUIqgSFEgKqhgQUH84Zev5YtgAURFELBQFWlShNCCCCQkIZUA6b1skt1s39vO748zc+/cuneT7L2b3ef9eu1r5p45M3Pm7u7nPPOc5zxHaa0RBEEQeg+uQjdAEARByC8i/IIgCL0MEX5BEIRehgi/IAhCL0OEXxAEoZfhKXQDcqFv3756xIgRhW6GIAjCQcXChQt3aa1rkssPCuEfMWIECxYsKHQzBEEQDiqUUhvSlYurRxAEoZfRZcKvlHpQKbVTKbU8zbHvKaW0UqpvV91fEARBSE9XWvx/Bk5PLlRKDQVOBTZ24b0FQRCEDHSZ8Gut3wL2pDn0G+D7gOSKEARBKAB59fErpc4Btmitl+RQd7ZSaoFSakFtbW0eWicIgtA7yJvwK6WKgZuBW3Kpr7W+T2s9VWs9taYmJRpJEARB2EfyafGPBkYCS5RS64EhwCKl1IA8tkEQBKHXkzfh11ov01r301qP0FqPADYDU7TW2/PVBkEQhG5BsBmWPA4FSovfleGcjwHvAuOVUpuVUld21b0EQRAOKl76Pjz9Vdg0ryC377KZu1rrL3RwfERX3VsQBKFb07DNbNubCnJ7mbkrCIJQMHqYq0cQBEHIgFJm29N8/IIgCEImLOEXi18QBKGXIRa/IAhCL8F29RQIEX5BEIS8I64eQRCE3oUM7gqCIPQ2xOIXBEHoPWz9AD55yezraEGaIMIvCIKQTxb+Ob4fai1IE0T4BUEQ8klbQ3z/qasL0gQRfkEQhHwRjUDjtkK3QoRfEAQhb7zwHdj4bqFbIcIvCIKQNxb9tdAtAET4BUEQ8kiaGbuN+V+LSoRfEAQhX6RL1fCr8bDimbw2Q4RfEAThQNC2F8LBDiplyNGz9s0D3ZqsiPALgiAcCG4bBo9nXXgwM6GWA9uWDhDhFwRBOFCsfjX78UxZOYPNB74tWRDhFwRByBuZhD9p7d2PXoBbqxInex1ARPgFQRD2l2gkt3qZLP7ksYH594GOwLq5+9euDIjwC4Ig7C+RUI4VMwh/NOn8imFm27xrn5uUDRF+QRCEXNj4Hsy9I/2xSEfRPB0QCcGCB+NCb2ft7KKVukT4BUEQcuHB0+CNn6Y/lqvFn0nIty2B578Nz1xjPnfxAi1dJvxKqQeVUjuVUssdZXcopT5SSi1VSj2tlKroqvsLgiDkjZwt/kwWvCX0bfVma48Z5Dp20Em60uL/M3B6Utkc4DCt9eHAJ8BNXXh/QRCE/JCr8HfkuvEE4PFL4MNnzOcuWqjF0yVXBbTWbymlRiSVveL4+B5wQVfdXxCEPBFqg92rYcBhhW5J4chF+NubUsM2k0mO4jkILf6O+ArwUqaDSqnZSqkFSqkFtbW1eWyWIAid4rlvwL0zoXl3oVtSOHIR/t2rOn9d3YOEXyl1MxAGHslUR2t9n9Z6qtZ6ak1NTf4aJwhC59hg5ZcP5Xf2acFwLp1o05HwL3kCXvhu5+/18g9h9WudP68D8i78SqnLgbOBS7Tu4qFrQRDygPVvrHpJkOC/rkst6yiq5+nZsGVhYtmwY3O7n9uXW71OkNfflFLqdOAHwDla6/xmJRIEoWuI2W9dE3NeMCJheOuXEMxBqjobx18xDL70ZG51vcWdu3YOdGU452PAu8B4pdRmpdSVwD1AGTBHKbVYKXVvV91fEIQ80cWTjQrGksfg9Z/A3F90XLezwj9oCvhKcqvrO/DC35VRPenykz7QVfcTBKFQ9FCPrZ0qOZfMmTFXT46d3xlWZ/LNRfDfO7MvyXgwWfyCIPQSbFdPTxuys58nlzcZ2+LPdZyjbIDZVo+GPkOz1831zaATiPALgnBg6KLQw8JhCX+4reOq4Xazdbk7fxtXB44XsfgFQeh+2BZ/18wyLTjZ3DA2tqtnXyKb3N7sx71Fnb9mB3SZj18QhF6C7qHC3xnXVSZXT+3HsOwf2c91dSD8XTBoLsIvCMJ+0kN9/J0ZtLaFP7nze+DT8cRrNlOvTPxcgGgocfUIgrB/9AaLv6NO7cXvmW24DRq2xcuTRR9g6lcSP+d5vV0Q4RcEYb/ppsLftBN2r9n3853P05lkaa174vtFVanHk336odbOtesAIMIvCML+0V0t/kcuhN9OMRb1qjkQ7Wz7HFZ+NNyJ06zzVjyT2AnYJEfxhPKfxECEXxCE/aSbCv+2xWb7wnfhkQvg/T/lfu7GeTDnlvjn5DVxs2F/D/+4PP3xFIs/Sfh9pbnfax8R4RcEYf+wDePuJvw2Sx4z2/oNuZ/z7DWJnztj8Xd2JrOdrO1TN8LwmVA2sHPn7wMS1SMIwn5iCV0XLRpywMg1eqZuQ6o7JtIZV0+GDnDGN2DvZigblFh++IUw+kQo6QvcBPcen/u99hERfkEQ9o/u6uNPJtvkqsbtsOAhGDAJnrgk9XhHFr/LCzXjYcfyzBFAQ6bBaT9Lf6ykb3zf489+rwOAuHoEQdhPChDHv/Tv8NGLmY+ne/v4712w7q309Z/+Ksy9DRY+lOF6ST7+dscSilqb43be/EzfQ6559T2B3OrtByL8giDsH4Ww+J+6Gh5PlwDYItPCKH87L325nXM/3TN4imDF04llz3w9vm+/DcQs9f0U/v6H5lZvPxDhFwRhPymgqycaNdb/sqRFTTLlx88kvrb/P507KNyaGOEDsOaN1HvFLP4M30OuCdxO/Qlc8s/c6u4j4uMXBGH/KKSPPxo21j/ApAvi5R0thZiCStp2gNMPb9/LLsvk6slV+D0+GHsKXLe0SxK0gVj8giDsNwUW/nRksvhDzbD4sdTybBZ/OpyCbAt/RxZ/ZzN3Vg6H0n6dOydHRPgFQdg/CmnxZ1oDINuEq7Vvpim0hT9Hi98Z7hlNsvgzxvF3n6UpRfgFQdhP8ij80QjcPtrxOYPF35ImVYJNoE9qWWczZDpdSTEfv+3qiaZ39+xLrv4uovu0RBCEg5OYxZ+HCVyhFmjZFf+cadLYA6dmvkbaDqqTwu9clcue3OVxhHNuWWj2z7nHcYvuI7fdpyWCIByc2K6OfFj84STffWd9/ADv359aZlv8HT3D91bDuDOShD/J4kebCWEAAw933KP7yG33aYkgCAc3+ZjAFU5KYdypHDoOMqVC7ijtRGkN9J+YXvg9jsFd+7jHMQgswi8IQo8jHxZ/KGnh8xe+u4/XSRJ+2+LftiSxvF+ayVSegOlwbBeP3fm4HeGc9vW9jlm4BVhpKxMi/IIg7DsJq1Tlw9WTJPyf/Du1TuOO+P7pt5lt1ajEOikWvyXKzvEDgEM/m3p9O3rHbkvM4ncM7vZWi18p9aBSaqdSarmjrEopNUcptcraVnbV/QVByAPOXPKFEP6EY5YA/2pcvGziufDDrXDNe9mvk8kaT1du59IJt5ttchw/Tou/lwk/8Gfg9KSyG4HXtNZjgdesz4IgHKzY4gcHTvhb9kD9pgz3yyL8r92a+Pnq16F8EPhKUjNepqx6lckNk6bcFnA7iindzF27nb1N+LXWbwHJwbTnAn+x9v8CfLar7i8IQh5wxrMfKOG/ezLceVj6Y8k+fie7ViV+Lu6b+Pnzf4VDzkl/nXQpHq5fnt7ijwm/9bzR5Jm7lsXv8iamaejFPv7+WuttANY243xkpdRspdQCpdSC2travDVQEIROEO0C4W/bm/lYclSPk+RcOCVJwj/xXDjmq2Y/wUWlYcPbqderGJreSrfvYz9vJh9/cp6d3mDx7y9a6/u01lO11lNramoK3RxBENLhjJffnxW4Nr2ffbatzRNfynzs4xfjfv6iKuPiScYWY6fLKGu7c7D4Yz5+Rxx/qCU1r343Ev58Z+fcoZQaqLXeppQaCOzM8/0FQTiQOJck3Nc4/mgUHjgFiqvhpB/Fy/96LtRMgDN+AcFm2La042vN+4PZ2pZ9Mu6kiBzIntcnnVhnEn6nxd/eBP7S9Od1A/LdkucAe+n5y4Fn83x/QRAOJE6Lf19dPbbbpWU3PP/tePnaN2HevWb/qdnwUHKsSBpa683WW5z+uO2mcVr5j38x8/WcfvkTb7bKkoU/2dWjIdgEvl4o/Eqpx4B3gfFKqc1KqSuB24BTlVKrgFOtz4IgHKw4reV9nUUbbOq4zsZ3c7uWnSrBX5b+uEryzwOseT213uy59gnxssPOt4o6Gty1Lf6kNnSjwd0uc/VorTOti3ZyV91TEIQ8ky5LZWcJNndcJ1druXal2Y4+Kft10r2dnPq/8ZW27IFh533tVMx2WTSDqwcNwUYoHZD+3t2A7tMSQRAOPg6I8Odg8Tst75IswR62C6dsYPrjrizC70zXbIu000pPFn77Gv/5tdk6ffzB5m7t45elFwVB2HfWO8Ig91X42zsQ/kgoUYC/9E/j/4+EzCLoO5ZD9RjYvRraG8wArifT2rq2tZ4mkse5uEpM+LNY/DpqIpGad8KEs8FrRRFpbZ4pJaqoF7h6BEE4CIlGYekTMOlCcOcgD2/+PL7vnMWbCa1Tfd0duXqCTYkRQ1WjYeARZt9e9HzINCP8bXsz+/chvY/fxin86dbgTSf8rXVm/5BzElM7B5vAl+zj7z4Wf/dpiSAIhWfhQ/DM18y2I5LDN3Ox+H85Dv44K7GsI1ePbVXbOCN23F5ra1n4bXtTXSxOktMtdFTP2Um50wl/vdkvqkgsD0o4pyAIBwv1G8y2vbHjuslCn4vF37wzNfVxR8Jfty6+7y2J++kB+ltpk6vHmK2OZrf4k2fdOnF2ZLn6+Nssiz9QQeztwH6e3hjOKQjCQYjtdkkWrXQkC326fDdOMk3w6sjV07DNbM9/AH64JfHYCTfBeffDIWfHy/zlma+Vzcfv7AxULq6eSJLFb9Vta7DaIcJfEH7/5mq+eP97HVcUBMFgi3A2d4lNssUf6cDin39fhnt2YPE3WsLvL08dH/CXwuGfT/TPZ+u0Yj7+DmYZ2/fp0OKvN/uBPvG69tuS+PgLw56mIB9srC90MwTh4MEWrUwzX5040x54Szp29bz0/fTlWxaZ7Rf/nv54k+Xfzzpo65CyrPXsAdh0Pn5nZ2ALv+O69r7L0XnYFn+gIn485upJiurpRhO4erTw9yny0hqKEAznYYEIQejuLP077FmXvU4uk6lsnEJfXNXx4O7Q6WZbM8G4Q975rYki+vhFUz7utHjdS5+BLz1l9m2rOuugrSMz5z77+J2uHiONH25zjHUkvwXYFr8nYC2xKK6ebkGfYjPiv7e1A9+jIPR0Qq3w1NXwl89kr2cLf0dRL5Ao9N4igu1tbNqTvMCJAzsCJxKEF78Hr/wI1r+Vvm7ZACgfDEC43vLrByoyX9uZkjmXqJ60Pv7Uwd21u1PTQIeiRuDX1jYYi99ul33tdkv4u/Hgbo+O4+9TZP7Q/rOqloF9TDrWWKdt1VFWgfMtLH4ssSTTuenOUaReN/U+Sdd1nBuKRGkLRSjxm1+R1+3C73HhdbvwuhU+a9/nduFymRP3toRoaAtR5HNT6vfg97gS2mgTjWpqm9ppbg9TXeqPfU82dc1BKkt8aK3Tnp9MrvVsWoMRNuxppqrER7+yQNo6/16+DY/LxSkT+3d4veb2MEs21XPsmL4d1l24oY6JA8sp8hmhCIaj3PbSR8yeNYoBfdK3pbO8u2Y3hw0upyzg7bhyFuw3VZ+nY8G4/d8fUexz842Txqav0LDVbJtr2Vrfyvx1e9jZ2MbsWaOTbmoJfwcplrfUtzIw1Ba3HN1+3l+znUtuf4P1t52V/iR78DcSgvqNZt/lMRbz0bMT67o8MaF0bXoXSvtDnyGZG5Rg8Wcb3M1i8TtdPdbfc8DjTqm1vSHIUODuVz/hzuF7zcCu45yY8PfGXD3dgQHl5h/5O39f0kHNgxuPS+F1u2gNJf6zul0Kv8eFx6XoVx6gLOChLOBlw+5mNuw2lplLwfgB5YzpV0qR18Xa2mYWbKjj8CF9WL2zCb/HRU2Zn1BEEwxHKS/y0tBqOpczDhvAnA938NH2Rk4YX8MFRw1h+qhq+pb60zUzxm9e/YT73loLwN+uPJrjx9awua6FYp+HqhIfzy/dyjce/QCAo0dWcf6UwZw6cQBVJelnYz46byM/e3Els8bVcMcFh9O/PL2A72kOcv4f3gHg8dnTmT6qmpXbGnjwv+t4bslWFvzoFADunbuGV1Zs58mvHRvrVN9ds5v/9+xyvnPqOM6clCEdANDQFuILVkDBuzedFDM47nz1E8b2K+Osw+Pnvrd2Nw2tIT596IC015r9twUEw1EeueqYDjvW37+5BoCLpg2jpizN9793s9mG2/jmL/7AQj0egPKAl4uPHhavZ/unswj/nuYgM297nVuOaOQrdqHbS6jd+Pwb20LpOz07mVkkGEu6tqouythwe+rSiMoVs+JdaKgYnl04ncdyGAvYuLuRYcnHRp+cUs/vSxX+gM/I5p7GNuPqSbL4Qy0NeEEs/kJx9Mgqnr12Ji3BCNruzR2dur2rY4d00mdHXaswVpbhnHTnJ5+bGFCQ+Z5+jytm9QUjUYLhqCXAEbONlZmfqIaKIi/lRV6ag2Ga28O0h6IEI1FqG9tpag+ztzWEx6U4c9IATp3Yn7W1zSzbspclm+ppDUUoC1h/1M1BAl43hw4qx+NSeNwutu1tpT0U5ZCBZTS0hvnt66tjbX171S7e/NislNa/3M/U4VUcOayC86cMoaLYy4qtDfzPcyu47NgRvLd2d+y8Sx+YzzlHDOK5JVspD3h4fPYM7v9P3A+9q6mdH/xzGTc/vZyffPYwvnD0MLTWzP2kljte/piLpg2loc0IyrtrdnHMz1/j8CF9GF5dwvIte7n/sqmM6Wf+Af+1ZGvsul/920JevO54gpFo7D7/WVXL8WNr+GBjHYs21vPfNbs4fqzJCzP3k1pW72zimkcW8ftLpmQU/+b2eIbKi+97j6e+fixRDXe+apYFHFw5k8lDKwC45E/ziEQ1z1wbL3OybpfpoF9ctj2hw4ix8yOTrmDSBbGin77wIXddfGRqXYfv/p/+Wzm//ccs1OP40TPL+cwRg8ybZagtLvzPfA0mp8+zaBsYc5Zt5Ct2X+zxU+Qylu7yLQ3MGF1NKBJl6k9f5ZazJ3L+UUMSLX6Lm/7+Pk/6tWMREwuXJ9F9Y1vVmXDWzRbVY9V7Yv5Grjk9bJ67cgQMOdqsuGVjC783VSK19aYeioSNq6d8kH0SABu37WC0izSDuyL8eUEpxRFp/qGEA8OOhjZqG9uZMKCMcNSI8XtrdzN/3R5e/2gnLyzbxk9fWMnQqiI27TG+0gUb6hKuUVXi4zlLkBvawpx5939ixx64fConTejHks17+cVLH3HTU8vY2dDO6H4lsTeCW55dwYVHDaFPkZdnr53JvXPX8Pj7m1i62Szfd/0TH/DctcexpyXIyytMyt6/XXk0lz84n4feXsescfGEX4/P38TxY2uobTSDln97d0NM+DfsbmZgnwClfg/XPrqIWWNr+MLRQzlpQn8iUc3K7Q1MGVZJe8h0JJfNGM4j8zZyzSOLuObEMbF7XPHQfP759WMZWV1CJGq6+W899gEvfOu4FCu5rtn40H/+4kpOPqQfAW+S9fn7Y8x20JGMqC5m/e4WXlq2nR9/Jpj6dpS02Mg//bfyjeA3eT46gz+/s55rp1fDL0ak/I7P/8M7zBpbw3WnxF1IYauzLMfy58+eC3Nu4Ri1grFqM7e9tJKnr5lJQ2uIva0hbnxqKeeXfQjbrYVUQnG/eYmyBoiTc+skC78zgVo6ch3ctcTXTZRV/32KyYv+nwlDdSVLoRFxvyeN8MfeRKzB3X4TE65dpqznE4tf6In0Lw/E3CoeN5x26ABOs9wWoUiU+ev28ODb61hTa6zIW885lH8u2szSzXuZPqqK319yFFUlPtbUNtHYFqbY5+a7f1/CKYf056ufGhUTuslDK/jLV47my3+ez29e/SSlHf9YuJmRfUsY0beE284/nGtPHMPj72/kjY9qWb6lgRueXMo/FxlXx/Fj+3L82BrOmDSQJxdtZvKwCgCmDq/k5RXb+Wh7A7uajOC+unIHW+tbGVRRxO7mIMOqinngimnc8I8lvLR8O3M/qeWbJ41hd3OQR+dtZO4NJ9AWNtbwjFHVTBxYzo1PLaM19DEAf7z0KL739yWc/Ku5PHTFNADOmjSQl5Zv49zf/ZcHLp/GyL7GSoxENQ1tYaaPquK9tXt44O11XGt1IKFIlE92NHKo/QX8dgqjAr+ieOB4PtzWwFOLNnPV8aMSv6Q0k6uOLG9gb01fHp23kWvG7ElNIRZqY+GGOhZuqOPUif1pCYY5op+Lvs9ewXGu6fRTVidePijmqpnj/z4jNj/KJzsbKXFH8ROkPeKDxY/Er+tYN7cIW/iT3HNJwt/uKSerA9HVOeF3qSjVC++GZit/v8uVtl5KOaCtkQ0XGt1aj0ry8ZfRgvaWoDJdsxvQfVoi9Ci8bhczx/TlgSum8dp3T+Cpa47l0unDefiqY7jwqCH8/HOTYlbp6JpSJg+tYFz/Mv71zeO47pSxKdatz+PiT5dNY/qoKsCMXzgpD8RtmKFVxdxw2gRe+NZxnDt5UEz0gZj/+4IpQ6hvCfHCUjM56MYzJlDsc3P6nf9h454WTj90ABr4+4JNgBmQLrYGze+8eHLseh9vb2TDbuNGWburOWbx+70uLj56GJ87cnDs7ePIYRX87LxJAPz2deP6OfmQftx23uGsrW3mxF++GWtP08al3OO9i9MPqWby0ArmfLgjds87Xv6Ye++5PeH5B0W2ccjAcqYMq+DR+Rtj7sUYaRZJaVcBzjliEFvqW9mwNrVDbdu5moAlzL99fRUX3PsuDz/2MCUb3+Ay9ysMUHuIKg8U902xmFftaKLmsdP4OHCFKfjwmZTrA5RgzQVwZ7f4d4aTFi5PphNx/FEULqKsiTrGVppqU+pB+nle2jpWTguqvcGRJtqUF6kg0XTr/YrwC70Jt0sxZVglLpeiPODljguPYFRNDjNDkyjyuXns6ums/N/TWfPzM1l/21k8erVxdxwzqjqlvlKKX5x/OKc6IoN8bvMnf9zYvgyvLual5cbiG1RRxPdOGx+rN3lYBdNHVvPs4q0Ew1FaQ5FYJJDf4+btH5wIwCsf7oiJw9bdDbRZ/m87GuT/zpvEUcMr8XlcVBb7OOeIQVx9/EgWWRMLK4q9fH7aUG6/4HB8Hhfff3IJu5raKXrhGs52z2NQ+3o+Na6GpZvrYy6o+ev28FvfPYnfcTSE2wVfPGY4a2ubmbfOsXB5Uy08nboGbdBVxEkT+gGwYfPGlOOB+2cy12+WQpy69RGucr/AsjWmI+yjmumv6mn29TVWcSgx7HHjnhYCez4y3y27Uq5tUxRz9ST7+F3giru+dlKV8RpAgqsn6s3+t6Vx4ULT1OqYgPbpnyRdz/yd6DQDyrbFP9G13hQMODzhHICwR4RfEA4YSqmYAAMcO7ovr3x7Ft8+ZVza+gGvm/svm8p7N53MKYf0i/mqvW4Xt5w9MVavyOvmshkjYkLocSkumjaUdbuaGfejl1i9s4kix0DfkMpiDh9i/M7vrt3NBLWRS145ipINrwHG4rfv/5evHM1TXz8Wr9Xp/OD0CbHrlFt+/c9PHcqL3zqe5mCEF+d/FFuH1uv1cM7kQUQ1TPvZqyx/7RGGRzelPGe13oPbpThr0kC8bsUbHzuyWe78MO13E3F5qS71M7ZfKTt31qat01/VA3BlywP8yPsIv/aZNXCLaaM/e9hJpanoEP7KYi9b6+Ofn/bfkvba9nWADBZ//PveHKnIeA1TP/430aSyzzqO4sJNlJJIgxmo/eYiqBmfWMm2+C2JjOp4B2CXDbBdXfagsKOTCLrSvKF0o3BOEX7hoGdc/7KEziAdA/oE+NPl02LhlQBHDa+M7duupauOHwnAhAHlnDEpMcyyyJf47/LwVeZtQ2uY4jKuG/+al8zWEf9d6vdw2OD44KTH7eKVb8/i5An9GD8g7pYY06+U0TUlXPbWLHz1JtzV63Ez2vF2dNh/ruGu3V9NECKAb0X/hj/aRpHPzWGD+7BwfR11zUEWbazj/lcWpv1OvJi3k2NGVbG3fnfaOoZUf0cx7Qzz1rO6tcy4lULG3RXGzaCKIjY7JnLZnUc67MHdSAfCv76tgzdEh6g26uzhxFHlQqGpUM00DDoOqkdnrGs/edghlfZbQAVWBJQ98OxoQ2sHnU+hEeEXei0VxT6uPXE0Vxw7ItZxHDu6L0tu+TTHje2L3+PmNxcdQQmtXOh+k2JX4qSf8oCXSZagRy3/7tINtfgJEvA6/rU2vAv//mHCueP6l/HAFdNSInlOGF2R8Nlv3fPtH5yIn/hMWZfSPBlJzGtfpk16gWkjqli0sY4jfzKH837/Dms3bSYdPmX8/kePrCYQMSIWHnVKSr3zvKkLnRerdmqoY2u00gyGtxjX0l5XH0bXlNK2Y1Xae8aoMoPP45R5e6kjKWonyce/vin3OJT6cHbh1yhKPFBBI02uLOMBxEM3WwiklFUqK51DbEZxXPid9bsjIvxCr+aG0ybwP+ccmlBmp/oA+NyRQ1gRuJI7vPcxas1fUs6//NgRAHgxInq++z/82Xt7fHC6rQEeOh3e+11iZM2m903unCROHJr42a+MVT6k7n2WlF2fcGxedELC54A27pULjxpC1GGk17A3th86P77Ait3mGaOqqVBNrIv2p/lzqc94lW9OSlk1eymONrNTV7KlvhUazVhJnauK8QPKGNn8Qco5CVgW/jT3Klq0n3VFib8DlDvBgl7bmLtUNQazZ96M4KJPwE2FaqaejoTf3LeJ+JuitsYTKmgirDzgtY45fPi7qci5vYVAhF/o3ax6FVY+n/m4Yz3Yvt7UJGQXHFpGf/ZQrRpiZTPcH8ZmjbPm9XjlkCOPzQOnmNw5kcRom6lVbQmf/ZZVzprXCITqE44tiCb6pYuiRvjH9o+LWSktXOH5d+xzYyTuUvFZnUoN9Zzi/oBFehz1bYkzdjdGaxgd3ZD82LFzd+gKNte1mLz4QJsqYnz/Mq5yv5hyTgJW3p6+1LNLl7OlPimzZ1Io5JoGFZv30BENHeTm0ij6uxuoVE3sjmR3ydh3bNQOn73VIQ1Wu2h2OVJFOzqqHdEO5h0UGBF+ofey5Al45Hx44pLMKYUb4gt/fGpUmhwwfziOeYFvcL3nqYRijzWQy+pX44UhS9TthUUAdie6RAJ1iWGVtjsmXUSIKzZj1DqXeKfx8JVm/GG42kGVinde9eG48NsWPyufI0CQ+8Nnsbo2PigbchdRoZrx68TOyMlqPZgtda3wqRtY5j8SL2GOe/syxri2ZjzHND7+VrWLPuatIQtNEQ87GzO3w0lDW2roqpMILia3GPfVOp09F5Tt1ml2WPxRSzb9KswS7xHxyo7f0Tan8JdlTvFRKHISfqVUiVLmqZRS45RS5yil9i8DlSDsKx0topErz3wtvp9pqcG69bFdbzRJnKIR2OsIgxxxfOJxrdNb/L92uGhaE2cyJy9L6LPF2XrzeGRIPELmh+dOIajjfnC/o33Hje3LT0Z/zAv+mxOu1xqN+8p9hKF+E7z4PaJuP5/oIazZFU/t8E7xSZSrLNk2gc3+0THRDuFhSHgjga0mV9Fj4RMT6n4QHcOF7Vb7x58ZK29wVbC5LvG7tedGxFGmg8mAc95CRxZ/FFesM3s1MjVrXdvkb3JY/Nohm0ujzoHhuMW/Kjo4Xjz7Tbjihez3yTO5WvxvAQGl1GDgNeDLwJ/39aZKqW8rpVYopZYrpR5TSnXvkRCh+7DhXbi1Ap6aDZvmwycvwxOXwtq52c/TGt5/AGrNLFo2zU/M0JhO+LWG5f80+0WVEEwSwa2LEz9PPBdmfR9Q5ty3f2PeGMZYA6ahNMLV1pD4efP7RIYey13hzwFO4W+APkO56MvfjlWdPKySY9p/x2XBHwAQcHZMO1dy6ZZbU27X6ugoPERgmRlnCH3md/i9Xv61JP420q47tu36V5bHBDmMlxJtBPtt9zG0kRipszg6mvf1BPj+Ojjqilh5Y2BgLPxzTcWxADy3OPWNIdtbga37C6NjY7mbMhHRRvYiuNnamn3QONBmxi626HjmV+0Q+KVBhzXvsPgXhxyDNWUDYMRxWe+Tb3IVfqW1bgHOA36rtf4cMLGDc9JfyHQe3wKmaq0PA9zAxftyLaEbonWHKX33mfZG+LcROZY+AQ+cCo9+HlY+B3+/NPN5e7fAu/fAC9+Bl39o2jj3F+bYKbfGr712LvxynPlp3g3/vsnc59DPmbTAIYcVWr8JHr3Q7FcMN9s+Q8FXDGho3gXv3G3KDz3PbEOtcaGf/CXrvg7h37oYti3BPfFsguMt4Vdhk4xt6ROwd1PchQRUFvuoo5wPomZuQlloV7yz+v30tF9Fa8Rp8Qdh3n0w4nj8ky/klIn9WbYlPhDsFNpvBL/J+miiW0QrN0Mqi1hd22RSc9s5/CuG85eBP2JhNHFuxR/C55id4ipwx9vRVjIkdq/HRt7GpLY/pQ3PTX4rcBLVmsltf+SLwZtpaM3u6nFbYazt7mLqO6irrBnPy/WIWJlzUtfS4KD424YjCmlLW/e2ZXONkVJKqRnAJcCVnTw3032LlFIhoBjowCHYy4hG0+YIyZlIGJq2GwvEX24WpoiEjPD4ShOvrbUpb9gKG/4L9RuMW2HINGjcCmWDYMAk6DvW1G1vMGugVo81A3RbFhp3hrfIzGCc8/9g12oz0BUJwvSvm5WX3vs9nPlL6GetvrRpnqkzxhE+GG6HHSvM9UKtcNz1pr2LHzUul7duT37SOPb6po07jLtk+1KYf7/5HpysfhXuPc5ktTzhhzDQ8tGufQPe+L94Hpk7rFw3Q6bBZ+6Ge2fCyn/BzwfD0GNgjZmoxbSrYOM8s19SE88z/+8bTebGK+fExw/m3xcLY2TIVFj8MLTsNs/6+xlQZ2UlPeILfHtMHfwO+oT3xJOx2Xz3E4i043IpxvQrZfVOaNU+Zm24G269O6Fqre5DjYqL+a7AMO4Kf47rPE9z+l4rquiM2wCYMKCMfzk8TXY6BX3893h+zhSeD85gru96hrvM5LDQoGnMGtuXOR/u4M5XV3F1aIU58exfM2hFFX9ZPZ2G/ifx1/P6c/nDK9jZ5pjN6ojR3zHwRLYsaUVrTXPERSPF+K2oqKtD32Xm1KlULfdmFX4NsQidjiz+Skxn2+LrS/3eUMJ6EtGBU3BtW0RbKELA62bN8C/wt8X1PBmZxf/Z93LMEt4eLac9HDVRXG4vd/iuZWdziMZImEhUp6QW6S7kKt7XAzcBT2utVyilRgFv7MsNtdZblFK/BDYCrcArWutXkusppWYDswGGDUvJnJ0bb/zcWEooa8Q9zdbcLEMd9uNc5zapXjRihDgSNGlwgy2ANmXRELTtNWueDj3GTA4JtRqB0FEIlJt6/rJ49r9Qi6kTaoWGzXHxwWp/5fAEXzVDpplrtNaZ9UzDaf6h5v+x4+/XE0hcdxVMmF71mPhs0f/eBdxl9u+daTIZ2lkaASacDVUjYcUzsDdpRuquj833sPSJeNnM60wntewfiXUbt8FDZ5rOqyN2LIfBR8GMa+Kunzm3gKcIzr4Tnr/elI0/Ey562Fhy9ncabIqL/pm/hKOvhgdPN5/9pVBipY5Y/qRZXGTo0bDZmkC19HGzHX0SHH6Ruc+/bzQ/Nkd8AYqr8NjjAS+baBn6HwYnWnMByuKW98XThvLTF1ayXI9gmrIGhpUbvvE+4dJBPH/PLXy5Mb7IeUsoym/CF3KM6yOmu1aat5mJnzWPM2kgd7z8cazuAj2OzzMXNfDwWNmZwf+jmHbOcM/jhvNvYnSdcQfd9doqPuOvYIxqghHHM3z7VkDRqgLQ/1DqA/VAfew62lPEdl3FR6Mup3jAOFoXfEhdS4j2WMoLY5zMiRzFISWjGVJZm5OrBzr28ds8f8wjRP69noa2cGwxontH3sVf1y3mync3cPWsUUTcPv4ROSHxXlYk1ie+Q6BN0dgWjoXv/stzKhsj5nfX2Baiojj9GhKFJifh11rPBeYCWIO8u7TW39qXGyqlKoFzgZGYv4R/KKW+pLV+OOme9wH3AUydOnXfRvMqhps822jrLyN5S/qytHVzOTfNNaLRxOPmWzBpaP1lZrq3r9SUub3mx1tkhH7zQmOBe4rM67FyGWs5EjSCHWo11ru32PopMrnF+0+CUZ8yIrxnrRE3XxnsWGZuv/l945YYNgNK+5kJKOE2I+Sl/Uwe8b7jjPUdbDJiWlxtOp4dK4zron6DmbTjK4Fti2HsaVA+EA45J547ffca80awZy1sfA/Wvmk6m8oRxufZtBM+et48V+WI+O+t73hjGa/8l3mG6dfCkZeYt5eKoeYa/Q+D5lqTiz4SNrloNrxj3lAqhsFpPzdCXNrPXH/XKtMh3XeCufc5vzWdcM146+1mB3zmThhzsum0Bk4297T5ysvmu2hvNL+PfoeYDhXgvPtN59R3HFSONO1Fw0n/zxzvfygc9WXzO6xbb9xLvmKzffXHps6Uy+DMX8XTE5fUmO9h18dwxu1wTGquHSC2QtuVwRv432OifPb008zfgsePB/jyd25n4+7/4dJfPc5QVcsJQSOsN4e+wg2DlnH6ubfFjJNhVSa0sV17+Gvk0/w9ciIroiN4YcLZ/PDM9bywbDtLNpkIl79GTuNH5f3pH4mPe1zSfiNnjnTzY48/lmV0215jGPiTVhGL4GJG+z18d8g4xlWagdMtda2x7KYBrzvuQlGKwRVFfLIjwwA8xtVj05HFf773Hs4c1EZFmYm8qW8JxoS/PuxlO9VE7DU00gQSBMuH85PQl6gfcT40tNLUHo4l/3OGnDa0hg9u4VdKPQp8DYgAC4E+Sqlfa63v2Id7ngKs01rXWtd+CjgWeDjrWfvCkZck/vMKhnC7EUN3B4N3tqV39NX7dp/q0fHp8JGw8ZEn51Vv2mmEKt06qQ1bTacYSAqjLKo0biAn31qUvS1DrOiN6xYnlvvL4KpXE8vOTPNnPSy9zxwwndGs75l9jw9O/3nicW/AdCrJHHc9zLjWvAF6k3zCHj98Y74xGLLkeOlfbgSngRJ2VE0w340TpehT7GODHsAGPYBjLOFfowfzcr+jOd2RRdJ2S4xv/2usbIUeCS43s2eN5qjhlZz/h/gsXq9bxecrADuoYlPAvI3MGF1NvzI/37BSSSdnWw1bAul2G1EH2FLfQpuV3dTjUjEbyqVgcEURb3y8M6dlPvd24LffxEBW9+nHqSXm77+uJcRw60XNXvjI47Jz9aRBKR6InMmFJdXA5oTFd7TWVBZ7qWsJdeu1vnN1JE/UWjcAnwVeBIYBWUbTsrIRmK6UKlbmN3gysHIfryXsCx5/x6J/oHF70i+mUdov8+LY5YNSRb+n4famir6TDkTuxPH94pfK4E8uc6Ssdi7P6Upz7S9Nz+xWTV6bWSkVe+OIt8FsA143828+JbasY7LFb6985nO7HMLfRsgq39HQRihq9hWKwZVFtIWi7G5OnUQHSRZ/R+Gc2lzVtsbrHNcMR5PWOU6XltkqK7W+10bHvIGI1lRa1+3ozaOQ5Cr8Xitu/7PAs1rrEBk6w47QWs8DngQWAcusNtyX9SRBENKilGKU5VbJJPwuR3lLMC787jT//T/97CTW/PzM1AOQcfH4JT/+NJccYwQ+0+TaFOF3LCRfUeyl2OdmS11rTMD/518f8uNnzWCxbfEDCVk/nST4+DsUXI1SxAS6riUu/KGwuZDHCoDQWWTO/j6aHBZ/VEO51UE29gDh/yOwHigB3lJKDQcasp6RBa31j7XWE7TWh2mtL9VaZ5g2KQhCRwy2fOTZIkjsFNRtIafwp//3d7sUc284ASBhCUfnm4OTPkVexllpIpoyzJp1ZisFh/C7XSjLh7+lvoWoY2rFqyvN4jNKxZ8x0yQuW56LvG6a2sNEs6R30NqEZthvMM43BPstw+NWsbrpzof44j9N7SHHMR37npydbHcjJ+HXWt+ttR6stT5TGzYAJ3Z4oiAIXc6QSjMomy2XzZdnjsDtUonWaZb6w6tLWPLjT/PW9+P/5kXJa/46qLAS2zW2p7dyr09as9dp8ZtnKOLlFTsS/OLFPiOgSimGVJhnzBTZY78pVBR70Roa2zP7+TXGzWULtDPFQzhirmMv2JPuG7LfAuzznZ1dJKoptdxfzZbwP/PBFi6+LzXDaSHJNWVDH6XUr5VSC6yfX2Gsf0EQCswQyxrevjdzLhulFNUlPp5fGp+Z6/T3p6NPkTcmYvY1MhF3b6QX3KFVxdx4hklVEYrouI/fEv6Rfc04z4fb4o6EYmsSl1JQXuSh1O/JGMtvW+HprPhkotq4erxuF8U+d6LFH8nd4rddPY1Jrh77O2uxyq9/YjHvrd2TNkKoUOTq6nkQaAQ+b/00AA9lPUMQhLxgC3+2CU4AOxsTPaq21d0ZPvrJ6Zx9+EAunT48obyiA+EHYiuQBZ0Wv1V2zYmpi6HY4utSCqUUgyoCmWP5k4U/i3/ddvWAWVPBWTdkWfz2wHc6H79dEvC68LhUgsUfjerYgHeyq6d9H77vriLXCVyjtdbnOz7fqpRa3AXtEQShkxw/toYir5tLZwzvuLKDTD77bAS8bu754pSUcjtCJtuAps8S8r0toZgI2hZ/31I/J46v4Y2P40tA2gOttkgPrihizoc7WL+rmRF9Ex0OtqsnbvFncfU4QkLLizwJde2oHtttlt7it9ulKA14kgZ3NR6XosjrpiVoyj0uRTiqaQlGUsJaC0WuFn+rUiqWZUgpNRMz61YQhAJTVeJj5U9OZ3qaBeedLPzRKfzui1P4/ukmj3/fsuwrVXUGW3Btizkd9gDvrDve4N65awBH2CRxd5GN7Xaxre9BVmTPWXf/J+Xa9l1zsviJR8kmW/z2sIfdkaT38Vso49ZpSgrndLkUJX53zOK3x0bsjqA7kGuX/zXgr0opOxC7Dri8a5okCEJXUF3q56zDB9LUHmZLXStfPyHzWrOdpTyHt4cKx8pmcz40ETvOMM/ypHDRWBy/JdJ23HxzmmgZ5+AukHXylHH1mIv2KfKyI02e/9i8gDQmf2xCMUb4k338LqUo8sWFP+Bz09ge7lZRPrmmbFgCHKGUKrc+NyilrgeWZj1REIRuR6nfw88+N+mAXtPOGpotJ1m6NwyfO+76KC9KlCPb1WMztl/mZRKTB1ybs0X1WIO75p5eVu1sSqljh5Wmf3+xXD1WZJDT4tda41JQ4vPELHx7/eXuJPydSgGptW6wZvACfKcL2iMIwkHKg1dM5dXvfCrj8ZrSNMLvsPgrihLz2iS7es6fMpj+5X76lqbmv7EHYTMNrCbWjXdQ5QFPWrdQPFdPmvOTLH6njz8S1SkWf8zVk6Uzyjf7s/Ri98w3KghCQThpQn9G1WRIv4EZnP3KzJHc6ljc3jk3wJ6kZWMLfzzBreKiacPY3RwkHEmMkLHFOOB14XaprP70qGNwt7LEx97WEO3hxI4iGhvczRzVoxSUBrwpM3ddLmVZ/Oaa9izgUI5rBueD/RH+7vMUgiB0e1wuxS2fmcjUEfFEcs4FV4akCH9iaCWYpHRaw66mxJw9cStcUexz09yexeJ3hHOOqC5Ba9i0J3F1NVujnSIXTYr0UagEi98+7lLmuWx3kz1BOhLtPuGcWYVfKdWolGpI89MIDMp2riAIQjqcyd5K/E7hL06ol2zxQzx19KqdiSmabVdPsn89HVaONnO9anO9jUnCn87Vc+drq6wy28dPgo/fHhB2K0WJzx2bIOe2HiCcJeIp32QVfq11mda6PM1PmdZ6f1bgEgShl+IU/oAjh09lcWJUj5262TljePLQCgCWbt6bUNe20JUyM37TRf7E0PG3iKpYhs5EP79OE8753OItCWW2j781FCEcicY6C5dLUeTzxN467PZnS6mRb/bH1SMIgtBpnGkgnJlDlVIMry5Oqe8cTCwLeAl4XdS3JLt64pOqiv1uWrMIf1THl0u3wz/rk8I/I2l8/DH3j0P5Y3l52iOO9QMsi99667CT54VF+AVB6K1ky/kz94bU3I/J6waUBbwpqSF0gsXvyR7OSeIELqVI6UjSaXSsMyDeyZTGEr2FYq4el4Jiv4eWUIRoVMciiMTiFwRByJHkfqIs4Mki/MbazhrOqXVsApfLpago8qYMFicP5NrnJbfLXnJxZ2N7TNhdygwwaw1t4Uis4wpFus/grvjpBUHIOy9ddzy1jbktw5E8KSw5zQIkDu4W+zw0BxMHaxPrJl5zZN8S1tYmTuKKDe46vPw6ZQf6l5nV03Y2tDGmnwlldbkUAXd80pbt6hGLXxCEXs0hA8uZNa4mp7oqacpQdYmP9bubE9YTSB7czebj105fDzCqppT1u5sT6kTTRPXEXT12u+JrHu9oaEsK57RTM0fExy8IgtBZkl09s8bVsGlPK9sb4jl2nIO7Jf7MPv54vTjlAW9K3H80SeQhdXBXKUVlsQ+vW7GjsT0ezuky7iaAllBYonoEQRA6S/JgcP9y417Z41gk3Tmbttjy8aeddet4M7Ap8btpDoazR/A47hIb3FXGrdOvLMCOhraYe0hZKRvARPtY2ajF4hcEQciV5BggO96/viVxrVswolvscxOOxlf5cmJLrzNSqNjnMQOxoXj95AiehDJHrh4wA7y1je2xcrdSsZxBrcH44O5BM3NXEAQh3ySv7pW8JnyltQB8nSME0ynG9lq9rUEzsSrRkk919dizh1scVr9O4+NPTuNg9x12lJEznNPOQdQcDMfmKojFLwiCkIGffPYw5t5wQuxz8uBubNKVQ/htTXUpFRPyXU3tjLn5JX77+upYvXSunvhCKZGYeEfSuomSJ3WZi9gpIpzhnIkWv6kdOVhSNgiCIBSCakcK5+TBXTt9c11LiHdW7+KZD7Yk+N1ti39LvRn8ffi9DbFz4/XiF7VF+ukPtsRy9sR9/Gn8/kntMoPJjpm7LhVbKL45GI7H8Xcji1/i+AVB6HY40zokD+76PC7K/B72NAf54p/mAfDit44HjJvFb4luulW40ln8dv6fX8/5JFaWbgJXfFUu6xpWuVlm0WnxExP+1mC8Q+j1Pn6lVIVS6kml1EdKqZVKqRmFaIcgCN0XO/493apeFSXeBFfPpjp7wpaKWfx2qmWnne1MqWwzqKKIwwaXJ1w/nDacMzmqx1yj2ErI5gzntNvQ3B6J1RcfP9wF/FtrPQE4AlhZoHYIgtBNsdfxTfbxA1QW+6hzRPXYoZ3KkTjtjpc/TjnP6RJyMqQifUrotIO7SRZ/qd9NMBKlPWynkVa4XQq/x0VLMBw7r1f7+K11e2cBDwBorYNa6/p8t0MQhO6NnQAtrcVf7Euw+G96aplVVzFhYMdr8yZfs29Z4nKObVYufWc4Z8pCLNY1bOveXpDFzr9f4vdY8wlMvd5u8Y8CaoGHlFIfKKX+pJQqSa6klJqtlFqglFpQW1ub/1YKglBQSv0meiddMs/KYi91LaGUYwrwul0cP7Zv2mvGwzkTT6wqSVwP2LbenRZ/OKpNeGjsXrbAG39+o5U/yO5UiryJE8Psa3YHCiH8HmAK8Aet9ZFAM3BjciWt9X1a66la66k1Nbnl9BAEoedQZrt60ih/ZbGPjXtaUhZDt6tWl8Qt+IQMm0n1YvfyJ8a5xC3+RJodM4KTLX47Y6hS8Q6hNRiJXSN5Xd9CUgjh3wxs1lrPsz4/iekIBEEQYtgrdaVLZ1yRtFqXjR06ObKvc9F3R4bNDN6WGaOrEz7HrHPrBPsNoj0USekM7DEFW/jtQekin4dmx9yA9lAvtvi11tuBTUqp8VbRycCH+W6HIAjdm7FWmuO1tc0px646flT6kywr/LTD+qc/7pjo5eSwwX24aOrQ2Genxa8UnDt5sFUeTePjt1091uLqdny/tQqXXb+3W/wA3wQeUUotBSYDPy9QOwRB6KbMnjWKT0/sz0XThqYcK/V7mDGqOqXclvO+pf6UY+Dw8acZNzh8aJ/YvtPHr4CA10hlWziC3XvEffzG4l+x1awDbKdoKPa5E8I827qRxV+QCVxa68XA1ELcWxCEg4OKYh/3XZZZJnyeVLvVtuQri+M+fmc65MRkC4mU+OJyaIu0RqOUwm8tCm+/CUC88xhUUQTAe2t3J7Sh2GcWYrcRi18QBGE/iaZx2AesvDtuR7ymcxlGZxbPZGyXDRhfvqmfZPE7XD02VSU+Zoyqji3f6FZOiz+evK23R/UIgiDsN+6kYPxRNSVMGVaRUq89HE1ZPSvd3IASR2SP3VnYPn67Q2kPR9JGBg2sCMT2XY5oH2fKBufbQqER4RcE4aDE6c4BeP27J+Bxp5c0e3JVNF2yHgtnR2Kv4GUsfkUg5uqJpk37YC+6bi7tsPiDYvELgiAcMD5vReH842szWHLLp1OO33zmIbF4/i11raYwKd2Ck4F9ArFtkzXxSmN8PXFXTyRt2gfnPAC7Ayn2u4k6FngRi18QBGE/mTG6mvW3ncW0EVX0SRPXf/WsUfzlK0cDsMFaTD3dClw2w6tLWHHraVx+7Ai0ttw9MR9/fHA3OVcPJGYTjYdzJqZyEItfEAQhD1RZFr+dojlbOCcYP78t4s3t4ZiP3x8L54ym9fGXBuIdjys2gct0Fk1WfH+vnsAlCIKQL5yra0FqZs10xGbitht3j0LFB3dDzkXcleOceESQ/TZhzzyubzXRPm3h9AvAFwIRfkEQeiy21d2alHsnk8WfcI4VkaMUjsHd1Dh+iCeUg3g4pz34HJsToCHUTVIzi/ALgtBj8VuTvFbtaASyx/HbJIdumoyfCpdKjuqJM2lwfNavvV5vVUnquENbN5nEJcIvCEKPxRb4ZxZvNVE6Obh6bPdQazBqWfwKpYy7JzGqJ36VPsVejrUSvdnpmZNTPUP38fOL8AuC0Ctoag87Eqxls/iNLN7xyscmZUOs3E17OL3FD/G1e+0ZwFUlvlgnYtNd0jaI8AuC0CvY0xyMW+tZ6tlivWRTPZv2tMYqBzyuxHDOpIt859RxPHLVMRw1vCpW9uWZIxLqdJdEbSL8giD0CvY0B+NLL2ZRvoDDSlcq3kkEfG427GmJDRQnr+LlcbuYOSZx5a+qksTZxS3B8L41/gAjwi8IQo/mqWuOBaC+JZRx6UUnTuFvD0djbqEir5v56/bwo2eWm2tke22wKPIlunrs+QSFRoRfEIQeTXkgPoM2l3BO28cPJnzTrrsvom37+/fnGl2BCL8gCD0aO8beObibvX48/UJ7OBp7N9jZ2J5QLyeL32uuZYeV1reI8AuCIHQ5JX5n6gSj/Oly9dg4I37aQ5HY52BSrp1skUE2tsVvL+QiFr8gCEIesJOlNbaHiWaIyEnm0auPASxXT4Y6ORj8MeGPak2R1019SzCHs7qegiy9KAiCkC9cLkWp38OyzfU5iTXE3T1toWiskzhqeCULN9R16t724G57KEp1qU9cPYIgCPniyGEVvPFxLXe9tgrAxOdnwXbNmAlXRvn/8pWjmTaiMlYnFx+/veh7n2IvfYq81IurRxAEIT8cOqhPwufzpwzOWt9eyL2+NRTLr1/q93DzWRNjdbKFhNr0Lw9w23mTuOviyVQUe9krFr8gCEJ+qE6aSNWvPJChpsEeENYapgyLW/l2WgbIzeIHuPjoYRw7ui/lAS8NbSL8giAIecHrztW7b6h2JFgb2780bZ3OXRFKA57YalyFRoRfEIQez/HjajhscHnO9Z0LryenXbDdQJ1V/jK/CD9KKbdS6gOl1POFaoMgCL2D0TWlPP/N4/fpXHv1LpvYbNxOKn9pwENTW7hbrMJVSIv/OmBlAe8vCIKQkbduOJEjh1Vw7uRBCeXFVi6fYKRzmTZL/V7CUd0tFl0viPArpYYAZwF/KsT9BUHondx+weH87otTcqo7rLqYp6+ZyZDK4oTyMyYNBEjJtd8RpVbOoIZuENJZqAlcdwLfB8oyVVBKzQZmAwwbNiw/rRIEoUfz+alD9/saPzzzEGbPGpXi+++IGiumf2dje4dRRV1N3i1+pdTZwE6t9cJs9bTW92mtp2qtp9bU1OSpdYIgCNlxuxT990G4+5cb4d/R0Hagm9RpCuHqmQmco5RaDzwOnKSUergA7RAEQcgbtpW/pT77rOF8kHfh11rfpLUeorUeAVwMvK61/lK+2yEIgpBPKotNeuhbnl3Bqx/uKGhbJI5fEAQhDzgHg19ctq2ALSlwdk6t9ZvAm4VsgyAIQj5IyN/f2Wm/Bxix+AVBEPJMY1thZ/CK8AuCIOSJy2YMB6CuubALsshCLIIgCHnif889jN1NQVZubyhoO8TiFwRByCNVJT5qG9sLmrNHhF8QBCGPjOxbQmNbmN0FdPeI8AuCIOSRUTUlAKzf1VywNojwC4Ig5JGaMpO6YVdTe8HaIMIvCIKQR+wF2Hc1iatHEAShV1BZbLJ67hbhFwRB6B3YSzf+5tVPCrb4ugi/IAhCgVi+eW9B7ivCLwiCUCCe+mBLQe4rwi8IglAgnly4mfqW/Pv6RfgFQRDyzDPXzoztv7tmN9FofmfxivALgiDkmTH9SmP7X39kEaN++CKRPIq/CL8gCEKeKfV7uP2CwxPK5q/bk7f7i/ALgiAUgMlDKxI+r9yWv4ydIvyCIAgFYFTfkth+kdfNO2t25+3eIvyCIAgFwOOOy+9njhjIO2t20R6O5OXeIvyCIAgF4lPjagD49MQBtAQjLFhfl5f7ygpcgiAIBeKPlx7F3tYQAY8bMH7+mWP6dvl9RfgFQRAKRMDrJuB1o7XG73GxszE/qZrF1SMIglBglFL0K/eztjY/i7OI8AuCIHQDjhhSwRsf72T73rYuv1fehV8pNVQp9YZSaqVSaoVS6rp8t0EQBKG7ccNp44lENU+8v6nL71UIH38Y+K7WepFSqgxYqJSao7X+sABtEQRB6BYMry5h+qgqfvPqJ8xbt5twVPPgFdMo9R94mc67xa+13qa1XmTtNwIrgcH5bocgCEJ347OTjRS+s2Y389ft4ZH3NnTJfQrq41dKjQCOBOalOTZbKbVAKbWgtrY2720TBEHINxdNG8qXZ46IfX579a4uuU/BhF8pVQr8E7hea52SpEJrfZ/WeqrWempNTU3+GygIgpBnlFJcPmNE7PO2LhroLUgcv1LKixH9R7TWTxWiDYIgCN2RQRVFVBR7CYWjbK1vJRrVuFzqgN6jEFE9CngAWKm1/nW+7y8IgtCd8XlcLL7l0/zsc5NoCUb4sAuydhbC1TMTuBQ4SSm12Po5swDtEARB6LYcO6aakyf0I6oP/AIteXf1aK3fBg7se4sgCEIPo19ZgAeumNYl15aZu4IgCL0MEX5BEIRehgi/IAhCL0OEXxAEoZchwi8IgtDLEOEXBEHoZYjwC4Ig9DJE+AVBEHoZSnfBrLADjVKqFtjX/KR9ga5Jcdd9kWfuHcgz9w7255mHa61TslweFMK/PyilFmitpxa6HflEnrl3IM/cO+iKZxZXjyAIQi9DhF8QBKGX0RuE/75CN6AAyDP3DuSZewcH/Jl7vI9fEARBSKQ3WPyCIAiCAxF+QRCEXkaPFn6l1OlKqY+VUquVUjcWuj0HAqXUUKXUG0qplUqpFUqp66zyKqXUHKXUKmtb6TjnJus7+FgpdVrhWr9/KKXcSqkPlFLPW5979DMrpSqUUk8qpT6yft8zesEzf9v6u16ulHpMKRXoac+slHpQKbVTKbXcUdbpZ1RKHaWUWmYdu9ta1jY3tNY98gdwA2uAUYAPWAJMLHS7DsBzDQSmWPtlwCfAROB24Ear/EbgF9b+ROvZ/cBI6ztxF/o59vHZvwM8Cjxvfe7Rzwz8BbjK2vcBFT35mYHBwDqgyPr8d+CKnvbMwCxgCrDcUdbpZwTmAzMwKxq+BJyRaxt6ssV/NLBaa71Wax0EHgfOLXCb9hut9Tat9SJrvxFYifmHORcjFFjbz1r75wKPa63btdbrgNWY7+agQik1BDgL+JOjuMc+s1KqHCMQDwBorYNa63p68DNbeIAipZQHKAa20sOeWWv9FrAnqbhTz6iUGgiUa63f1aYX+KvjnA7pycI/GNjk+LzZKusxKKVGAEcC84D+WuttYDoHoJ9Vrad8D3cC3weijrKe/MyjgFrgIcu99SelVAk9+Jm11luAXwIbgW3AXq31K/TgZ3bQ2WccbO0nl+dETxb+dP6uHhO7qpQqBf4JXK+1bshWNU3ZQfU9KKXOBnZqrRfmekqasoPqmTGW7xTgD1rrI4FmjAsgEwf9M1t+7XMxLo1BQIlS6kvZTklTdlA9cw5kesb9evaeLPybgaGOz0Mwr40HPUopL0b0H9FaP2UV77Be/7C2O63ynvA9zATOUUqtx7jsTlJKPUzPfubNwGat9Tzr85OYjqAnP/MpwDqtda3WOgQ8BRxLz35mm84+42ZrP7k8J3qy8L8PjFVKjVRK+YCLgecK3Kb9xhq5fwBYqbX+tePQc8Dl1v7lwLOO8ouVUn6l1EhgLGZQ6KBBa32T1nqI1noE5vf4utb6S/TsZ94ObFJKjbeKTgY+pAc/M8bFM10pVWz9nZ+MGcPqyc9s06lntNxBjUqp6dZ3dZnjnI4p9Ah3F4+en4mJelkD3Fzo9hygZzoO80q3FFhs/ZwJVAOvAausbZXjnJut7+BjOjHy3x1/gBOIR/X06GcGJgMLrN/1M0BlL3jmW4GPgOXA3zDRLD3qmYHHMGMYIYzlfuW+PCMw1fqe1gD3YGViyOVHUjYIgiD0Mnqyq0cQBEFIgwi/IAhCL0OEXxAEoZchwi8IgtDLEOEXBEHoZYjwCwKglIoopRY7fg5YNlel1AhnJkZBKDSeQjdAELoJrVrryYVuhCDkA7H4BSELSqn1SqlfKKXmWz9jrPLhSqnXlFJLre0wq7y/UupppdQS6+dY61JupdT9Vq75V5RSRQV7KKHXI8IvCIaiJFfPRY5jDVrrozGzI++0yu4B/qq1Phx4BLjbKr8bmKu1PgKTW2eFVT4W+J3W+lCgHji/S59GELIgM3cFAVBKNWmtS9OUrwdO0lqvtZLjbddaVyuldgEDtdYhq3yb1rqvUqoWGKK1bndcYwQwR2s91vr8A8Crtf5pHh5NEFIQi18QOkZn2M9UJx3tjv0IMr4mFBARfkHomIsc23et/XcwmUIBLgHetvZfA74OsTWCy/PVSEHIFbE6BMFQpJRa7Pj8b621HdLpV0rNwxhKX7DKvgU8qJS6AbNS1pet8uuA+5RSV2Is+69jMjEKQrdBfPyCkAXLxz9Va72r0G0RhAOFuHoEQRB6GWLxC4Ig9DLE4hcEQehliPALgiD0MkT4BUEQehki/IIgCL0MEX5BEIRexv8Hprp1kUIFqG4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot epoch loss to test for convergence\n",
    "plt.plot(epoch_loss_averages)\n",
    "plt.plot(test_epoch_loss_averages)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0808f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
