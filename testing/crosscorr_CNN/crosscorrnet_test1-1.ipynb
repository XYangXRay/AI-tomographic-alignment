{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8fed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential packages\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import tomography and imaging packages\n",
    "import tomopy\n",
    "from skimage.transform import rotate, AffineTransform\n",
    "from skimage import transform as tf\n",
    "from scipy.fft import fft2, fftshift\n",
    "from scipy.signal import correlate\n",
    "\n",
    "# Import neural net packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.profiler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a6e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Environment: pytorch\n",
      "Cuda Version: 11.8\n",
      "Cuda Availability: True\n",
      "/home/liam/Projects/Tomographic Alignment\r\n"
     ]
    }
   ],
   "source": [
    "# Checking to ensure environment and cuda are correct\n",
    "print(\"Working Environment: {}\".format(os.environ['CONDA_DEFAULT_ENV']))\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"Cuda Version: {}\".format(torch.version.cuda))\n",
    "print(\"Cuda Availability: {}\".format(torch.cuda.is_available()))\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2db6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr_reprojection(data, entries):\n",
    "    \n",
    "    ang = tomopy.angles(data[0][0][0, 0].shape[0])\n",
    "    _rec = 1e-12 * np.ones((data[0][0][0, 0].shape[1], data[0][0][0, 0].shape[2], data[0][0][0, 0].shape[2]))\n",
    "    data_copy = data.copy()\n",
    "    out_data = np.zeros((entries, 2), dtype = object)\n",
    "    \n",
    "    for i in range (entries):\n",
    "    \n",
    "        out_data[i, 0] = np.zeros((1, 1, data[0][0][0, 0].shape[0], data[0][0][0, 0].shape[1] * 2 - 1,\n",
    "                                      data[0][0][0, 0].shape[2] * 2 - 1))\n",
    "        out_data[i, 1] = data_copy[i, 1]\n",
    "        \n",
    "        rec = tomopy.recon(data_copy[i][0][0, 0], ang, center = None, \n",
    "                            algorithm = 'mlem', init_recon = _rec)\n",
    "        reproj = tomopy.project(rec, ang, center = None, pad = False)\n",
    "        \n",
    "        for j in range (data[0][0][0, 0].shape[0]):\n",
    "            \n",
    "            out_data[i, 0][0, 0, j] = correlate(data_copy[i][0][0, 0, j], reproj[j], method = 'fft')\n",
    "        \n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ecfea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_projections(data, entries):\n",
    "    \n",
    "    projections = np.zeros((entries * data[0][0][0, 0].shape[0], 2), dtype = object)\n",
    "\n",
    "    for i in range (entries):\n",
    "\n",
    "        for j in range (data[0][0][0, 0].shape[0]):\n",
    "\n",
    "            projections[i * data[0][0][0, 0].shape[0] + j, 0] = data[i, 0][0, 0, j, :, :]\n",
    "            projections[i * data[0][0][0, 0].shape[0] + j, 1] = np.asarray([data[i, 1][0, 2 * j], \n",
    "                                                                            data[i, 1][0, 2 * j + 1]])\n",
    "            \n",
    "    return projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0408a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Projections: 4500\n"
     ]
    }
   ],
   "source": [
    "# Loading data, 25 entries of 128 resolution shepp3ds\n",
    "res = 128\n",
    "entries = 25\n",
    "data = []\n",
    "\n",
    "for i in range(entries):\n",
    "    data.append(np.load('./shepp{}-{}/shepp{}-{}_{}.npy'.format(res, entries, res, entries, i), \n",
    "                        allow_pickle = True))\n",
    "    \n",
    "data = np.asarray(data)\n",
    "angles = entries * data[0][0][0, 0].shape[0]\n",
    "print(\"Total Projections: {}\".format(angles))\n",
    "crosscorr_reproj = crosscorr_reprojection(data, entries)\n",
    "crosscorr_data = to_projections(crosscorr_reproj, entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f58f197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 2)\n",
      "(1, 1, 255, 367)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "for i in range (crosscorr_data.shape[0]):\n",
    "    \n",
    "    crosscorr_data[i, 0] = np.expand_dims(crosscorr_data[i, 0], axis = 0)\n",
    "    crosscorr_data[i, 0] = np.expand_dims(crosscorr_data[i, 0], axis = 0)\n",
    "    crosscorr_data[i, 1] = np.expand_dims(crosscorr_data[i, 1], axis = 0)\n",
    "    \n",
    "print(crosscorr_data.shape)\n",
    "print(crosscorr_data[0, 0].shape)\n",
    "print(crosscorr_data[0, 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73524b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Dataset: (3600, 2)\n",
      "Shape of Testing Dataset: (900, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of training and testing splits\n",
    "trainset, testset = np.split(crosscorr_data, [int(angles * 4 / 5)])\n",
    "print(\"Shape of Training Dataset: {}\".format(trainset.shape))\n",
    "print(\"Shape of Testing Dataset: {}\".format(testset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5df0c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def norm(proj):\n",
    "    proj = (proj - torch.min(proj)) / (torch.max(proj) - torch.min(proj))\n",
    "    return proj\n",
    "\n",
    "# Get inplanes for resnet\n",
    "def get_inplanes():\n",
    "    return [64, 128, 256, 512]\n",
    "\n",
    "\n",
    "# Preset for a 3x3x3 kernel convolution\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "# Preset for a 1x1x1 kernel convolution\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=1,\n",
    "                     stride=stride,\n",
    "                     bias=False)\n",
    "\n",
    "# Basic block for resnet\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# Bottleneck block for resnet\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv1x1(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Resnet structure\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 block_inplanes,\n",
    "                 n_input_channels=1,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 shortcut_type='B',\n",
    "                 widen_factor=1.0,\n",
    "                 n_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
    "\n",
    "        self.in_planes = block_inplanes[0]\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_input_channels,\n",
    "                               self.in_planes,\n",
    "                               kernel_size=(conv1_t_size, 7),\n",
    "                               stride=(conv1_t_stride, 2),\n",
    "                               padding=(conv1_t_size // 2, 3),\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n",
    "                                       shortcut_type)\n",
    "        self.layer2 = self._make_layer(block,\n",
    "                                       block_inplanes[1],\n",
    "                                       layers[1],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer3 = self._make_layer(block,\n",
    "                                       block_inplanes[2],\n",
    "                                       layers[2],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer4 = self._make_layer(block,\n",
    "                                       block_inplanes[3],\n",
    "                                       layers[3],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _downsample_basic_block(self, x, planes, stride):\n",
    "        out = F.avg_pool2d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n",
    "                                out.size(3), out.size(4))\n",
    "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "            zero_pads = zero_pads.cuda()\n",
    "\n",
    "        out = torch.cat([out.data, zero_pads], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # make layer helper function\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            \n",
    "                downsample = nn.Sequential(\n",
    "                    conv1x1(self.in_planes, planes * block.expansion, stride),\n",
    "                    nn.BatchNorm2d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(in_planes=self.in_planes,\n",
    "                  planes=planes,\n",
    "                  stride=stride,\n",
    "                  downsample=downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Generates form of resnet\n",
    "def generate_model(model_depth, **kwargs):\n",
    "    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
    "\n",
    "    if model_depth == 10:\n",
    "        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 18:\n",
    "        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 34:\n",
    "        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 50:\n",
    "        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 101:\n",
    "        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 152:\n",
    "        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 200:\n",
    "        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6861d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 2]                    --\n",
       "├─Conv2d: 1-1                            [1, 64, 255, 184]         3,136\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 255, 184]         128\n",
       "├─ReLU: 1-3                              [1, 64, 255, 184]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 128, 92]          --\n",
       "├─Sequential: 1-5                        [1, 256, 128, 92]         --\n",
       "│    └─Bottleneck: 2-1                   [1, 256, 128, 92]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 128, 92]          4,096\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 128, 92]          36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-7                  [1, 256, 128, 92]         16,384\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 256, 128, 92]         512\n",
       "│    │    └─Sequential: 3-9              [1, 256, 128, 92]         16,896\n",
       "│    │    └─ReLU: 3-10                   [1, 256, 128, 92]         --\n",
       "│    └─Bottleneck: 2-2                   [1, 256, 128, 92]         --\n",
       "│    │    └─Conv2d: 3-11                 [1, 64, 128, 92]          16,384\n",
       "│    │    └─BatchNorm2d: 3-12            [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-13                   [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-14                 [1, 64, 128, 92]          36,864\n",
       "│    │    └─BatchNorm2d: 3-15            [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-16                   [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-17                 [1, 256, 128, 92]         16,384\n",
       "│    │    └─BatchNorm2d: 3-18            [1, 256, 128, 92]         512\n",
       "│    │    └─ReLU: 3-19                   [1, 256, 128, 92]         --\n",
       "│    └─Bottleneck: 2-3                   [1, 256, 128, 92]         --\n",
       "│    │    └─Conv2d: 3-20                 [1, 64, 128, 92]          16,384\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-22                   [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-23                 [1, 64, 128, 92]          36,864\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 64, 128, 92]          128\n",
       "│    │    └─ReLU: 3-25                   [1, 64, 128, 92]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 128, 92]         16,384\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 128, 92]         512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 128, 92]         --\n",
       "├─Sequential: 1-6                        [1, 512, 64, 46]          --\n",
       "│    └─Bottleneck: 2-4                   [1, 512, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 128, 128, 92]         32,768\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 128, 128, 92]         256\n",
       "│    │    └─ReLU: 3-31                   [1, 128, 128, 92]         --\n",
       "│    │    └─Conv2d: 3-32                 [1, 128, 64, 46]          147,456\n",
       "│    │    └─BatchNorm2d: 3-33            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-34                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-35                 [1, 512, 64, 46]          65,536\n",
       "│    │    └─BatchNorm2d: 3-36            [1, 512, 64, 46]          1,024\n",
       "│    │    └─Sequential: 3-37             [1, 512, 64, 46]          132,096\n",
       "│    │    └─ReLU: 3-38                   [1, 512, 64, 46]          --\n",
       "│    └─Bottleneck: 2-5                   [1, 512, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-39                 [1, 128, 64, 46]          65,536\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-41                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-42                 [1, 128, 64, 46]          147,456\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-44                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-45                 [1, 512, 64, 46]          65,536\n",
       "│    │    └─BatchNorm2d: 3-46            [1, 512, 64, 46]          1,024\n",
       "│    │    └─ReLU: 3-47                   [1, 512, 64, 46]          --\n",
       "│    └─Bottleneck: 2-6                   [1, 512, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-48                 [1, 128, 64, 46]          65,536\n",
       "│    │    └─BatchNorm2d: 3-49            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-50                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-51                 [1, 128, 64, 46]          147,456\n",
       "│    │    └─BatchNorm2d: 3-52            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-53                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-54                 [1, 512, 64, 46]          65,536\n",
       "│    │    └─BatchNorm2d: 3-55            [1, 512, 64, 46]          1,024\n",
       "│    │    └─ReLU: 3-56                   [1, 512, 64, 46]          --\n",
       "│    └─Bottleneck: 2-7                   [1, 512, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-57                 [1, 128, 64, 46]          65,536\n",
       "│    │    └─BatchNorm2d: 3-58            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-59                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-60                 [1, 128, 64, 46]          147,456\n",
       "│    │    └─BatchNorm2d: 3-61            [1, 128, 64, 46]          256\n",
       "│    │    └─ReLU: 3-62                   [1, 128, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-63                 [1, 512, 64, 46]          65,536\n",
       "│    │    └─BatchNorm2d: 3-64            [1, 512, 64, 46]          1,024\n",
       "│    │    └─ReLU: 3-65                   [1, 512, 64, 46]          --\n",
       "├─Sequential: 1-7                        [1, 1024, 32, 23]         --\n",
       "│    └─Bottleneck: 2-8                   [1, 1024, 32, 23]         --\n",
       "│    │    └─Conv2d: 3-66                 [1, 256, 64, 46]          131,072\n",
       "│    │    └─BatchNorm2d: 3-67            [1, 256, 64, 46]          512\n",
       "│    │    └─ReLU: 3-68                   [1, 256, 64, 46]          --\n",
       "│    │    └─Conv2d: 3-69                 [1, 256, 32, 23]          589,824\n",
       "│    │    └─BatchNorm2d: 3-70            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-71                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-72                 [1, 1024, 32, 23]         262,144\n",
       "│    │    └─BatchNorm2d: 3-73            [1, 1024, 32, 23]         2,048\n",
       "│    │    └─Sequential: 3-74             [1, 1024, 32, 23]         526,336\n",
       "│    │    └─ReLU: 3-75                   [1, 1024, 32, 23]         --\n",
       "│    └─Bottleneck: 2-9                   [1, 1024, 32, 23]         --\n",
       "│    │    └─Conv2d: 3-76                 [1, 256, 32, 23]          262,144\n",
       "│    │    └─BatchNorm2d: 3-77            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-78                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-79                 [1, 256, 32, 23]          589,824\n",
       "│    │    └─BatchNorm2d: 3-80            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-81                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-82                 [1, 1024, 32, 23]         262,144\n",
       "│    │    └─BatchNorm2d: 3-83            [1, 1024, 32, 23]         2,048\n",
       "│    │    └─ReLU: 3-84                   [1, 1024, 32, 23]         --\n",
       "│    └─Bottleneck: 2-10                  [1, 1024, 32, 23]         --\n",
       "│    │    └─Conv2d: 3-85                 [1, 256, 32, 23]          262,144\n",
       "│    │    └─BatchNorm2d: 3-86            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-87                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-88                 [1, 256, 32, 23]          589,824\n",
       "│    │    └─BatchNorm2d: 3-89            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-90                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-91                 [1, 1024, 32, 23]         262,144\n",
       "│    │    └─BatchNorm2d: 3-92            [1, 1024, 32, 23]         2,048\n",
       "│    │    └─ReLU: 3-93                   [1, 1024, 32, 23]         --\n",
       "│    └─Bottleneck: 2-11                  [1, 1024, 32, 23]         --\n",
       "│    │    └─Conv2d: 3-94                 [1, 256, 32, 23]          262,144\n",
       "│    │    └─BatchNorm2d: 3-95            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-96                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-97                 [1, 256, 32, 23]          589,824\n",
       "│    │    └─BatchNorm2d: 3-98            [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-99                   [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-100                [1, 1024, 32, 23]         262,144\n",
       "│    │    └─BatchNorm2d: 3-101           [1, 1024, 32, 23]         2,048\n",
       "│    │    └─ReLU: 3-102                  [1, 1024, 32, 23]         --\n",
       "│    └─Bottleneck: 2-12                  [1, 1024, 32, 23]         --\n",
       "│    │    └─Conv2d: 3-103                [1, 256, 32, 23]          262,144\n",
       "│    │    └─BatchNorm2d: 3-104           [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-105                  [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-106                [1, 256, 32, 23]          589,824\n",
       "│    │    └─BatchNorm2d: 3-107           [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-108                  [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-109                [1, 1024, 32, 23]         262,144\n",
       "│    │    └─BatchNorm2d: 3-110           [1, 1024, 32, 23]         2,048\n",
       "│    │    └─ReLU: 3-111                  [1, 1024, 32, 23]         --\n",
       "│    └─Bottleneck: 2-13                  [1, 1024, 32, 23]         --\n",
       "│    │    └─Conv2d: 3-112                [1, 256, 32, 23]          262,144\n",
       "│    │    └─BatchNorm2d: 3-113           [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-114                  [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-115                [1, 256, 32, 23]          589,824\n",
       "│    │    └─BatchNorm2d: 3-116           [1, 256, 32, 23]          512\n",
       "│    │    └─ReLU: 3-117                  [1, 256, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-118                [1, 1024, 32, 23]         262,144\n",
       "│    │    └─BatchNorm2d: 3-119           [1, 1024, 32, 23]         2,048\n",
       "│    │    └─ReLU: 3-120                  [1, 1024, 32, 23]         --\n",
       "├─Sequential: 1-8                        [1, 2048, 16, 12]         --\n",
       "│    └─Bottleneck: 2-14                  [1, 2048, 16, 12]         --\n",
       "│    │    └─Conv2d: 3-121                [1, 512, 32, 23]          524,288\n",
       "│    │    └─BatchNorm2d: 3-122           [1, 512, 32, 23]          1,024\n",
       "│    │    └─ReLU: 3-123                  [1, 512, 32, 23]          --\n",
       "│    │    └─Conv2d: 3-124                [1, 512, 16, 12]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-125           [1, 512, 16, 12]          1,024\n",
       "│    │    └─ReLU: 3-126                  [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-127                [1, 2048, 16, 12]         1,048,576\n",
       "│    │    └─BatchNorm2d: 3-128           [1, 2048, 16, 12]         4,096\n",
       "│    │    └─Sequential: 3-129            [1, 2048, 16, 12]         2,101,248\n",
       "│    │    └─ReLU: 3-130                  [1, 2048, 16, 12]         --\n",
       "│    └─Bottleneck: 2-15                  [1, 2048, 16, 12]         --\n",
       "│    │    └─Conv2d: 3-131                [1, 512, 16, 12]          1,048,576\n",
       "│    │    └─BatchNorm2d: 3-132           [1, 512, 16, 12]          1,024\n",
       "│    │    └─ReLU: 3-133                  [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-134                [1, 512, 16, 12]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-135           [1, 512, 16, 12]          1,024\n",
       "│    │    └─ReLU: 3-136                  [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-137                [1, 2048, 16, 12]         1,048,576\n",
       "│    │    └─BatchNorm2d: 3-138           [1, 2048, 16, 12]         4,096\n",
       "│    │    └─ReLU: 3-139                  [1, 2048, 16, 12]         --\n",
       "│    └─Bottleneck: 2-16                  [1, 2048, 16, 12]         --\n",
       "│    │    └─Conv2d: 3-140                [1, 512, 16, 12]          1,048,576\n",
       "│    │    └─BatchNorm2d: 3-141           [1, 512, 16, 12]          1,024\n",
       "│    │    └─ReLU: 3-142                  [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-143                [1, 512, 16, 12]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-144           [1, 512, 16, 12]          1,024\n",
       "│    │    └─ReLU: 3-145                  [1, 512, 16, 12]          --\n",
       "│    │    └─Conv2d: 3-146                [1, 2048, 16, 12]         1,048,576\n",
       "│    │    └─BatchNorm2d: 3-147           [1, 2048, 16, 12]         4,096\n",
       "│    │    └─ReLU: 3-148                  [1, 2048, 16, 12]         --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 2048, 1, 1]           --\n",
       "├─Linear: 1-10                           [1, 2]                    4,098\n",
       "==========================================================================================\n",
       "Total params: 23,505,858\n",
       "Trainable params: 23,505,858\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 15.17\n",
       "==========================================================================================\n",
       "Input size (MB): 0.37\n",
       "Forward/backward pass size (MB): 668.93\n",
       "Params size (MB): 94.02\n",
       "Estimated Total Size (MB): 763.33\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = generate_model(50)\n",
    "summary(model, (1, 1, 255, 367))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "802e1625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared Cache\n",
      "Device: cuda:0\n",
      "Epoch: 0   Training Loss: 49.416168408869126 \n",
      "Epoch: 0   Validation Loss: 7.971459150877264 \n",
      "Epoch: 1   Training Loss: 8.965161440515763 \n",
      "Epoch: 1   Validation Loss: 7.9713978324913315 \n",
      "Epoch: 2   Training Loss: 8.964842759062853 \n",
      "Epoch: 2   Validation Loss: 7.971397876996133 \n",
      "Epoch: 3   Training Loss: 8.964842715136385 \n",
      "Epoch: 3   Validation Loss: 7.971397876996133 \n",
      "Epoch: 4   Training Loss: 8.964842724395284 \n",
      "Epoch: 4   Validation Loss: 7.971397876996133 \n",
      "Epoch: 5   Training Loss: 8.964842710258583 \n",
      "Epoch: 5   Validation Loss: 7.971397876996133 \n",
      "Epoch: 6   Training Loss: 8.964842710258583 \n",
      "Epoch: 6   Validation Loss: 7.971397876996133 \n",
      "Epoch: 7   Training Loss: 8.964842710258583 \n",
      "Epoch: 7   Validation Loss: 7.971397876996133 \n",
      "Epoch: 8   Training Loss: 8.964842710258583 \n",
      "Epoch: 8   Validation Loss: 7.971397876996133 \n",
      "Epoch: 9   Training Loss: 8.964842710258583 \n",
      "Epoch: 9   Validation Loss: 7.971397876996133 \n",
      "Epoch: 10   Training Loss: 8.964842710258583 \n",
      "Epoch: 10   Validation Loss: 7.971397876996133 \n",
      "Epoch: 11   Training Loss: 8.964842710258583 \n",
      "Epoch: 11   Validation Loss: 7.971397876996133 \n",
      "Epoch: 12   Training Loss: 8.964842710258583 \n",
      "Epoch: 12   Validation Loss: 7.971397876996133 \n",
      "Epoch: 13   Training Loss: 8.964842710258583 \n",
      "Epoch: 13   Validation Loss: 7.971397876996133 \n",
      "Epoch: 14   Training Loss: 8.964842710258583 \n",
      "Epoch: 14   Validation Loss: 7.971397876996133 \n",
      "Epoch: 15   Training Loss: 8.964842710258583 \n",
      "Epoch: 15   Validation Loss: 7.971397876996133 \n",
      "Epoch: 16   Training Loss: 8.964842710258583 \n",
      "Epoch: 16   Validation Loss: 7.971397876996133 \n",
      "Epoch: 17   Training Loss: 8.964842710258583 \n",
      "Epoch: 17   Validation Loss: 7.971397876996133 \n",
      "Epoch: 18   Training Loss: 8.964842710258583 \n",
      "Epoch: 18   Validation Loss: 7.971397876996133 \n",
      "Epoch: 19   Training Loss: 8.964842710258583 \n",
      "Epoch: 19   Validation Loss: 7.971397876996133 \n",
      "Epoch: 20   Training Loss: 8.964842710258583 \n",
      "Epoch: 20   Validation Loss: 7.971397876996133 \n",
      "Epoch: 21   Training Loss: 8.964842710258583 \n",
      "Epoch: 21   Validation Loss: 7.971397876996133 \n",
      "Epoch: 22   Training Loss: 8.964842710258583 \n",
      "Epoch: 22   Validation Loss: 7.971397876996133 \n",
      "Epoch: 23   Training Loss: 8.964842710258583 \n",
      "Epoch: 23   Validation Loss: 7.971397876996133 \n",
      "Epoch: 24   Training Loss: 8.964842710258583 \n",
      "Epoch: 24   Validation Loss: 7.971397876996133 \n",
      "Epoch: 25   Training Loss: 8.964842710258583 \n",
      "Epoch: 25   Validation Loss: 7.971397876996133 \n",
      "Epoch: 26   Training Loss: 8.964842710258583 \n",
      "Epoch: 26   Validation Loss: 7.971397876996133 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     45\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 46\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared Cache\")\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# Create writer and profiler to analyze loss over each epoch\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "# Set device to CUDA if available, initialize model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "net = generate_model(50)\n",
    "net.to(device)\n",
    "\n",
    "# Set up optimizer and loss function, set number of epochs\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-2, momentum=0.9, weight_decay = 0)\n",
    "criterion = nn.MSELoss(reduction = 'mean')\n",
    "criterion.to(device)\n",
    "num_epochs = 1000\n",
    "\n",
    "# Iniitializing variables to show statistics\n",
    "iteration = 0\n",
    "test_iteration = 0\n",
    "loss_list = []\n",
    "test_loss_list = []\n",
    "epoch_loss_averages = []\n",
    "test_epoch_loss_averages = []\n",
    "\n",
    "# Iterates over dataset multiple times\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    test_epoch_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(trainset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(data[0]).to(device).float()), torch.from_numpy(data[1]).to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs).to(device)\n",
    "        loss = criterion(outputs, truths)\n",
    "        # writer.add_scalar(\"Loss / Train\", loss, epoch) # adds training loss scalar\n",
    "        loss_list.append(loss.cpu().detach().numpy())\n",
    "        epoch_loss += loss.cpu().detach().numpy()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration += 1\n",
    "        if iteration % trainset.shape[0] == 0:\n",
    "            epoch_loss_averages.append(epoch_loss / trainset.shape[0])\n",
    "            print('Epoch: {}   Training Loss: {} '.format(epoch, epoch_loss / trainset.shape[0]))\n",
    "            \n",
    "    for i, test_data in enumerate(testset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(test_data[0]).to(device).float()), torch.from_numpy(test_data[1]).to(device).float()\n",
    "        outputs = net(inputs).to(device)\n",
    "        test_loss = criterion(outputs, truths)\n",
    "        \n",
    "        # writer.add_scalar(\"Loss / Test\", test_loss, epoch) # adds testing loss scalar\n",
    "        test_loss_list.append(test_loss.cpu().detach().numpy())\n",
    "        test_epoch_loss += test_loss.cpu().detach().numpy()\n",
    "        \n",
    "        test_iteration +=1\n",
    "        if test_iteration % testset.shape[0] == 0:\n",
    "            test_epoch_loss_averages.append(test_epoch_loss / testset.shape[0])\n",
    "            print('Epoch: {}   Validation Loss: {} '.format(epoch, test_epoch_loss / testset.shape[0]))\n",
    "            \n",
    "# writer.flush()\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af95666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot epoch loss to test for convergence\n",
    "plt.plot(epoch_loss_averages)\n",
    "plt.plot(test_epoch_loss_averages)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e965c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
