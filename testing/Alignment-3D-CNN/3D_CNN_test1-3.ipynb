{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38a6e3f",
   "metadata": {},
   "source": [
    "# 3D Convolutional Neural Network for Tomographic Alignment\n",
    "\n",
    "## Regular CNN\n",
    "\n",
    "In order to test potential methods for performing automatic tomographic alignment using neural networks, we start with a standard model using a three dimensional convolution. The main problem with tomographic alignment is that a stack of two dimensional projections have to be processed simultameously for optimal results. While two dimensional convolutions can be used with channels corresponding with each projection angle, this is likely similar in computational complexity to a three dimensional neural network. Therefore the approach for this test is similar to video classification, where each frame in a video is is instead each projection angle.\n",
    "\n",
    "In order to test if this method can provide a convergence, phantoms will be artificially misaligned to create a training and testing set. But first packages for tomography, image transformations, and neural networks have to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857669b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential packages\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import tomography and imaging packages\n",
    "import tomopy\n",
    "from skimage.transform import rotate, AffineTransform\n",
    "from skimage import transform as tf\n",
    "\n",
    "# Import neural net packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.profiler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3211a1",
   "metadata": {},
   "source": [
    "Since the model will be a computationally complex CNN, we must ensure that the GPU is being used for calculations or else computation will be far too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a4cdb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Environment: pytorch\n",
      "Cuda Version: 11.8\n",
      "Cuda Availability: True\n"
     ]
    }
   ],
   "source": [
    "# Checking to ensure environment and cuda are correct\n",
    "print(\"Working Environment: {}\".format(os.environ['CONDA_DEFAULT_ENV']))\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"Cuda Version: {}\".format(torch.version.cuda))\n",
    "print(\"Cuda Availability: {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59c4b1",
   "metadata": {},
   "source": [
    "Now that the packages have been imported and CUDA is set up correctly, the next step is to create the dataset to be used for training and testing the neural network. The misalignment function is created to perform different random misalignments on the phantom set of tomographic scans. The shape of all of the data is then checked for errors and the data is split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6e3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for artificial misalignment\n",
    "def misalign(prj, mis_axis, ang_tilt = False, noise = False, background = False):\n",
    "    num_prj, col, row = prj.shape\n",
    "    dx = mis_axis[:, 0]\n",
    "    dy = mis_axis[:, 1]\n",
    "    prj_tmp = tomopy.shift_images(prj, dx, dy)\n",
    "    \n",
    "    for i in range(num_prj):\n",
    "        d_row, d_col, d_ang = mis_axis[i]\n",
    "        \n",
    "        if ang_tilt == True:\n",
    "            prj_tmp[i, :, :] = rotate(prj[i,:,:], d_ang)\n",
    "        else:\n",
    "            prj_tmp[i, :, :] = prj[i,:,:]\n",
    "        \n",
    "        if noise == True:\n",
    "            prj_tmp[i, :, :] = random_noise(prj_tmp[i, :, :], mode = 'gaussian')\n",
    "            \n",
    "        if background == True:\n",
    "            prj_tmp[i, :, :] = prj_tmp[i, :, :]+np.random.random()/5\n",
    "            prj_tmp[i, :, :] = prj_tmp[i, :, :]/prj_tmp[i, :, :].max()  \n",
    "            \n",
    "    return prj_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d6d6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating ground truth tomography\n",
    "data = tomopy.shepp3d(256)\n",
    "ang = tomopy.angles(180)\n",
    "proj = tomopy.project(data, ang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad05f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset to store misaligned projections and \n",
    "entries = 120\n",
    "dataset = np.zeros((entries, 2), dtype = object)\n",
    "\n",
    "for i in range(entries):\n",
    "    # Randomly determined misalignment axis\n",
    "    mis_axis = np.random.normal(0, 1, (200, 3))\n",
    "    mis_axis[:, :1] = mis_axis[:, :1]*4\n",
    "    mis_axis = np.round(mis_axis).astype(int)\n",
    "    mis_axis_in = np.expand_dims(mis_axis, axis = 0)\n",
    "    \n",
    "    proj_mis = misalign(proj.copy(), mis_axis, ang_tilt = True)\n",
    "    proj_mis = np.expand_dims(proj_mis, axis = 0)\n",
    "    proj_mis = np.expand_dims(proj_mis, axis = 0)\n",
    "    \n",
    "    dataset[i, 0] = proj_mis\n",
    "    dataset[i, 1] = np.concatenate((mis_axis_in[:, :180, 0], mis_axis_in[:, :180, 1]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d7e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2   1  -5  -3   1   3  -7   3   1  -1   2   5   8   3   0   0   3  -2\n",
      "    7   5   2  -1   3  -6   1  -2   3  -8 -10  -8  -2   2 -12  -3  -2   6\n",
      "   -2   6  -4   0  -1   4   1  -2   8  -2  -4   2   5   0  -3  -1   9   4\n",
      "   -1   1   5   3   2  -7   1   3   8   0  -4   3  -4  -1  -2   0  -2   1\n",
      "    1  -4   7   1  -4   0   1   4   4  -1   5  -7   2   2  -6  -1   5   5\n",
      "   -2   0   2   3  -3   0  -4  -2   0  -1  -7   1  -2   1   2   1  -4  -6\n",
      "    0  -6  -4   5   5  -3  -4  -2  10   8  -2  -3   6  -4   5  -4  11   0\n",
      "    1  -5   7  -2   4   2   3  -3   0   4  -1  -5   2   2   1  -3  -6  -8\n",
      "   -9  -4  -3   2   1 -10  -2   9  -5  -2  -4  -4   2   1  -3  -7  -7   2\n",
      "    2  -2   2   4  -1  -1  -1  -1   5   0   2  -4   1   0   6  10   9   2\n",
      "   -1   0   1  -1   1   1   0  -1   1   1   0   1   1   1   1  -1  -1   1\n",
      "    0  -1  -2   0   0   2  -1   0  -1  -1  -2  -1  -1   0   1   0   0  -1\n",
      "   -2   3   0  -1  -1  -1  -1   0   1   0   0   1   1   2   0   2  -1   0\n",
      "   -1   0   0  -1   1   0  -1   0   0   0  -1   1   1  -1   1   0  -2   1\n",
      "   -1  -1   1   0   1   0   1   0   1  -1  -1   0   0   0   0  -1   1   1\n",
      "    0  -2   1  -1   0   1  -1   0  -2   1   1   0   0   0   0   0   1   1\n",
      "   -1   1  -2   2   0   1   2   1   1   0   0   0   0  -1   1  -1   0  -2\n",
      "   -2  -1   1   0  -1   1   1  -3   0  -1   1   2   0   1  -2  -1  -1  -1\n",
      "    0   0  -1   0   0  -1  -1   1   1   1  -3   0   1   1   1   1  -1  -1\n",
      "    1   0   1   0   0   1  -1   1  -1   0   0   0   0   0  -1   1   0   2]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a40b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 2)\n",
      "(2,)\n",
      "(1, 1, 180, 256, 366)\n",
      "(1, 360)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of dataset\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db372e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 2)\n",
      "(24, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of training and testing splits\n",
    "trainset, testset = np.split(dataset, [int(entries* 4 / 5)])\n",
    "print(trainset.shape)\n",
    "print(testset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3333fc",
   "metadata": {},
   "source": [
    "Now that the data has been set up, the CUDA cache should be cleared and the model will be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f9f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared Cache.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared Cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a0e77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def norm(proj):\n",
    "    proj = (proj - torch.min(proj)) / (torch.max(proj) - torch.min(proj))\n",
    "    return proj\n",
    "\n",
    "def g_norm(shift):\n",
    "    mean_tmp = torch.mean(shift)\n",
    "    std_tmp = torch.std(shift)\n",
    "    shift = (shift - mean_tmp) / std_tmp\n",
    "    # shift = (shift - torch.min(shift)) / (torch.max(shift) - torch.min(shift))\n",
    "    return 10 * shift\n",
    "\n",
    "# 3D CNN to determine shift parameters\n",
    "\n",
    "class CNN_3D_aligner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_3D_aligner, self).__init__()\n",
    "\n",
    "        self.group1 = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group2 = nn.Sequential(\n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group3 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Conv3d(64, 64, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group4 = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Conv3d(128, 128, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group5 = nn.Sequential(\n",
    "            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group6 = nn.Sequential(\n",
    "            nn.Conv3d(256, 16, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(7040, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 360)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = norm(x)\n",
    "        \n",
    "        x = self.group1(x)\n",
    "        x = self.group2(x)\n",
    "        x = self.group3(x)\n",
    "        x = self.group4(x)\n",
    "        x = self.group5(x)\n",
    "        x = self.group6(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = g_norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24834a02",
   "metadata": {},
   "source": [
    "In order to ensure the network works and understand its structure before training data on it, use the summary function in order to get an understanding of the network and fix any linear algebra errors in creating the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3073b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN_3D_aligner                           [1, 360]                  --\n",
       "├─Sequential: 1-1                        [1, 16, 90, 128, 183]     --\n",
       "│    └─Conv3d: 2-1                       [1, 16, 180, 256, 366]    448\n",
       "│    └─BatchNorm3d: 2-2                  [1, 16, 180, 256, 366]    32\n",
       "│    └─Sigmoid: 2-3                      [1, 16, 180, 256, 366]    --\n",
       "│    └─MaxPool3d: 2-4                    [1, 16, 90, 128, 183]     --\n",
       "├─Sequential: 1-2                        [1, 32, 45, 64, 91]       --\n",
       "│    └─Conv3d: 2-5                       [1, 32, 90, 128, 183]     13,856\n",
       "│    └─BatchNorm3d: 2-6                  [1, 32, 90, 128, 183]     64\n",
       "│    └─Sigmoid: 2-7                      [1, 32, 90, 128, 183]     --\n",
       "│    └─MaxPool3d: 2-8                    [1, 32, 45, 64, 91]       --\n",
       "├─Sequential: 1-3                        [1, 64, 22, 32, 45]       --\n",
       "│    └─Conv3d: 2-9                       [1, 64, 45, 64, 91]       55,360\n",
       "│    └─BatchNorm3d: 2-10                 [1, 64, 45, 64, 91]       128\n",
       "│    └─Sigmoid: 2-11                     [1, 64, 45, 64, 91]       --\n",
       "│    └─Conv3d: 2-12                      [1, 64, 45, 64, 91]       110,656\n",
       "│    └─BatchNorm3d: 2-13                 [1, 64, 45, 64, 91]       128\n",
       "│    └─Sigmoid: 2-14                     [1, 64, 45, 64, 91]       --\n",
       "│    └─MaxPool3d: 2-15                   [1, 64, 22, 32, 45]       --\n",
       "├─Sequential: 1-4                        [1, 128, 11, 16, 22]      --\n",
       "│    └─Conv3d: 2-16                      [1, 128, 22, 32, 45]      221,312\n",
       "│    └─BatchNorm3d: 2-17                 [1, 128, 22, 32, 45]      256\n",
       "│    └─Sigmoid: 2-18                     [1, 128, 22, 32, 45]      --\n",
       "│    └─Conv3d: 2-19                      [1, 128, 22, 32, 45]      442,496\n",
       "│    └─BatchNorm3d: 2-20                 [1, 128, 22, 32, 45]      256\n",
       "│    └─Sigmoid: 2-21                     [1, 128, 22, 32, 45]      --\n",
       "│    └─MaxPool3d: 2-22                   [1, 128, 11, 16, 22]      --\n",
       "├─Sequential: 1-5                        [1, 256, 5, 8, 11]        --\n",
       "│    └─Conv3d: 2-23                      [1, 256, 11, 16, 22]      884,992\n",
       "│    └─BatchNorm3d: 2-24                 [1, 256, 11, 16, 22]      512\n",
       "│    └─Sigmoid: 2-25                     [1, 256, 11, 16, 22]      --\n",
       "│    └─Conv3d: 2-26                      [1, 256, 11, 16, 22]      1,769,728\n",
       "│    └─BatchNorm3d: 2-27                 [1, 256, 11, 16, 22]      512\n",
       "│    └─Sigmoid: 2-28                     [1, 256, 11, 16, 22]      --\n",
       "│    └─MaxPool3d: 2-29                   [1, 256, 5, 8, 11]        --\n",
       "├─Sequential: 1-6                        [1, 16, 5, 8, 11]         --\n",
       "│    └─Conv3d: 2-30                      [1, 16, 5, 8, 11]         110,608\n",
       "│    └─BatchNorm3d: 2-31                 [1, 16, 5, 8, 11]         32\n",
       "│    └─Sigmoid: 2-32                     [1, 16, 5, 8, 11]         --\n",
       "├─Flatten: 1-7                           [1, 7040]                 --\n",
       "├─Sequential: 1-8                        [1, 256]                  --\n",
       "│    └─Linear: 2-33                      [1, 512]                  3,604,992\n",
       "│    └─Sigmoid: 2-34                     [1, 512]                  --\n",
       "│    └─Dropout: 2-35                     [1, 512]                  --\n",
       "│    └─Linear: 2-36                      [1, 256]                  131,328\n",
       "├─Linear: 1-9                            [1, 360]                  92,520\n",
       "==========================================================================================\n",
       "Total params: 7,440,216\n",
       "Trainable params: 7,440,216\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 111.64\n",
       "==========================================================================================\n",
       "Input size (MB): 67.46\n",
       "Forward/backward pass size (MB): 6095.23\n",
       "Params size (MB): 29.76\n",
       "Estimated Total Size (MB): 6192.45\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model shape\n",
    "model = CNN_3D_aligner()\n",
    "summary(model, (1, 1, 180, 256, 366))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016ceed",
   "metadata": {},
   "source": [
    "Now the model can be trained, making sure to move all of the elements of the training process to the GPU to optimize computational speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625db813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Iteration: 96   Loss: 108.38527830441792 \n",
      "Iteration: 192   Loss: 108.40818643569946 \n",
      "Iteration: 288   Loss: 108.17373450597127 \n",
      "Iteration: 384   Loss: 108.26605947812398 \n",
      "Iteration: 480   Loss: 108.37963628768921 \n",
      "Iteration: 576   Loss: 108.22290086746216 \n",
      "Iteration: 672   Loss: 108.23287963867188 \n",
      "Iteration: 768   Loss: 108.33371567726135 \n",
      "Iteration: 864   Loss: 108.22760232289632 \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Dataloader for the trainset\n",
    "trainload = DataLoader(trainset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Create writer and profiler to analyze loss over each epoch\n",
    "writer = SummaryWriter()\n",
    "prof = torch.profiler.profile(\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/net3d'),\n",
    "    record_shapes=True, profile_memory=True, with_stack=True)\n",
    "prof.start()\n",
    "\n",
    "# Set device to CUDA if available, initialize model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "net = CNN_3D_aligner()\n",
    "net.to(device)\n",
    "\n",
    "# Set up optimizer and loss function, set number of epochs\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-2, momentum = 0.9)\n",
    "criterion = nn.MSELoss(reduction = 'mean')\n",
    "criterion.to(device)\n",
    "num_epochs = 50\n",
    "\n",
    "# Iniitializing variables to show statistics\n",
    "iteration = 0\n",
    "loss_list = []\n",
    "epoch_loss_averages = []\n",
    "\n",
    "# Iterates over dataset multiple times\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(trainset, 0):\n",
    "        inputs, truths = norm(torch.from_numpy(data[0]).to(device)), torch.from_numpy(data[1]).to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs).to(device)\n",
    "        loss = criterion(outputs, truths)\n",
    "        if i == 0 and epoch == num_epochs - 1:\n",
    "            print(truths)\n",
    "            print(\"_\"*75)\n",
    "            print(outputs)\n",
    "        writer.add_scalar(\"Loss / Train\", loss, epoch) # adds training loss scalar\n",
    "        loss_list.append(loss.cpu().detach().numpy())\n",
    "        epoch_loss += loss.cpu().detach().numpy()\n",
    "        loss.backward()\n",
    "        optimizer.step\n",
    "        prof.step\n",
    "\n",
    "        iteration += 1\n",
    "        if iteration % trainset.shape[0] == 0:\n",
    "            epoch_loss_averages.append(epoch_loss / trainset.shape[0])\n",
    "            print('Iteration: {}   Loss: {} '.format(iteration, epoch_loss / trainset.shape[0]))\n",
    "            \n",
    "prof.stop()\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c50b0",
   "metadata": {},
   "source": [
    "Now in order to observe convergence or lack thereof graphs of loss per iteration as well as a moving average based on each epoch are created for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a43387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot epoch loss to test for convergence\n",
    "plt.plot(epoch_loss_averages)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f0db4f",
   "metadata": {},
   "source": [
    "As seen above, this neural network fails to create convergence for the dataset. However, the fact that the loss does not stay completely constant is promising and it is possible that a deeper neural network can prove to solve this problem. The main restrictions at this point are memory allocation errors and different frameworks will have to be used in order to create a neural network with the depth necessary to observe convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
