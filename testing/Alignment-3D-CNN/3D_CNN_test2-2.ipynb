{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1586819",
   "metadata": {},
   "source": [
    "# 3D Convolutional Neural Network for Tomographic Alignment\n",
    "\n",
    "## Regular CNN Expanding the Dataset\n",
    "\n",
    "In order to test potential methods for performing automatic tomographic alignment using neural networks, we start with a standard model using a three dimensional convolution. The main problem with tomographic alignment is that a stack of two dimensional projections have to be processed simultameously for optimal results. While two dimensional convolutions can be used with channels corresponding with each projection angle, this is likely similar in computational complexity to a three dimensional neural network. Therefore the approach for this test is similar to video classification, where each frame in a video is is instead each projection angle. Now the dataset will be expanded to be larger in order to determine if this can be generalized on the phantom data.\n",
    "\n",
    "In order to test if this method can provide a convergence, phantoms will be artificially misaligned to create a training and testing set. But first packages for tomography, image transformations, and neural networks have to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1cdf4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential packages\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import tomography and imaging packages\n",
    "import tomopy\n",
    "from skimage.transform import rotate, AffineTransform\n",
    "from skimage import transform as tf\n",
    "\n",
    "# Import neural net packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.profiler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d73afe",
   "metadata": {},
   "source": [
    "Since the model will be a computationally complex CNN, we must ensure that the GPU is being used for calculations or else computation will be far too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0b669c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Environment: pytorch\n",
      "Cuda Version: 11.8\n",
      "Cuda Availability: True\n"
     ]
    }
   ],
   "source": [
    "# Checking to ensure environment and cuda are correct\n",
    "print(\"Working Environment: {}\".format(os.environ['CONDA_DEFAULT_ENV']))\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"Cuda Version: {}\".format(torch.version.cuda))\n",
    "print(\"Cuda Availability: {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73daf4d6",
   "metadata": {},
   "source": [
    "Now that the packages have been imported and CUDA is set up correctly, the next step is to create the dataset to be used for training and testing the neural network. The misalignment function is created to perform different random misalignments on the phantom set of tomographic scans. The shape of all of the data is then checked for errors and the data is split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d7e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for artificial misalignment\n",
    "def misalign(prj, mis_axis, ang_tilt = False, noise = False, background = False):\n",
    "    num_prj, col, row = prj.shape\n",
    "    dx = mis_axis[:, 0]\n",
    "    dy = mis_axis[:, 1]\n",
    "    prj_tmp = tomopy.shift_images(prj, dx, dy)\n",
    "    \n",
    "    for i in range(num_prj):\n",
    "        d_row, d_col, d_ang = mis_axis[i]\n",
    "        \n",
    "        if ang_tilt == True:\n",
    "            prj_tmp[i, :, :] = rotate(prj[i,:,:], d_ang)\n",
    "        else:\n",
    "            prj_tmp[i, :, :] = prj[i,:,:]\n",
    "        \n",
    "        if noise == True:\n",
    "            prj_tmp[i, :, :] = random_noise(prj_tmp[i, :, :], mode = 'gaussian')\n",
    "            \n",
    "        if background == True:\n",
    "            prj_tmp[i, :, :] = prj_tmp[i, :, :]+np.random.random()/5\n",
    "            prj_tmp[i, :, :] = prj_tmp[i, :, :]/prj_tmp[i, :, :].max()  \n",
    "            \n",
    "    return prj_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3b066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating ground truth tomography\n",
    "data = tomopy.shepp3d(128)\n",
    "ang = tomopy.angles(180)\n",
    "proj = tomopy.project(data, ang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a796e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset to store misaligned projections and \n",
    "training_entries = 1000\n",
    "entries = int(np.ceil(training_entries * 5 / 4))\n",
    "dataset = np.zeros((entries, 2), dtype = object)\n",
    "\n",
    "for i in range(entries):\n",
    "    # Randomly determined misalignment axis\n",
    "    mis_axis = np.random.normal(0, 1, (200, 3))\n",
    "    mis_axis[:, :1] = mis_axis[:, :1]*4\n",
    "    mis_axis = np.round(mis_axis).astype(int)\n",
    "    mis_axis_in = np.expand_dims(mis_axis, axis = 0)\n",
    "    \n",
    "    proj_mis = misalign(proj.copy(), mis_axis, ang_tilt = True)\n",
    "    proj_mis = np.expand_dims(proj_mis, axis = 0)\n",
    "    proj_mis = np.expand_dims(proj_mis, axis = 0)\n",
    "    \n",
    "    dataset[i, 0] = proj_mis\n",
    "    dataset[i, 1] = np.concatenate((mis_axis_in[:, :180, 0], mis_axis_in[:, :180, 1]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd11e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 2)\n",
      "(2,)\n",
      "(1, 1, 180, 128, 184)\n",
      "(1, 360)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of dataset\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb36ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(250, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of training and testing splits\n",
    "trainset, testset = np.split(dataset, [training_entries])\n",
    "print(trainset.shape)\n",
    "print(testset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b55a9d1",
   "metadata": {},
   "source": [
    "Now that the data has been set up, the CUDA cache should be cleared and the model will be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dfd20fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared Cache.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared Cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7319a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def norm(proj):\n",
    "    proj = (proj - torch.min(proj)) / (torch.max(proj) - torch.min(proj))\n",
    "    return proj\n",
    "\n",
    "# 3D CNN to determine shift parameters\n",
    "\n",
    "class CNN_3D_aligner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_3D_aligner, self).__init__()\n",
    "\n",
    "        self.group1 = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group2 = nn.Sequential(\n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
    "        )\n",
    "        \n",
    "        self.group3 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.Conv3d(64, 64, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group4 = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.Conv3d(128, 128, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group5 = nn.Sequential(\n",
    "            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group6 = nn.Sequential(\n",
    "            nn.Conv3d(256, 16, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.BatchNorm3d(16),\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1600, 512),\n",
    "            nn.Dropout(0.20),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 360)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = norm(x)\n",
    "        \n",
    "        x = self.group1(x)\n",
    "        x = self.group2(x)\n",
    "        x = self.group3(x)\n",
    "        x = self.group4(x)\n",
    "        x = self.group5(x)\n",
    "        x = self.group6(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f078f4",
   "metadata": {},
   "source": [
    "In order to ensure the network works and understand its structure before training data on it, use the summary function in order to get an understanding of the network and fix any linear algebra errors in creating the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78be5986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN_3D_aligner                           [1, 360]                  --\n",
       "├─Sequential: 1-1                        [1, 16, 90, 64, 92]       --\n",
       "│    └─Conv3d: 2-1                       [1, 16, 180, 128, 184]    448\n",
       "│    └─BatchNorm3d: 2-2                  [1, 16, 180, 128, 184]    32\n",
       "│    └─MaxPool3d: 2-3                    [1, 16, 90, 64, 92]       --\n",
       "├─Sequential: 1-2                        [1, 32, 45, 32, 46]       --\n",
       "│    └─Conv3d: 2-4                       [1, 32, 90, 64, 92]       13,856\n",
       "│    └─BatchNorm3d: 2-5                  [1, 32, 90, 64, 92]       64\n",
       "│    └─MaxPool3d: 2-6                    [1, 32, 45, 32, 46]       --\n",
       "├─Sequential: 1-3                        [1, 64, 22, 16, 23]       --\n",
       "│    └─Conv3d: 2-7                       [1, 64, 45, 32, 46]       55,360\n",
       "│    └─BatchNorm3d: 2-8                  [1, 64, 45, 32, 46]       128\n",
       "│    └─Conv3d: 2-9                       [1, 64, 45, 32, 46]       110,656\n",
       "│    └─BatchNorm3d: 2-10                 [1, 64, 45, 32, 46]       128\n",
       "│    └─MaxPool3d: 2-11                   [1, 64, 22, 16, 23]       --\n",
       "├─Sequential: 1-4                        [1, 128, 11, 8, 11]       --\n",
       "│    └─Conv3d: 2-12                      [1, 128, 22, 16, 23]      221,312\n",
       "│    └─BatchNorm3d: 2-13                 [1, 128, 22, 16, 23]      256\n",
       "│    └─Conv3d: 2-14                      [1, 128, 22, 16, 23]      442,496\n",
       "│    └─BatchNorm3d: 2-15                 [1, 128, 22, 16, 23]      256\n",
       "│    └─MaxPool3d: 2-16                   [1, 128, 11, 8, 11]       --\n",
       "├─Sequential: 1-5                        [1, 256, 5, 4, 5]         --\n",
       "│    └─Conv3d: 2-17                      [1, 256, 11, 8, 11]       884,992\n",
       "│    └─BatchNorm3d: 2-18                 [1, 256, 11, 8, 11]       512\n",
       "│    └─Conv3d: 2-19                      [1, 256, 11, 8, 11]       1,769,728\n",
       "│    └─BatchNorm3d: 2-20                 [1, 256, 11, 8, 11]       512\n",
       "│    └─MaxPool3d: 2-21                   [1, 256, 5, 4, 5]         --\n",
       "├─Sequential: 1-6                        [1, 16, 5, 4, 5]          --\n",
       "│    └─Conv3d: 2-22                      [1, 16, 5, 4, 5]          110,608\n",
       "│    └─BatchNorm3d: 2-23                 [1, 16, 5, 4, 5]          32\n",
       "├─Flatten: 1-7                           [1, 1600]                 --\n",
       "├─Sequential: 1-8                        [1, 256]                  --\n",
       "│    └─Linear: 2-24                      [1, 512]                  819,712\n",
       "│    └─Dropout: 2-25                     [1, 512]                  --\n",
       "│    └─Linear: 2-26                      [1, 256]                  131,328\n",
       "├─Linear: 1-9                            [1, 360]                  92,520\n",
       "==========================================================================================\n",
       "Total params: 4,654,936\n",
       "Trainable params: 4,654,936\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 28.19\n",
       "==========================================================================================\n",
       "Input size (MB): 16.96\n",
       "Forward/backward pass size (MB): 1533.38\n",
       "Params size (MB): 18.62\n",
       "Estimated Total Size (MB): 1568.96\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model shape\n",
    "model = CNN_3D_aligner()\n",
    "summary(model, (1, 1, 180, 128, 184))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae80e4",
   "metadata": {},
   "source": [
    "Now the model can be trained, making sure to move all of the elements of the training process to the GPU to optimize computational speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7b99b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Epoch: 0   Training Loss: 8.642643230438232 \n",
      "Epoch: 0   Validation Loss: 8.563699531555176 \n",
      "Epoch: 1   Training Loss: 8.609818638324738 \n",
      "Epoch: 1   Validation Loss: 8.655936079025269 \n",
      "Epoch: 2   Training Loss: 8.601429895401001 \n",
      "Epoch: 2   Validation Loss: 8.75670161819458 \n",
      "Epoch: 3   Training Loss: 8.596141483783722 \n",
      "Epoch: 3   Validation Loss: 8.724922523498535 \n",
      "Epoch: 4   Training Loss: 8.596008585929871 \n",
      "Epoch: 4   Validation Loss: 8.738231664657592 \n",
      "Epoch: 5   Training Loss: 8.59750215768814 \n",
      "Epoch: 5   Validation Loss: 8.696263256072998 \n",
      "Epoch: 6   Training Loss: 8.597915828704833 \n",
      "Epoch: 6   Validation Loss: 8.693554763793946 \n",
      "Epoch: 7   Training Loss: 8.594435597896576 \n",
      "Epoch: 7   Validation Loss: 8.742098991394043 \n",
      "Epoch: 8   Training Loss: 8.596364966869354 \n",
      "Epoch: 8   Validation Loss: 8.61024478340149 \n",
      "Epoch: 9   Training Loss: 8.59762455558777 \n",
      "Epoch: 9   Validation Loss: 8.57925965309143 \n",
      "Epoch: 10   Training Loss: 8.601579393863679 \n",
      "Epoch: 10   Validation Loss: 8.596027767181397 \n",
      "Epoch: 11   Training Loss: 8.603337718963623 \n",
      "Epoch: 11   Validation Loss: 8.608770505905152 \n",
      "Epoch: 12   Training Loss: 8.603864558696747 \n",
      "Epoch: 12   Validation Loss: 8.67343992805481 \n",
      "Epoch: 13   Training Loss: 8.605848488807679 \n",
      "Epoch: 13   Validation Loss: 8.764819351196289 \n",
      "Epoch: 14   Training Loss: 8.602890012264252 \n",
      "Epoch: 14   Validation Loss: 8.859895393371582 \n",
      "Epoch: 15   Training Loss: 8.606792022705077 \n",
      "Epoch: 15   Validation Loss: 8.685669309616088 \n",
      "Epoch: 16   Training Loss: 8.614018119812012 \n",
      "Epoch: 16   Validation Loss: 8.567536790847779 \n",
      "Epoch: 17   Training Loss: 8.603620011806488 \n",
      "Epoch: 17   Validation Loss: 8.61313737297058 \n",
      "Epoch: 18   Training Loss: 8.60241464614868 \n",
      "Epoch: 18   Validation Loss: 8.680630071640014 \n",
      "Epoch: 19   Training Loss: 8.607300417423248 \n",
      "Epoch: 19   Validation Loss: 8.58862961769104 \n",
      "Epoch: 20   Training Loss: 8.601197787761688 \n",
      "Epoch: 20   Validation Loss: 8.602088088989257 \n",
      "Epoch: 21   Training Loss: 8.604898280143738 \n",
      "Epoch: 21   Validation Loss: 8.613557676315308 \n",
      "Epoch: 22   Training Loss: 8.603074316024781 \n",
      "Epoch: 22   Validation Loss: 8.650557119369507 \n",
      "Epoch: 23   Training Loss: 8.60419024515152 \n",
      "Epoch: 23   Validation Loss: 8.69242133331299 \n",
      "Epoch: 24   Training Loss: 8.604509682655335 \n",
      "Epoch: 24   Validation Loss: 8.751962911605835 \n",
      "Epoch: 25   Training Loss: 8.601406640529632 \n",
      "Epoch: 25   Validation Loss: 8.722606924057008 \n",
      "Epoch: 26   Training Loss: 8.608542041778565 \n",
      "Epoch: 26   Validation Loss: 8.590826105117797 \n",
      "Epoch: 27   Training Loss: 8.607080678462982 \n",
      "Epoch: 27   Validation Loss: 8.62997686958313 \n",
      "Epoch: 28   Training Loss: 8.605176051616668 \n",
      "Epoch: 28   Validation Loss: 8.699435119628907 \n",
      "Epoch: 29   Training Loss: 8.601308044433594 \n",
      "Epoch: 29   Validation Loss: 8.705211187362671 \n",
      "Epoch: 30   Training Loss: 8.602211521625518 \n",
      "Epoch: 30   Validation Loss: 8.736840127944946 \n",
      "Epoch: 31   Training Loss: 8.602662942409516 \n",
      "Epoch: 31   Validation Loss: 8.68027346420288 \n",
      "Epoch: 32   Training Loss: 8.600839406967163 \n",
      "Epoch: 32   Validation Loss: 8.732575231552124 \n",
      "Epoch: 33   Training Loss: 8.599742582798005 \n",
      "Epoch: 33   Validation Loss: 8.713876049041748 \n",
      "Epoch: 34   Training Loss: 8.601327469348908 \n",
      "Epoch: 34   Validation Loss: 8.765871742248535 \n",
      "Epoch: 35   Training Loss: 8.599081798076629 \n",
      "Epoch: 35   Validation Loss: 8.69220952796936 \n",
      "Epoch: 36   Training Loss: 8.600093957424164 \n",
      "Epoch: 36   Validation Loss: 8.634541481018067 \n",
      "Epoch: 37   Training Loss: 8.600008925437928 \n",
      "Epoch: 37   Validation Loss: 8.651823141098022 \n",
      "Epoch: 38   Training Loss: 8.59736430978775 \n",
      "Epoch: 38   Validation Loss: 8.70300032043457 \n",
      "Epoch: 39   Training Loss: 8.59942568731308 \n",
      "Epoch: 39   Validation Loss: 8.629109157562256 \n",
      "Epoch: 40   Training Loss: 8.603304819107056 \n",
      "Epoch: 40   Validation Loss: 8.613108213424683 \n",
      "Epoch: 41   Training Loss: 8.602227033138275 \n",
      "Epoch: 41   Validation Loss: 8.603805116653442 \n",
      "Epoch: 42   Training Loss: 8.597913160800934 \n",
      "Epoch: 42   Validation Loss: 8.710223217010498 \n",
      "Epoch: 43   Training Loss: 8.600440314769745 \n",
      "Epoch: 43   Validation Loss: 8.708597818374633 \n",
      "Epoch: 44   Training Loss: 8.600829933166503 \n",
      "Epoch: 44   Validation Loss: 8.632564422607421 \n",
      "Epoch: 45   Training Loss: 8.605882345676422 \n",
      "Epoch: 45   Validation Loss: 8.6433154296875 \n",
      "Epoch: 46   Training Loss: 8.606276795387268 \n",
      "Epoch: 46   Validation Loss: 8.64630074119568 \n",
      "Epoch: 47   Training Loss: 8.604804677963257 \n",
      "Epoch: 47   Validation Loss: 8.607057647705078 \n",
      "Epoch: 48   Training Loss: 8.59949403333664 \n",
      "Epoch: 48   Validation Loss: 8.641657936096191 \n",
      "Epoch: 49   Training Loss: 8.59570946931839 \n",
      "Epoch: 49   Validation Loss: 8.608280401229859 \n",
      "Epoch: 50   Training Loss: 8.592539102077485 \n",
      "Epoch: 50   Validation Loss: 8.69165959739685 \n",
      "Epoch: 51   Training Loss: 8.596411864757538 \n",
      "Epoch: 51   Validation Loss: 8.766146320343017 \n",
      "Epoch: 52   Training Loss: 8.595455842018128 \n",
      "Epoch: 52   Validation Loss: 8.6847230758667 \n",
      "Epoch: 53   Training Loss: 8.593881870269776 \n",
      "Epoch: 53   Validation Loss: 8.6383314037323 \n",
      "Epoch: 54   Training Loss: 8.597429799556732 \n",
      "Epoch: 54   Validation Loss: 8.600742015838623 \n",
      "Epoch: 55   Training Loss: 8.596265556812286 \n",
      "Epoch: 55   Validation Loss: 8.648077123641968 \n",
      "Epoch: 56   Training Loss: 8.5938384642601 \n",
      "Epoch: 56   Validation Loss: 8.717011709213256 \n",
      "Epoch: 57   Training Loss: 8.590575532913208 \n",
      "Epoch: 57   Validation Loss: 8.753071712493897 \n",
      "Epoch: 58   Training Loss: 8.590800553321838 \n",
      "Epoch: 58   Validation Loss: 8.763410367965697 \n",
      "Epoch: 59   Training Loss: 8.59264864873886 \n",
      "Epoch: 59   Validation Loss: 8.650826524734496 \n",
      "Epoch: 60   Training Loss: 8.58941523218155 \n",
      "Epoch: 60   Validation Loss: 8.652995559692382 \n",
      "Epoch: 61   Training Loss: 8.590399143218994 \n",
      "Epoch: 61   Validation Loss: 8.635931674957275 \n",
      "Epoch: 62   Training Loss: 8.592882063865662 \n",
      "Epoch: 62   Validation Loss: 8.617905668258667 \n",
      "Epoch: 63   Training Loss: 8.59141896533966 \n",
      "Epoch: 63   Validation Loss: 8.618374483108521 \n",
      "Epoch: 64   Training Loss: 8.589748826026916 \n",
      "Epoch: 64   Validation Loss: 8.58871579170227 \n",
      "Epoch: 65   Training Loss: 8.590363153457641 \n",
      "Epoch: 65   Validation Loss: 8.658185684204101 \n",
      "Epoch: 66   Training Loss: 8.594501124382019 \n",
      "Epoch: 66   Validation Loss: 8.69124028968811 \n",
      "Epoch: 67   Training Loss: 8.592887406349183 \n",
      "Epoch: 67   Validation Loss: 8.704942953109741 \n",
      "Epoch: 68   Training Loss: 8.588219902038574 \n",
      "Epoch: 68   Validation Loss: 8.74684776687622 \n",
      "Epoch: 69   Training Loss: 8.592976072311401 \n",
      "Epoch: 69   Validation Loss: 8.771020797729491 \n",
      "Epoch: 70   Training Loss: 8.59382569694519 \n",
      "Epoch: 70   Validation Loss: 8.768240657806396 \n",
      "Epoch: 71   Training Loss: 8.591383910179138 \n",
      "Epoch: 71   Validation Loss: 8.816209163665771 \n",
      "Epoch: 72   Training Loss: 8.594525483608246 \n",
      "Epoch: 72   Validation Loss: 8.683739669799804 \n",
      "Epoch: 73   Training Loss: 8.590532798290253 \n",
      "Epoch: 73   Validation Loss: 8.719662452697754 \n",
      "Epoch: 74   Training Loss: 8.587957067489624 \n",
      "Epoch: 74   Validation Loss: 8.684302299499512 \n",
      "Epoch: 75   Training Loss: 8.59201548576355 \n",
      "Epoch: 75   Validation Loss: 8.631848573684692 \n",
      "Epoch: 76   Training Loss: 8.58958625459671 \n",
      "Epoch: 76   Validation Loss: 8.655568315505981 \n",
      "Epoch: 77   Training Loss: 8.593640565395356 \n",
      "Epoch: 77   Validation Loss: 8.633263853073121 \n",
      "Epoch: 78   Training Loss: 8.588708982944489 \n",
      "Epoch: 78   Validation Loss: 8.652500495910644 \n",
      "Epoch: 79   Training Loss: 8.585688838005066 \n",
      "Epoch: 79   Validation Loss: 8.682800640106201 \n",
      "Epoch: 80   Training Loss: 8.582793589115143 \n",
      "Epoch: 80   Validation Loss: 8.751058826446533 \n",
      "Epoch: 81   Training Loss: 8.587041307926178 \n",
      "Epoch: 81   Validation Loss: 8.75526845932007 \n",
      "Epoch: 82   Training Loss: 8.593226182460786 \n",
      "Epoch: 82   Validation Loss: 8.663509141921997 \n",
      "Epoch: 83   Training Loss: 8.589212454319 \n",
      "Epoch: 83   Validation Loss: 8.755954124450684 \n",
      "Epoch: 84   Training Loss: 8.586675187587739 \n",
      "Epoch: 84   Validation Loss: 8.646752349853516 \n",
      "Epoch: 85   Training Loss: 8.588369483470917 \n",
      "Epoch: 85   Validation Loss: 8.648702432632446 \n",
      "Epoch: 86   Training Loss: 8.588500938415528 \n",
      "Epoch: 86   Validation Loss: 8.722207529067994 \n",
      "Epoch: 87   Training Loss: 8.59128807592392 \n",
      "Epoch: 87   Validation Loss: 8.739865097045898 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88   Training Loss: 8.58947057723999 \n",
      "Epoch: 88   Validation Loss: 8.83425366783142 \n",
      "Epoch: 89   Training Loss: 8.593850605487823 \n",
      "Epoch: 89   Validation Loss: 8.69634292793274 \n",
      "Epoch: 90   Training Loss: 8.593288571357727 \n",
      "Epoch: 90   Validation Loss: 8.62561773300171 \n",
      "Epoch: 91   Training Loss: 8.59214875793457 \n",
      "Epoch: 91   Validation Loss: 8.646277307510376 \n",
      "Epoch: 92   Training Loss: 8.593090329170227 \n",
      "Epoch: 92   Validation Loss: 8.651688636779785 \n",
      "Epoch: 93   Training Loss: 8.591628393173218 \n",
      "Epoch: 93   Validation Loss: 8.655544601440429 \n",
      "Epoch: 94   Training Loss: 8.59320011138916 \n",
      "Epoch: 94   Validation Loss: 8.716644279479981 \n",
      "Epoch: 95   Training Loss: 8.589705968856812 \n",
      "Epoch: 95   Validation Loss: 8.696051061630248 \n",
      "Epoch: 96   Training Loss: 8.591252727031709 \n",
      "Epoch: 96   Validation Loss: 8.655776252746582 \n",
      "Epoch: 97   Training Loss: 8.592098700523376 \n",
      "Epoch: 97   Validation Loss: 8.718390188217164 \n",
      "Epoch: 98   Training Loss: 8.589513939857483 \n",
      "Epoch: 98   Validation Loss: 8.596707025527953 \n",
      "Epoch: 99   Training Loss: 8.58891058921814 \n",
      "Epoch: 99   Validation Loss: 8.590512832641602 \n",
      "Epoch: 100   Training Loss: 8.587414563179015 \n",
      "Epoch: 100   Validation Loss: 8.613533306121827 \n",
      "Epoch: 101   Training Loss: 8.589610836982727 \n",
      "Epoch: 101   Validation Loss: 8.558261686325073 \n",
      "Epoch: 102   Training Loss: 8.58961442232132 \n",
      "Epoch: 102   Validation Loss: 8.595499979019165 \n",
      "Epoch: 103   Training Loss: 8.587720572948456 \n",
      "Epoch: 103   Validation Loss: 8.612891786575318 \n",
      "Epoch: 104   Training Loss: 8.589114538669586 \n",
      "Epoch: 104   Validation Loss: 8.641370155334473 \n",
      "Epoch: 105   Training Loss: 8.58843602657318 \n",
      "Epoch: 105   Validation Loss: 8.736153734207154 \n",
      "Epoch: 106   Training Loss: 8.588360726356507 \n",
      "Epoch: 106   Validation Loss: 8.64028281211853 \n",
      "Epoch: 107   Training Loss: 8.592430742740632 \n",
      "Epoch: 107   Validation Loss: 8.64053748512268 \n",
      "Epoch: 108   Training Loss: 8.589708570480347 \n",
      "Epoch: 108   Validation Loss: 8.661539407730103 \n",
      "Epoch: 109   Training Loss: 8.5897663731575 \n",
      "Epoch: 109   Validation Loss: 8.761353387832642 \n",
      "Epoch: 110   Training Loss: 8.586569224357605 \n",
      "Epoch: 110   Validation Loss: 8.821932109832764 \n",
      "Epoch: 111   Training Loss: 8.590141983985902 \n",
      "Epoch: 111   Validation Loss: 8.784201192855836 \n",
      "Epoch: 112   Training Loss: 8.589401287555695 \n",
      "Epoch: 112   Validation Loss: 8.663265560150146 \n",
      "Epoch: 113   Training Loss: 8.590069291114807 \n",
      "Epoch: 113   Validation Loss: 8.602147365570069 \n",
      "Epoch: 114   Training Loss: 8.58815431880951 \n",
      "Epoch: 114   Validation Loss: 8.602354347229005 \n",
      "Epoch: 115   Training Loss: 8.58854179573059 \n",
      "Epoch: 115   Validation Loss: 8.592061904907226 \n",
      "Epoch: 116   Training Loss: 8.589007643222809 \n",
      "Epoch: 116   Validation Loss: 8.602941524505615 \n",
      "Epoch: 117   Training Loss: 8.59062705039978 \n",
      "Epoch: 117   Validation Loss: 8.644757106781006 \n",
      "Epoch: 118   Training Loss: 8.591896721363067 \n",
      "Epoch: 118   Validation Loss: 8.625666431427002 \n",
      "Epoch: 119   Training Loss: 8.590912936210632 \n",
      "Epoch: 119   Validation Loss: 8.652897785186768 \n",
      "Epoch: 120   Training Loss: 8.588007102489472 \n",
      "Epoch: 120   Validation Loss: 8.72138346862793 \n",
      "Epoch: 121   Training Loss: 8.589109721183776 \n",
      "Epoch: 121   Validation Loss: 8.80613032913208 \n",
      "Epoch: 122   Training Loss: 8.588462282657623 \n",
      "Epoch: 122   Validation Loss: 8.645222091674805 \n",
      "Epoch: 123   Training Loss: 8.590955126285554 \n",
      "Epoch: 123   Validation Loss: 8.622606103897095 \n",
      "Epoch: 124   Training Loss: 8.594740655899049 \n",
      "Epoch: 124   Validation Loss: 8.64689336013794 \n",
      "Epoch: 125   Training Loss: 8.589659379959107 \n",
      "Epoch: 125   Validation Loss: 8.677697778701782 \n",
      "Epoch: 126   Training Loss: 8.58717343378067 \n",
      "Epoch: 126   Validation Loss: 8.689510122299195 \n",
      "Epoch: 127   Training Loss: 8.587060115337373 \n",
      "Epoch: 127   Validation Loss: 8.704455644607544 \n",
      "Epoch: 128   Training Loss: 8.586310648441314 \n",
      "Epoch: 128   Validation Loss: 8.631950914382935 \n",
      "Epoch: 129   Training Loss: 8.589713383197784 \n",
      "Epoch: 129   Validation Loss: 8.566191179275513 \n",
      "Epoch: 130   Training Loss: 8.587025632381438 \n",
      "Epoch: 130   Validation Loss: 8.597018226623534 \n",
      "Epoch: 131   Training Loss: 8.586886915683746 \n",
      "Epoch: 131   Validation Loss: 8.601919958114625 \n",
      "Epoch: 132   Training Loss: 8.589231704235077 \n",
      "Epoch: 132   Validation Loss: 8.637287246704101 \n",
      "Epoch: 133   Training Loss: 8.591115890026092 \n",
      "Epoch: 133   Validation Loss: 8.723504322052001 \n",
      "Epoch: 134   Training Loss: 8.59094078540802 \n",
      "Epoch: 134   Validation Loss: 8.735762487411499 \n",
      "Epoch: 135   Training Loss: 8.590355338573456 \n",
      "Epoch: 135   Validation Loss: 8.700476850509643 \n",
      "Epoch: 136   Training Loss: 8.588263719081878 \n",
      "Epoch: 136   Validation Loss: 8.76241552734375 \n",
      "Epoch: 137   Training Loss: 8.589520644664765 \n",
      "Epoch: 137   Validation Loss: 8.798998008728027 \n",
      "Epoch: 138   Training Loss: 8.591054736614227 \n",
      "Epoch: 138   Validation Loss: 8.756469455718994 \n",
      "Epoch: 139   Training Loss: 8.588031432628632 \n",
      "Epoch: 139   Validation Loss: 8.661096651077271 \n",
      "Epoch: 140   Training Loss: 8.590356294631958 \n",
      "Epoch: 140   Validation Loss: 8.685619966506959 \n",
      "Epoch: 141   Training Loss: 8.58597501039505 \n",
      "Epoch: 141   Validation Loss: 8.656932968139648 \n",
      "Epoch: 142   Training Loss: 8.584278059482575 \n",
      "Epoch: 142   Validation Loss: 8.707742910385132 \n",
      "Epoch: 143   Training Loss: 8.586659257888794 \n",
      "Epoch: 143   Validation Loss: 8.803345653533935 \n",
      "Epoch: 144   Training Loss: 8.587712740421296 \n",
      "Epoch: 144   Validation Loss: 8.712912281036377 \n",
      "Epoch: 145   Training Loss: 8.588601181030274 \n",
      "Epoch: 145   Validation Loss: 8.699609128952027 \n",
      "Epoch: 146   Training Loss: 8.58827522277832 \n",
      "Epoch: 146   Validation Loss: 8.672535552978516 \n",
      "Epoch: 147   Training Loss: 8.588888673305512 \n",
      "Epoch: 147   Validation Loss: 8.647177482604981 \n",
      "Epoch: 148   Training Loss: 8.5862966299057 \n",
      "Epoch: 148   Validation Loss: 8.689682037353515 \n",
      "Epoch: 149   Training Loss: 8.587315994262696 \n",
      "Epoch: 149   Validation Loss: 8.715241453170776 \n",
      "Epoch: 150   Training Loss: 8.588654001235962 \n",
      "Epoch: 150   Validation Loss: 8.71470979309082 \n",
      "Epoch: 151   Training Loss: 8.591374883174897 \n",
      "Epoch: 151   Validation Loss: 8.64058030128479 \n",
      "Epoch: 152   Training Loss: 8.586600872516632 \n",
      "Epoch: 152   Validation Loss: 8.595599252700806 \n",
      "Epoch: 153   Training Loss: 8.587108729839326 \n",
      "Epoch: 153   Validation Loss: 8.645695278167725 \n",
      "Epoch: 154   Training Loss: 8.58758370256424 \n",
      "Epoch: 154   Validation Loss: 8.664728395462037 \n",
      "Epoch: 155   Training Loss: 8.584915985584258 \n",
      "Epoch: 155   Validation Loss: 8.694917430877686 \n",
      "Epoch: 156   Training Loss: 8.587842110157013 \n",
      "Epoch: 156   Validation Loss: 8.622751298904419 \n",
      "Epoch: 157   Training Loss: 8.584479610919953 \n",
      "Epoch: 157   Validation Loss: 8.660407327651978 \n",
      "Epoch: 158   Training Loss: 8.585120347023011 \n",
      "Epoch: 158   Validation Loss: 8.63810348701477 \n",
      "Epoch: 159   Training Loss: 8.585025152206422 \n",
      "Epoch: 159   Validation Loss: 8.656100118637085 \n",
      "Epoch: 160   Training Loss: 8.587388135433198 \n",
      "Epoch: 160   Validation Loss: 8.680853248596192 \n",
      "Epoch: 161   Training Loss: 8.588665458679198 \n",
      "Epoch: 161   Validation Loss: 8.660069395065308 \n",
      "Epoch: 162   Training Loss: 8.586244698524474 \n",
      "Epoch: 162   Validation Loss: 8.63444019126892 \n",
      "Epoch: 163   Training Loss: 8.589026524066925 \n",
      "Epoch: 163   Validation Loss: 8.626785688400268 \n",
      "Epoch: 164   Training Loss: 8.590165739536285 \n",
      "Epoch: 164   Validation Loss: 8.765589210510253 \n",
      "Epoch: 165   Training Loss: 8.591882872104645 \n",
      "Epoch: 165   Validation Loss: 8.772675107955932 \n",
      "Epoch: 166   Training Loss: 8.588232975959778 \n",
      "Epoch: 166   Validation Loss: 8.654631105422974 \n",
      "Epoch: 167   Training Loss: 8.587201664924622 \n",
      "Epoch: 167   Validation Loss: 8.701790256500244 \n",
      "Epoch: 168   Training Loss: 8.591689844608307 \n",
      "Epoch: 168   Validation Loss: 8.680418869018554 \n",
      "Epoch: 169   Training Loss: 8.585785314559937 \n",
      "Epoch: 169   Validation Loss: 8.626207592010498 \n",
      "Epoch: 170   Training Loss: 8.586421506881713 \n",
      "Epoch: 170   Validation Loss: 8.577715175628661 \n",
      "Epoch: 171   Training Loss: 8.585653903007508 \n",
      "Epoch: 171   Validation Loss: 8.633159772872926 \n",
      "Epoch: 172   Training Loss: 8.58751458454132 \n",
      "Epoch: 172   Validation Loss: 8.69671908569336 \n",
      "Epoch: 173   Training Loss: 8.586234693527222 \n",
      "Epoch: 173   Validation Loss: 8.759330451965331 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 174   Training Loss: 8.58823017168045 \n",
      "Epoch: 174   Validation Loss: 8.747939893722535 \n",
      "Epoch: 175   Training Loss: 8.590489282131195 \n",
      "Epoch: 175   Validation Loss: 8.75098991394043 \n",
      "Epoch: 176   Training Loss: 8.590502599716187 \n",
      "Epoch: 176   Validation Loss: 8.66930061531067 \n",
      "Epoch: 177   Training Loss: 8.588084573745727 \n",
      "Epoch: 177   Validation Loss: 8.653852022171021 \n",
      "Epoch: 178   Training Loss: 8.586275534629822 \n",
      "Epoch: 178   Validation Loss: 8.669071996688842 \n",
      "Epoch: 179   Training Loss: 8.58651868057251 \n",
      "Epoch: 179   Validation Loss: 8.719119916915893 \n",
      "Epoch: 180   Training Loss: 8.584412204742431 \n",
      "Epoch: 180   Validation Loss: 8.672446311950683 \n",
      "Epoch: 181   Training Loss: 8.581327202320098 \n",
      "Epoch: 181   Validation Loss: 8.651077392578125 \n",
      "Epoch: 182   Training Loss: 8.586694644927979 \n",
      "Epoch: 182   Validation Loss: 8.661489274978639 \n",
      "Epoch: 183   Training Loss: 8.58706369638443 \n",
      "Epoch: 183   Validation Loss: 8.737984273910522 \n",
      "Epoch: 184   Training Loss: 8.587377392292023 \n",
      "Epoch: 184   Validation Loss: 8.771436820983887 \n",
      "Epoch: 185   Training Loss: 8.588059484004974 \n",
      "Epoch: 185   Validation Loss: 8.794348384857178 \n",
      "Epoch: 186   Training Loss: 8.585579005241394 \n",
      "Epoch: 186   Validation Loss: 8.70386940574646 \n",
      "Epoch: 187   Training Loss: 8.584421650409698 \n",
      "Epoch: 187   Validation Loss: 8.646516580581665 \n",
      "Epoch: 188   Training Loss: 8.585138979911804 \n",
      "Epoch: 188   Validation Loss: 8.592223083496094 \n",
      "Epoch: 189   Training Loss: 8.584392155647278 \n",
      "Epoch: 189   Validation Loss: 8.645941507339478 \n",
      "Epoch: 190   Training Loss: 8.586922708511352 \n",
      "Epoch: 190   Validation Loss: 8.71297096824646 \n",
      "Epoch: 191   Training Loss: 8.58822866821289 \n",
      "Epoch: 191   Validation Loss: 8.652850080490113 \n",
      "Epoch: 192   Training Loss: 8.586344499111176 \n",
      "Epoch: 192   Validation Loss: 8.65618950843811 \n",
      "Epoch: 193   Training Loss: 8.586192533493042 \n",
      "Epoch: 193   Validation Loss: 8.725306959152222 \n",
      "Epoch: 194   Training Loss: 8.587078546047211 \n",
      "Epoch: 194   Validation Loss: 8.814507026672363 \n",
      "Epoch: 195   Training Loss: 8.587543340206146 \n",
      "Epoch: 195   Validation Loss: 8.707151563644409 \n",
      "Epoch: 196   Training Loss: 8.587302304744721 \n",
      "Epoch: 196   Validation Loss: 8.668964073181153 \n",
      "Epoch: 197   Training Loss: 8.587098237991333 \n",
      "Epoch: 197   Validation Loss: 8.709903057098389 \n",
      "Epoch: 198   Training Loss: 8.585589728832245 \n",
      "Epoch: 198   Validation Loss: 8.665842445373535 \n",
      "Epoch: 199   Training Loss: 8.583544188022614 \n",
      "Epoch: 199   Validation Loss: 8.615678104400635 \n",
      "Epoch: 200   Training Loss: 8.58644794511795 \n",
      "Epoch: 200   Validation Loss: 8.617821098327637 \n",
      "Epoch: 201   Training Loss: 8.589053592681886 \n",
      "Epoch: 201   Validation Loss: 8.654483697891235 \n",
      "Epoch: 202   Training Loss: 8.590334212303162 \n",
      "Epoch: 202   Validation Loss: 8.776194545745849 \n",
      "Epoch: 203   Training Loss: 8.584992025375366 \n",
      "Epoch: 203   Validation Loss: 8.774362462997436 \n",
      "Epoch: 204   Training Loss: 8.585411704063416 \n",
      "Epoch: 204   Validation Loss: 8.6993081073761 \n",
      "Epoch: 205   Training Loss: 8.587129179000854 \n",
      "Epoch: 205   Validation Loss: 8.681426542282104 \n",
      "Epoch: 206   Training Loss: 8.588954354286194 \n",
      "Epoch: 206   Validation Loss: 8.697550031661986 \n",
      "Epoch: 207   Training Loss: 8.586640354633332 \n",
      "Epoch: 207   Validation Loss: 8.667204635620116 \n",
      "Epoch: 208   Training Loss: 8.588256714344025 \n",
      "Epoch: 208   Validation Loss: 8.623286256790161 \n",
      "Epoch: 209   Training Loss: 8.590151651382445 \n",
      "Epoch: 209   Validation Loss: 8.60796732711792 \n",
      "Epoch: 210   Training Loss: 8.590039869308471 \n",
      "Epoch: 210   Validation Loss: 8.59229545211792 \n",
      "Epoch: 211   Training Loss: 8.589193533420563 \n",
      "Epoch: 211   Validation Loss: 8.564983116149902 \n",
      "Epoch: 212   Training Loss: 8.588687201499939 \n",
      "Epoch: 212   Validation Loss: 8.583239725112914 \n",
      "Epoch: 213   Training Loss: 8.587960091590881 \n",
      "Epoch: 213   Validation Loss: 8.699124486923218 \n",
      "Epoch: 214   Training Loss: 8.587748658180237 \n",
      "Epoch: 214   Validation Loss: 8.72860655593872 \n",
      "Epoch: 215   Training Loss: 8.588519670009614 \n",
      "Epoch: 215   Validation Loss: 8.743683305740356 \n",
      "Epoch: 216   Training Loss: 8.589341634750367 \n",
      "Epoch: 216   Validation Loss: 8.66909077644348 \n",
      "Epoch: 217   Training Loss: 8.587219960689545 \n",
      "Epoch: 217   Validation Loss: 8.66141215133667 \n",
      "Epoch: 218   Training Loss: 8.589000076293946 \n",
      "Epoch: 218   Validation Loss: 8.57837900352478 \n",
      "Epoch: 219   Training Loss: 8.584670545101165 \n",
      "Epoch: 219   Validation Loss: 8.583603101730347 \n",
      "Epoch: 220   Training Loss: 8.588893332481383 \n",
      "Epoch: 220   Validation Loss: 8.62305584335327 \n",
      "Epoch: 221   Training Loss: 8.58968959903717 \n",
      "Epoch: 221   Validation Loss: 8.646212720870972 \n",
      "Epoch: 222   Training Loss: 8.58979403591156 \n",
      "Epoch: 222   Validation Loss: 8.590077716827393 \n",
      "Epoch: 223   Training Loss: 8.587951451301574 \n",
      "Epoch: 223   Validation Loss: 8.595803436279297 \n",
      "Epoch: 224   Training Loss: 8.587295524597168 \n",
      "Epoch: 224   Validation Loss: 8.631340309143066 \n",
      "Epoch: 225   Training Loss: 8.588454205989837 \n",
      "Epoch: 225   Validation Loss: 8.683556213378907 \n",
      "Epoch: 226   Training Loss: 8.590268172740936 \n",
      "Epoch: 226   Validation Loss: 8.706349950790406 \n",
      "Epoch: 227   Training Loss: 8.585204553604125 \n",
      "Epoch: 227   Validation Loss: 8.71033664894104 \n",
      "Epoch: 228   Training Loss: 8.584489171028137 \n",
      "Epoch: 228   Validation Loss: 8.711101839065552 \n",
      "Epoch: 229   Training Loss: 8.580835176944733 \n",
      "Epoch: 229   Validation Loss: 8.751987073898315 \n",
      "Epoch: 230   Training Loss: 8.58331626367569 \n",
      "Epoch: 230   Validation Loss: 8.72813000869751 \n",
      "Epoch: 231   Training Loss: 8.582852092266084 \n",
      "Epoch: 231   Validation Loss: 8.638169340133667 \n",
      "Epoch: 232   Training Loss: 8.583364660263062 \n",
      "Epoch: 232   Validation Loss: 8.639677585601806 \n",
      "Epoch: 233   Training Loss: 8.585835652828216 \n",
      "Epoch: 233   Validation Loss: 8.602367406845092 \n",
      "Epoch: 234   Training Loss: 8.58597653055191 \n",
      "Epoch: 234   Validation Loss: 8.627936340332031 \n",
      "Epoch: 235   Training Loss: 8.58636372089386 \n",
      "Epoch: 235   Validation Loss: 8.650148056030273 \n",
      "Epoch: 236   Training Loss: 8.585271903514862 \n",
      "Epoch: 236   Validation Loss: 8.700611896514893 \n",
      "Epoch: 237   Training Loss: 8.586808362960815 \n",
      "Epoch: 237   Validation Loss: 8.693781261444093 \n",
      "Epoch: 238   Training Loss: 8.588735443592071 \n",
      "Epoch: 238   Validation Loss: 8.677255460739136 \n",
      "Epoch: 239   Training Loss: 8.588842853546142 \n",
      "Epoch: 239   Validation Loss: 8.646993551254273 \n",
      "Epoch: 240   Training Loss: 8.587777178287507 \n",
      "Epoch: 240   Validation Loss: 8.624726642608643 \n",
      "Epoch: 241   Training Loss: 8.592431555747986 \n",
      "Epoch: 241   Validation Loss: 8.609761156082154 \n",
      "Epoch: 242   Training Loss: 8.588128601074219 \n",
      "Epoch: 242   Validation Loss: 8.601084901809692 \n",
      "Epoch: 243   Training Loss: 8.586214148044586 \n",
      "Epoch: 243   Validation Loss: 8.601899684906005 \n",
      "Epoch: 244   Training Loss: 8.586689509391785 \n",
      "Epoch: 244   Validation Loss: 8.638346105575561 \n",
      "Epoch: 245   Training Loss: 8.58443863773346 \n",
      "Epoch: 245   Validation Loss: 8.747252487182617 \n",
      "Epoch: 246   Training Loss: 8.582663106441498 \n",
      "Epoch: 246   Validation Loss: 8.78579754638672 \n",
      "Epoch: 247   Training Loss: 8.583512630939484 \n",
      "Epoch: 247   Validation Loss: 8.74470252609253 \n",
      "Epoch: 248   Training Loss: 8.5873978433609 \n",
      "Epoch: 248   Validation Loss: 8.663693294525146 \n",
      "Epoch: 249   Training Loss: 8.587914863586425 \n",
      "Epoch: 249   Validation Loss: 8.66318677520752 \n",
      "Epoch: 250   Training Loss: 8.589062996387481 \n",
      "Epoch: 250   Validation Loss: 8.656049785614014 \n",
      "Epoch: 251   Training Loss: 8.587998999118804 \n",
      "Epoch: 251   Validation Loss: 8.644390186309815 \n",
      "Epoch: 252   Training Loss: 8.587571184158325 \n",
      "Epoch: 252   Validation Loss: 8.71321157836914 \n",
      "Epoch: 253   Training Loss: 8.586728984355927 \n",
      "Epoch: 253   Validation Loss: 8.6496353931427 \n",
      "Epoch: 254   Training Loss: 8.587708146572114 \n",
      "Epoch: 254   Validation Loss: 8.624999479293823 \n",
      "Epoch: 255   Training Loss: 8.582984022140502 \n",
      "Epoch: 255   Validation Loss: 8.715244981765746 \n",
      "Epoch: 256   Training Loss: 8.585951902389526 \n",
      "Epoch: 256   Validation Loss: 8.664198347091675 \n",
      "Epoch: 257   Training Loss: 8.586627293109894 \n",
      "Epoch: 257   Validation Loss: 8.587288370132447 \n",
      "Epoch: 258   Training Loss: 8.587107359409332 \n",
      "Epoch: 258   Validation Loss: 8.610567405700683 \n",
      "Epoch: 259   Training Loss: 8.586263880729675 \n",
      "Epoch: 259   Validation Loss: 8.596638792037965 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 260   Training Loss: 8.580712954998017 \n",
      "Epoch: 260   Validation Loss: 8.703208503723145 \n",
      "Epoch: 261   Training Loss: 8.585499019145965 \n",
      "Epoch: 261   Validation Loss: 8.701355556488037 \n",
      "Epoch: 262   Training Loss: 8.585973012447358 \n",
      "Epoch: 262   Validation Loss: 8.628688653945924 \n",
      "Epoch: 263   Training Loss: 8.58925435590744 \n",
      "Epoch: 263   Validation Loss: 8.619425382614136 \n",
      "Epoch: 264   Training Loss: 8.589356283187866 \n",
      "Epoch: 264   Validation Loss: 8.655866556167602 \n",
      "Epoch: 265   Training Loss: 8.590764015197754 \n",
      "Epoch: 265   Validation Loss: 8.572189121246337 \n",
      "Epoch: 266   Training Loss: 8.586740717887878 \n",
      "Epoch: 266   Validation Loss: 8.637754669189453 \n",
      "Epoch: 267   Training Loss: 8.588712004184723 \n",
      "Epoch: 267   Validation Loss: 8.674295984268188 \n",
      "Epoch: 268   Training Loss: 8.590760768413544 \n",
      "Epoch: 268   Validation Loss: 8.64420712852478 \n",
      "Epoch: 269   Training Loss: 8.587794331550597 \n",
      "Epoch: 269   Validation Loss: 8.743238996505736 \n",
      "Epoch: 270   Training Loss: 8.589768385887146 \n",
      "Epoch: 270   Validation Loss: 8.65565224647522 \n",
      "Epoch: 271   Training Loss: 8.590353362560272 \n",
      "Epoch: 271   Validation Loss: 8.604169343948364 \n",
      "Epoch: 272   Training Loss: 8.589734104156495 \n",
      "Epoch: 272   Validation Loss: 8.591149677276611 \n",
      "Epoch: 273   Training Loss: 8.587884828090667 \n",
      "Epoch: 273   Validation Loss: 8.61466046333313 \n",
      "Epoch: 274   Training Loss: 8.588413826942444 \n",
      "Epoch: 274   Validation Loss: 8.608375520706177 \n",
      "Epoch: 275   Training Loss: 8.589294307708741 \n",
      "Epoch: 275   Validation Loss: 8.592890657424928 \n",
      "Epoch: 276   Training Loss: 8.5867985663414 \n",
      "Epoch: 276   Validation Loss: 8.64469987487793 \n",
      "Epoch: 277   Training Loss: 8.587364437103272 \n",
      "Epoch: 277   Validation Loss: 8.696376104354858 \n",
      "Epoch: 278   Training Loss: 8.59037752866745 \n",
      "Epoch: 278   Validation Loss: 8.764334756851197 \n",
      "Epoch: 279   Training Loss: 8.58791105890274 \n",
      "Epoch: 279   Validation Loss: 8.674704086303711 \n",
      "Epoch: 280   Training Loss: 8.589967519283295 \n",
      "Epoch: 280   Validation Loss: 8.604645574569702 \n",
      "Epoch: 281   Training Loss: 8.5861341547966 \n",
      "Epoch: 281   Validation Loss: 8.552227514266967 \n",
      "Epoch: 282   Training Loss: 8.582947844505311 \n",
      "Epoch: 282   Validation Loss: 8.567928691864013 \n",
      "Epoch: 283   Training Loss: 8.582027057647705 \n",
      "Epoch: 283   Validation Loss: 8.603963409423828 \n",
      "Epoch: 284   Training Loss: 8.587325426101685 \n",
      "Epoch: 284   Validation Loss: 8.605772151947022 \n",
      "Epoch: 285   Training Loss: 8.58567369222641 \n",
      "Epoch: 285   Validation Loss: 8.633849857330322 \n",
      "Epoch: 286   Training Loss: 8.583601364135742 \n",
      "Epoch: 286   Validation Loss: 8.683517330169678 \n",
      "Epoch: 287   Training Loss: 8.581628806591034 \n",
      "Epoch: 287   Validation Loss: 8.719359058380126 \n",
      "Epoch: 288   Training Loss: 8.58538195514679 \n",
      "Epoch: 288   Validation Loss: 8.724557502746581 \n",
      "Epoch: 289   Training Loss: 8.586206467628479 \n",
      "Epoch: 289   Validation Loss: 8.70914510345459 \n",
      "Epoch: 290   Training Loss: 8.589993622303009 \n",
      "Epoch: 290   Validation Loss: 8.66307763671875 \n",
      "Epoch: 291   Training Loss: 8.58584927892685 \n",
      "Epoch: 291   Validation Loss: 8.596501762390137 \n",
      "Epoch: 292   Training Loss: 8.585519990921021 \n",
      "Epoch: 292   Validation Loss: 8.590278356552124 \n",
      "Epoch: 293   Training Loss: 8.586290235042572 \n",
      "Epoch: 293   Validation Loss: 8.635518869400025 \n",
      "Epoch: 294   Training Loss: 8.586695969581603 \n",
      "Epoch: 294   Validation Loss: 8.717558032989501 \n",
      "Epoch: 295   Training Loss: 8.583844381809234 \n",
      "Epoch: 295   Validation Loss: 8.770444913864136 \n",
      "Epoch: 296   Training Loss: 8.585932395458222 \n",
      "Epoch: 296   Validation Loss: 8.786113019943237 \n",
      "Epoch: 297   Training Loss: 8.589500593185425 \n",
      "Epoch: 297   Validation Loss: 8.662441186904907 \n",
      "Epoch: 298   Training Loss: 8.589628561496735 \n",
      "Epoch: 298   Validation Loss: 8.607411752700806 \n",
      "Epoch: 299   Training Loss: 8.583530294418335 \n",
      "Epoch: 299   Validation Loss: 8.647066335678101 \n",
      "Epoch: 300   Training Loss: 8.583893134593964 \n",
      "Epoch: 300   Validation Loss: 8.650788232803345 \n",
      "Epoch: 301   Training Loss: 8.582863144397736 \n",
      "Epoch: 301   Validation Loss: 8.694419416427612 \n",
      "Epoch: 302   Training Loss: 8.584555103302002 \n",
      "Epoch: 302   Validation Loss: 8.739622253417968 \n",
      "Epoch: 303   Training Loss: 8.587347970962524 \n",
      "Epoch: 303   Validation Loss: 8.725237003326416 \n",
      "Epoch: 304   Training Loss: 8.591122751235963 \n",
      "Epoch: 304   Validation Loss: 8.663866250991822 \n",
      "Epoch: 305   Training Loss: 8.591659452438355 \n",
      "Epoch: 305   Validation Loss: 8.621424253463745 \n",
      "Epoch: 306   Training Loss: 8.590776882171632 \n",
      "Epoch: 306   Validation Loss: 8.583266744613647 \n",
      "Epoch: 307   Training Loss: 8.586963465690612 \n",
      "Epoch: 307   Validation Loss: 8.649415306091308 \n",
      "Epoch: 308   Training Loss: 8.583949110507966 \n",
      "Epoch: 308   Validation Loss: 8.711357158660888 \n",
      "Epoch: 309   Training Loss: 8.585052716255188 \n",
      "Epoch: 309   Validation Loss: 8.70664990234375 \n",
      "Epoch: 310   Training Loss: 8.587259840011596 \n",
      "Epoch: 310   Validation Loss: 8.704203824996949 \n",
      "Epoch: 311   Training Loss: 8.586063914775849 \n",
      "Epoch: 311   Validation Loss: 8.585934865951538 \n",
      "Epoch: 312   Training Loss: 8.584955626010895 \n",
      "Epoch: 312   Validation Loss: 8.635337541580201 \n",
      "Epoch: 313   Training Loss: 8.582760561943054 \n",
      "Epoch: 313   Validation Loss: 8.678537771224976 \n",
      "Epoch: 314   Training Loss: 8.582018321514129 \n",
      "Epoch: 314   Validation Loss: 8.771578664779662 \n",
      "Epoch: 315   Training Loss: 8.582657422065735 \n",
      "Epoch: 315   Validation Loss: 8.6658030128479 \n",
      "Epoch: 316   Training Loss: 8.584033788204193 \n",
      "Epoch: 316   Validation Loss: 8.687065656661987 \n",
      "Epoch: 317   Training Loss: 8.586068447113037 \n",
      "Epoch: 317   Validation Loss: 8.694892385482788 \n",
      "Epoch: 318   Training Loss: 8.581720012187958 \n",
      "Epoch: 318   Validation Loss: 8.661156276702881 \n",
      "Epoch: 319   Training Loss: 8.582118449687957 \n",
      "Epoch: 319   Validation Loss: 8.664156789779662 \n",
      "Epoch: 320   Training Loss: 8.585841484069825 \n",
      "Epoch: 320   Validation Loss: 8.64176191520691 \n",
      "Epoch: 321   Training Loss: 8.587912816047668 \n",
      "Epoch: 321   Validation Loss: 8.6331350440979 \n",
      "Epoch: 322   Training Loss: 8.584198009014129 \n",
      "Epoch: 322   Validation Loss: 8.67922554397583 \n",
      "Epoch: 323   Training Loss: 8.583441306114198 \n",
      "Epoch: 323   Validation Loss: 8.662348886489868 \n",
      "Epoch: 324   Training Loss: 8.586354207038879 \n",
      "Epoch: 324   Validation Loss: 8.583404039382934 \n",
      "Epoch: 325   Training Loss: 8.583832327365876 \n",
      "Epoch: 325   Validation Loss: 8.591994590759278 \n",
      "Epoch: 326   Training Loss: 8.583174937725067 \n",
      "Epoch: 326   Validation Loss: 8.65026432800293 \n",
      "Epoch: 327   Training Loss: 8.587451137065887 \n",
      "Epoch: 327   Validation Loss: 8.767005424499512 \n",
      "Epoch: 328   Training Loss: 8.586905165195464 \n",
      "Epoch: 328   Validation Loss: 8.767156089782715 \n",
      "Epoch: 329   Training Loss: 8.588965548992157 \n",
      "Epoch: 329   Validation Loss: 8.780269430160523 \n",
      "Epoch: 330   Training Loss: 8.590532327651978 \n",
      "Epoch: 330   Validation Loss: 8.745832593917847 \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m test_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainset, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     inputs, truths \u001b[38;5;241m=\u001b[39m norm(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(data[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)), torch\u001b[38;5;241m.\u001b[39mfrom_numpy(data[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     35\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m net(inputs)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Create writer and profiler to analyze loss over each epoch\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Set device to CUDA if available, initialize model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "net = CNN_3D_aligner()\n",
    "net.to(device)\n",
    "\n",
    "# Set up optimizer and loss function, set number of epochs\n",
    "optimizer = optim.SGD(net.parameters(), lr = 2e-2, momentum=0.9, weight_decay = 0)\n",
    "criterion = nn.MSELoss(reduction = 'mean')\n",
    "criterion.to(device)\n",
    "num_epochs = 1000\n",
    "\n",
    "# Iniitializing variables to show statistics\n",
    "iteration = 0\n",
    "test_iteration = 0\n",
    "loss_list = []\n",
    "test_loss_list = []\n",
    "epoch_loss_averages = []\n",
    "test_epoch_loss_averages = []\n",
    "\n",
    "# Iterates over dataset multiple times\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    test_epoch_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(trainset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(data[0]).to(device)), torch.from_numpy(data[1]).to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs).to(device)\n",
    "        loss = criterion(outputs, truths)\n",
    "        writer.add_scalar(\"Loss / Train\", loss, epoch) # adds training loss scalar\n",
    "        loss_list.append(loss.cpu().detach().numpy())\n",
    "        epoch_loss += loss.cpu().detach().numpy()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration += 1\n",
    "        if iteration % trainset.shape[0] == 0:\n",
    "            epoch_loss_averages.append(epoch_loss / trainset.shape[0])\n",
    "            print('Epoch: {}   Training Loss: {} '.format(epoch, epoch_loss / trainset.shape[0]))\n",
    "            \n",
    "    for i, test_data in enumerate(testset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(test_data[0]).to(device)), torch.from_numpy(test_data[1]).to(device).float()\n",
    "        outputs = net(inputs).to(device)\n",
    "        test_loss = criterion(outputs, truths)\n",
    "        \n",
    "        writer.add_scalar(\"Loss / Test\", test_loss, epoch) # adds testing loss scalar\n",
    "        test_loss_list.append(test_loss.cpu().detach().numpy())\n",
    "        test_epoch_loss += test_loss.cpu().detach().numpy()\n",
    "        \n",
    "        test_iteration +=1\n",
    "        if test_iteration % testset.shape[0] == 0:\n",
    "            test_epoch_loss_averages.append(test_epoch_loss / testset.shape[0])\n",
    "            print('Epoch: {}   Validation Loss: {} '.format(epoch, test_epoch_loss / testset.shape[0]))\n",
    "            \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9d536",
   "metadata": {},
   "source": [
    "Now in order to observe convergence or lack thereof a graph of loss per epoch are created for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec9c6ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot epoch loss to test for convergence\n",
    "plt.plot(epoch_loss_averages)\n",
    "plt.plot(test_epoch_loss_averages)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253f989",
   "metadata": {},
   "source": [
    "Through this step of the process, it shows that convergence is possible for training data when the amount of entries is small. However, this comes with the caveat of the testing data not converging and multiple techniques to solve overfitting were found to not be satisfactory. Then the size of the training data was increased but this resulted in loss of convergence altogether, suggesting that the existing neural net is not sufficient to solve this problem. Now that a more traditional structure has been ruled out of the questions, the next logical step is to use another network design used for image alignment with larger data size: ResNet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
