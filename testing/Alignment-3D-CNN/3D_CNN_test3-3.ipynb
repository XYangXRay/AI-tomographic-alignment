{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b87aa476",
   "metadata": {},
   "source": [
    "# 3D Convolutional Neural Network for Tomographic Alignment\n",
    "\n",
    "## Expanding the Network: ResNet and Engineering the Data\n",
    "\n",
    "Since based on previous analysis it is likely that the network was too small to properly identify the features necessary for alignment, the next logical step is to expand the network. Making deeper neural networks usually results in diminishing returns, but residual neural networks have proven to successfully allow for deeper neural networks. Now this model structure will be used in order to find some form of convergence.\n",
    "\n",
    "While it seems like the ResNet will increase the performance of the model on larger training sets, testing convergence is still a major problem. Now the input data will be modified to find the difference between each projection and the mean \"projection\", which should help the network recognize difference between projections more effectively. This will hopefully allow for better understanding of the specific alignment problem by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d27f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential packages\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import tomography and imaging packages\n",
    "import tomopy\n",
    "from skimage.transform import rotate, AffineTransform\n",
    "from skimage import transform as tf\n",
    "\n",
    "# Import neural net packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.profiler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5db9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Environment: pytorch\n",
      "Cuda Version: 11.8\n",
      "Cuda Availability: True\n",
      "/home/liam/Projects/Tomographic Alignment\r\n"
     ]
    }
   ],
   "source": [
    "# Checking to ensure environment and cuda are correct\n",
    "print(\"Working Environment: {}\".format(os.environ['CONDA_DEFAULT_ENV']))\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"Cuda Version: {}\".format(torch.version.cuda))\n",
    "print(\"Cuda Availability: {}\".format(torch.cuda.is_available()))\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1876ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting function that can be used for modifying the data\n",
    "def data_mean_difference(data):\n",
    "    \n",
    "    # Create copy of dataset for projections to be modified, also initializes array for mean projections\n",
    "    projections = data[:, 0].copy()\n",
    "    mean_projections = np.zeros((projections.shape[0]), dtype = object)\n",
    "    \n",
    "    # Iterate through each projection stack\n",
    "    for i in range (projections.shape[0]):\n",
    "\n",
    "        # Get rid of extra dimensions for neural networks and create a mean projection\n",
    "        projections[i] = np.squeeze(projections[i])\n",
    "        mean_projections[i] = np.mean(projections[i], axis = 0)\n",
    "\n",
    "        # Iterate through every projection in stack\n",
    "        for j in range (projections[0].shape[0]):\n",
    "\n",
    "            # Change current projection to difference between current projection and mean projection\n",
    "            projections[i][j] = projections[i][j] - mean_projections[i]\n",
    "            \n",
    "        # Expand dimensions to original form for neural networks\n",
    "        projections[i] = np.expand_dims(projections[i], axis = 0)\n",
    "        projections[i] = np.expand_dims(projections[i], axis = 0)\n",
    "\n",
    "    # Create data difference array\n",
    "    data_diff = data.copy()\n",
    "\n",
    "    # Replace all data with the new difference projections\n",
    "    for i in range (data.shape[0]):\n",
    "\n",
    "        data_diff[i][0] = projections[i]\n",
    "        \n",
    "    return data_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df2f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data, 25 entries of 128 resolution shepp3ds\n",
    "res = 128\n",
    "entries = 250\n",
    "data = []\n",
    "\n",
    "for i in range(entries):\n",
    "    data.append(np.load('./shepp{}-{}/shepp{}-{}_{}.npy'.format(res, entries, res, entries, i), \n",
    "                        allow_pickle = True))\n",
    "    \n",
    "data = np.asarray(data)\n",
    "\n",
    "data = data_mean_difference(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c8c622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Dataset: (200, 2)\n",
      "Shape of Testing Dataset: (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of training and testing splits\n",
    "trainset, testset = np.split(data, [int(entries * 4 / 5)])\n",
    "print(\"Shape of Training Dataset: {}\".format(trainset.shape))\n",
    "print(\"Shape of Testing Dataset: {}\".format(testset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb3e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def norm(proj):\n",
    "    proj = (proj - torch.min(proj)) / (torch.max(proj) - torch.min(proj))\n",
    "    return proj\n",
    "\n",
    "# Get inplanes for resnet\n",
    "def get_inplanes():\n",
    "    return [64, 128, 256, 512]\n",
    "\n",
    "\n",
    "# Preset for a 3x3x3 kernel convolution\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "# Preset for a 1x1x1 kernel convolution\n",
    "def conv1x1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=1,\n",
    "                     stride=stride,\n",
    "                     bias=False)\n",
    "\n",
    "# Basic block for resnet\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv3x3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# Bottleneck block for resnet\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv1x1x1(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = conv3x3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Resnet structure\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 block_inplanes,\n",
    "                 n_input_channels=1,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 shortcut_type='B',\n",
    "                 widen_factor=1.0,\n",
    "                 n_classes=360):\n",
    "        super().__init__()\n",
    "\n",
    "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
    "\n",
    "        self.in_planes = block_inplanes[0]\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        self.conv1 = nn.Conv3d(n_input_channels,\n",
    "                               self.in_planes,\n",
    "                               kernel_size=(conv1_t_size, 7, 7),\n",
    "                               stride=(conv1_t_stride, 2, 2),\n",
    "                               padding=(conv1_t_size // 2, 3, 3),\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n",
    "                                       shortcut_type)\n",
    "        self.layer2 = self._make_layer(block,\n",
    "                                       block_inplanes[1],\n",
    "                                       layers[1],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer3 = self._make_layer(block,\n",
    "                                       block_inplanes[2],\n",
    "                                       layers[2],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer4 = self._make_layer(block,\n",
    "                                       block_inplanes[3],\n",
    "                                       layers[3],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _downsample_basic_block(self, x, planes, stride):\n",
    "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n",
    "                                out.size(3), out.size(4))\n",
    "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "            zero_pads = zero_pads.cuda()\n",
    "\n",
    "        out = torch.cat([out.data, zero_pads], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # make layer helper function\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            \n",
    "                downsample = nn.Sequential(\n",
    "                    conv1x1x1(self.in_planes, planes * block.expansion, stride),\n",
    "                    nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(in_planes=self.in_planes,\n",
    "                  planes=planes,\n",
    "                  stride=stride,\n",
    "                  downsample=downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Generates form of resnet\n",
    "def generate_model(model_depth, **kwargs):\n",
    "    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
    "\n",
    "    if model_depth == 10:\n",
    "        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 18:\n",
    "        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 34:\n",
    "        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 50:\n",
    "        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 101:\n",
    "        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 152:\n",
    "        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 200:\n",
    "        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9f8c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 360]                  --\n",
       "├─Conv3d: 1-1                            [1, 64, 180, 64, 92]      21,952\n",
       "├─BatchNorm3d: 1-2                       [1, 64, 180, 64, 92]      128\n",
       "├─ReLU: 1-3                              [1, 64, 180, 64, 92]      --\n",
       "├─MaxPool3d: 1-4                         [1, 64, 90, 32, 46]       --\n",
       "├─Sequential: 1-5                        [1, 256, 90, 32, 46]      --\n",
       "│    └─Bottleneck: 2-1                   [1, 256, 90, 32, 46]      --\n",
       "│    │    └─Conv3d: 3-1                  [1, 64, 90, 32, 46]       4,096\n",
       "│    │    └─BatchNorm3d: 3-2             [1, 64, 90, 32, 46]       128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 90, 32, 46]       --\n",
       "│    │    └─Conv3d: 3-4                  [1, 64, 90, 32, 46]       110,592\n",
       "│    │    └─BatchNorm3d: 3-5             [1, 64, 90, 32, 46]       128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 90, 32, 46]       --\n",
       "│    │    └─Conv3d: 3-7                  [1, 256, 90, 32, 46]      16,384\n",
       "│    │    └─BatchNorm3d: 3-8             [1, 256, 90, 32, 46]      512\n",
       "│    │    └─Sequential: 3-9              [1, 256, 90, 32, 46]      16,896\n",
       "│    │    └─ReLU: 3-10                   [1, 256, 90, 32, 46]      --\n",
       "│    └─Bottleneck: 2-2                   [1, 256, 90, 32, 46]      --\n",
       "│    │    └─Conv3d: 3-11                 [1, 64, 90, 32, 46]       16,384\n",
       "│    │    └─BatchNorm3d: 3-12            [1, 64, 90, 32, 46]       128\n",
       "│    │    └─ReLU: 3-13                   [1, 64, 90, 32, 46]       --\n",
       "│    │    └─Conv3d: 3-14                 [1, 64, 90, 32, 46]       110,592\n",
       "│    │    └─BatchNorm3d: 3-15            [1, 64, 90, 32, 46]       128\n",
       "│    │    └─ReLU: 3-16                   [1, 64, 90, 32, 46]       --\n",
       "│    │    └─Conv3d: 3-17                 [1, 256, 90, 32, 46]      16,384\n",
       "│    │    └─BatchNorm3d: 3-18            [1, 256, 90, 32, 46]      512\n",
       "│    │    └─ReLU: 3-19                   [1, 256, 90, 32, 46]      --\n",
       "│    └─Bottleneck: 2-3                   [1, 256, 90, 32, 46]      --\n",
       "│    │    └─Conv3d: 3-20                 [1, 64, 90, 32, 46]       16,384\n",
       "│    │    └─BatchNorm3d: 3-21            [1, 64, 90, 32, 46]       128\n",
       "│    │    └─ReLU: 3-22                   [1, 64, 90, 32, 46]       --\n",
       "│    │    └─Conv3d: 3-23                 [1, 64, 90, 32, 46]       110,592\n",
       "│    │    └─BatchNorm3d: 3-24            [1, 64, 90, 32, 46]       128\n",
       "│    │    └─ReLU: 3-25                   [1, 64, 90, 32, 46]       --\n",
       "│    │    └─Conv3d: 3-26                 [1, 256, 90, 32, 46]      16,384\n",
       "│    │    └─BatchNorm3d: 3-27            [1, 256, 90, 32, 46]      512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 90, 32, 46]      --\n",
       "├─Sequential: 1-6                        [1, 512, 45, 16, 23]      --\n",
       "│    └─Bottleneck: 2-4                   [1, 512, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-29                 [1, 128, 90, 32, 46]      32,768\n",
       "│    │    └─BatchNorm3d: 3-30            [1, 128, 90, 32, 46]      256\n",
       "│    │    └─ReLU: 3-31                   [1, 128, 90, 32, 46]      --\n",
       "│    │    └─Conv3d: 3-32                 [1, 128, 45, 16, 23]      442,368\n",
       "│    │    └─BatchNorm3d: 3-33            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-34                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-35                 [1, 512, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-36            [1, 512, 45, 16, 23]      1,024\n",
       "│    │    └─Sequential: 3-37             [1, 512, 45, 16, 23]      132,096\n",
       "│    │    └─ReLU: 3-38                   [1, 512, 45, 16, 23]      --\n",
       "│    └─Bottleneck: 2-5                   [1, 512, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-39                 [1, 128, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-40            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-41                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-42                 [1, 128, 45, 16, 23]      442,368\n",
       "│    │    └─BatchNorm3d: 3-43            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-44                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-45                 [1, 512, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-46            [1, 512, 45, 16, 23]      1,024\n",
       "│    │    └─ReLU: 3-47                   [1, 512, 45, 16, 23]      --\n",
       "│    └─Bottleneck: 2-6                   [1, 512, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-48                 [1, 128, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-49            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-50                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-51                 [1, 128, 45, 16, 23]      442,368\n",
       "│    │    └─BatchNorm3d: 3-52            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-53                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-54                 [1, 512, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-55            [1, 512, 45, 16, 23]      1,024\n",
       "│    │    └─ReLU: 3-56                   [1, 512, 45, 16, 23]      --\n",
       "│    └─Bottleneck: 2-7                   [1, 512, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-57                 [1, 128, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-58            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-59                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-60                 [1, 128, 45, 16, 23]      442,368\n",
       "│    │    └─BatchNorm3d: 3-61            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-62                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-63                 [1, 512, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-64            [1, 512, 45, 16, 23]      1,024\n",
       "│    │    └─ReLU: 3-65                   [1, 512, 45, 16, 23]      --\n",
       "├─Sequential: 1-7                        [1, 1024, 23, 8, 12]      --\n",
       "│    └─Bottleneck: 2-8                   [1, 1024, 23, 8, 12]      --\n",
       "│    │    └─Conv3d: 3-66                 [1, 256, 45, 16, 23]      131,072\n",
       "│    │    └─BatchNorm3d: 3-67            [1, 256, 45, 16, 23]      512\n",
       "│    │    └─ReLU: 3-68                   [1, 256, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-69                 [1, 256, 23, 8, 12]       1,769,472\n",
       "│    │    └─BatchNorm3d: 3-70            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-71                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-72                 [1, 1024, 23, 8, 12]      262,144\n",
       "│    │    └─BatchNorm3d: 3-73            [1, 1024, 23, 8, 12]      2,048\n",
       "│    │    └─Sequential: 3-74             [1, 1024, 23, 8, 12]      526,336\n",
       "│    │    └─ReLU: 3-75                   [1, 1024, 23, 8, 12]      --\n",
       "│    └─Bottleneck: 2-9                   [1, 1024, 23, 8, 12]      --\n",
       "│    │    └─Conv3d: 3-76                 [1, 256, 23, 8, 12]       262,144\n",
       "│    │    └─BatchNorm3d: 3-77            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-78                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-79                 [1, 256, 23, 8, 12]       1,769,472\n",
       "│    │    └─BatchNorm3d: 3-80            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-81                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-82                 [1, 1024, 23, 8, 12]      262,144\n",
       "│    │    └─BatchNorm3d: 3-83            [1, 1024, 23, 8, 12]      2,048\n",
       "│    │    └─ReLU: 3-84                   [1, 1024, 23, 8, 12]      --\n",
       "│    └─Bottleneck: 2-10                  [1, 1024, 23, 8, 12]      --\n",
       "│    │    └─Conv3d: 3-85                 [1, 256, 23, 8, 12]       262,144\n",
       "│    │    └─BatchNorm3d: 3-86            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-87                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-88                 [1, 256, 23, 8, 12]       1,769,472\n",
       "│    │    └─BatchNorm3d: 3-89            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-90                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-91                 [1, 1024, 23, 8, 12]      262,144\n",
       "│    │    └─BatchNorm3d: 3-92            [1, 1024, 23, 8, 12]      2,048\n",
       "│    │    └─ReLU: 3-93                   [1, 1024, 23, 8, 12]      --\n",
       "│    └─Bottleneck: 2-11                  [1, 1024, 23, 8, 12]      --\n",
       "│    │    └─Conv3d: 3-94                 [1, 256, 23, 8, 12]       262,144\n",
       "│    │    └─BatchNorm3d: 3-95            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-96                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-97                 [1, 256, 23, 8, 12]       1,769,472\n",
       "│    │    └─BatchNorm3d: 3-98            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-99                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-100                [1, 1024, 23, 8, 12]      262,144\n",
       "│    │    └─BatchNorm3d: 3-101           [1, 1024, 23, 8, 12]      2,048\n",
       "│    │    └─ReLU: 3-102                  [1, 1024, 23, 8, 12]      --\n",
       "│    └─Bottleneck: 2-12                  [1, 1024, 23, 8, 12]      --\n",
       "│    │    └─Conv3d: 3-103                [1, 256, 23, 8, 12]       262,144\n",
       "│    │    └─BatchNorm3d: 3-104           [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-105                  [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-106                [1, 256, 23, 8, 12]       1,769,472\n",
       "│    │    └─BatchNorm3d: 3-107           [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-108                  [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-109                [1, 1024, 23, 8, 12]      262,144\n",
       "│    │    └─BatchNorm3d: 3-110           [1, 1024, 23, 8, 12]      2,048\n",
       "│    │    └─ReLU: 3-111                  [1, 1024, 23, 8, 12]      --\n",
       "│    └─Bottleneck: 2-13                  [1, 1024, 23, 8, 12]      --\n",
       "│    │    └─Conv3d: 3-112                [1, 256, 23, 8, 12]       262,144\n",
       "│    │    └─BatchNorm3d: 3-113           [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-114                  [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-115                [1, 256, 23, 8, 12]       1,769,472\n",
       "│    │    └─BatchNorm3d: 3-116           [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-117                  [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-118                [1, 1024, 23, 8, 12]      262,144\n",
       "│    │    └─BatchNorm3d: 3-119           [1, 1024, 23, 8, 12]      2,048\n",
       "│    │    └─ReLU: 3-120                  [1, 1024, 23, 8, 12]      --\n",
       "├─Sequential: 1-8                        [1, 2048, 12, 4, 6]       --\n",
       "│    └─Bottleneck: 2-14                  [1, 2048, 12, 4, 6]       --\n",
       "│    │    └─Conv3d: 3-121                [1, 512, 23, 8, 12]       524,288\n",
       "│    │    └─BatchNorm3d: 3-122           [1, 512, 23, 8, 12]       1,024\n",
       "│    │    └─ReLU: 3-123                  [1, 512, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-124                [1, 512, 12, 4, 6]        7,077,888\n",
       "│    │    └─BatchNorm3d: 3-125           [1, 512, 12, 4, 6]        1,024\n",
       "│    │    └─ReLU: 3-126                  [1, 512, 12, 4, 6]        --\n",
       "│    │    └─Conv3d: 3-127                [1, 2048, 12, 4, 6]       1,048,576\n",
       "│    │    └─BatchNorm3d: 3-128           [1, 2048, 12, 4, 6]       4,096\n",
       "│    │    └─Sequential: 3-129            [1, 2048, 12, 4, 6]       2,101,248\n",
       "│    │    └─ReLU: 3-130                  [1, 2048, 12, 4, 6]       --\n",
       "│    └─Bottleneck: 2-15                  [1, 2048, 12, 4, 6]       --\n",
       "│    │    └─Conv3d: 3-131                [1, 512, 12, 4, 6]        1,048,576\n",
       "│    │    └─BatchNorm3d: 3-132           [1, 512, 12, 4, 6]        1,024\n",
       "│    │    └─ReLU: 3-133                  [1, 512, 12, 4, 6]        --\n",
       "│    │    └─Conv3d: 3-134                [1, 512, 12, 4, 6]        7,077,888\n",
       "│    │    └─BatchNorm3d: 3-135           [1, 512, 12, 4, 6]        1,024\n",
       "│    │    └─ReLU: 3-136                  [1, 512, 12, 4, 6]        --\n",
       "│    │    └─Conv3d: 3-137                [1, 2048, 12, 4, 6]       1,048,576\n",
       "│    │    └─BatchNorm3d: 3-138           [1, 2048, 12, 4, 6]       4,096\n",
       "│    │    └─ReLU: 3-139                  [1, 2048, 12, 4, 6]       --\n",
       "│    └─Bottleneck: 2-16                  [1, 2048, 12, 4, 6]       --\n",
       "│    │    └─Conv3d: 3-140                [1, 512, 12, 4, 6]        1,048,576\n",
       "│    │    └─BatchNorm3d: 3-141           [1, 512, 12, 4, 6]        1,024\n",
       "│    │    └─ReLU: 3-142                  [1, 512, 12, 4, 6]        --\n",
       "│    │    └─Conv3d: 3-143                [1, 512, 12, 4, 6]        7,077,888\n",
       "│    │    └─BatchNorm3d: 3-144           [1, 512, 12, 4, 6]        1,024\n",
       "│    │    └─ReLU: 3-145                  [1, 512, 12, 4, 6]        --\n",
       "│    │    └─Conv3d: 3-146                [1, 2048, 12, 4, 6]       1,048,576\n",
       "│    │    └─BatchNorm3d: 3-147           [1, 2048, 12, 4, 6]       4,096\n",
       "│    │    └─ReLU: 3-148                  [1, 2048, 12, 4, 6]       --\n",
       "├─AdaptiveAvgPool3d: 1-9                 [1, 2048, 1, 1, 1]        --\n",
       "├─Linear: 1-10                           [1, 360]                  737,640\n",
       "==========================================================================================\n",
       "Total params: 46,892,712\n",
       "Trainable params: 46,892,712\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 166.72\n",
       "==========================================================================================\n",
       "Input size (MB): 16.96\n",
       "Forward/backward pass size (MB): 5744.99\n",
       "Params size (MB): 187.57\n",
       "Estimated Total Size (MB): 5949.52\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = generate_model(50)\n",
    "summary(model, (1, 1, 180, 128, 184))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60436868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Epoch: 0   Training Loss: 8.910967659950256 \n",
      "Epoch: 0   Validation Loss: 8.727133646011353 \n",
      "Epoch: 1   Training Loss: 8.56054787158966 \n",
      "Epoch: 1   Validation Loss: 8.72879846572876 \n",
      "Epoch: 2   Training Loss: 8.546824946403504 \n",
      "Epoch: 2   Validation Loss: 8.73028904914856 \n",
      "Epoch: 3   Training Loss: 8.53639750480652 \n",
      "Epoch: 3   Validation Loss: 8.733432140350342 \n",
      "Epoch: 4   Training Loss: 8.533109016418457 \n",
      "Epoch: 4   Validation Loss: 8.733615894317627 \n",
      "Epoch: 5   Training Loss: 8.522382926940917 \n",
      "Epoch: 5   Validation Loss: 8.736032991409301 \n",
      "Epoch: 6   Training Loss: 8.511893513202667 \n",
      "Epoch: 6   Validation Loss: 8.739968938827515 \n",
      "Epoch: 7   Training Loss: 8.50456552028656 \n",
      "Epoch: 7   Validation Loss: 8.740758304595948 \n",
      "Epoch: 8   Training Loss: 8.500386874675751 \n",
      "Epoch: 8   Validation Loss: 8.739876985549927 \n",
      "Epoch: 9   Training Loss: 8.498206617832183 \n",
      "Epoch: 9   Validation Loss: 8.740848627090454 \n",
      "Epoch: 10   Training Loss: 8.493323023319244 \n",
      "Epoch: 10   Validation Loss: 8.741176462173462 \n",
      "Epoch: 11   Training Loss: 8.490486031770706 \n",
      "Epoch: 11   Validation Loss: 8.740142498016358 \n",
      "Epoch: 12   Training Loss: 8.495931247472763 \n",
      "Epoch: 12   Validation Loss: 8.746806716918945 \n",
      "Epoch: 13   Training Loss: 8.53394905090332 \n",
      "Epoch: 13   Validation Loss: 8.752853488922119 \n",
      "Epoch: 14   Training Loss: 8.495201891362667 \n",
      "Epoch: 14   Validation Loss: 8.745799283981324 \n",
      "Epoch: 15   Training Loss: 8.490128810256719 \n",
      "Epoch: 15   Validation Loss: 8.736058940887451 \n",
      "Epoch: 16   Training Loss: 8.489431681111455 \n",
      "Epoch: 16   Validation Loss: 8.739855613708496 \n",
      "Epoch: 17   Training Loss: 8.488322930335999 \n",
      "Epoch: 17   Validation Loss: 8.734357986450195 \n",
      "Epoch: 18   Training Loss: 8.487361671552062 \n",
      "Epoch: 18   Validation Loss: 8.73612687110901 \n",
      "Epoch: 19   Training Loss: 8.488561597913504 \n",
      "Epoch: 19   Validation Loss: 8.735890645980835 \n",
      "Epoch: 20   Training Loss: 8.485866877622902 \n",
      "Epoch: 20   Validation Loss: 8.73556492805481 \n",
      "Epoch: 21   Training Loss: 8.485301236957312 \n",
      "Epoch: 21   Validation Loss: 8.735656642913819 \n",
      "Epoch: 22   Training Loss: 8.48485296813771 \n",
      "Epoch: 22   Validation Loss: 8.735643301010132 \n",
      "Epoch: 23   Training Loss: 8.484602576056496 \n",
      "Epoch: 23   Validation Loss: 8.736018676757812 \n",
      "Epoch: 24   Training Loss: 8.484448083639144 \n",
      "Epoch: 24   Validation Loss: 8.73601469039917 \n",
      "Epoch: 25   Training Loss: 8.484311765863094 \n",
      "Epoch: 25   Validation Loss: 8.736189661026001 \n",
      "Epoch: 26   Training Loss: 8.48422124209581 \n",
      "Epoch: 26   Validation Loss: 8.736212997436523 \n",
      "Epoch: 27   Training Loss: 8.484118406163761 \n",
      "Epoch: 27   Validation Loss: 8.736457090377808 \n",
      "Epoch: 28   Training Loss: 8.484208049702575 \n",
      "Epoch: 28   Validation Loss: 8.736303672790527 \n",
      "Epoch: 29   Training Loss: 8.484057368916401 \n",
      "Epoch: 29   Validation Loss: 8.73632767677307 \n",
      "Epoch: 30   Training Loss: 8.484024922184762 \n",
      "Epoch: 30   Validation Loss: 8.736357622146606 \n",
      "Epoch: 31   Training Loss: 8.483985280457564 \n",
      "Epoch: 31   Validation Loss: 8.736408939361572 \n",
      "Epoch: 32   Training Loss: 8.483960548439791 \n",
      "Epoch: 32   Validation Loss: 8.73633744239807 \n",
      "Epoch: 33   Training Loss: 8.483908964750736 \n",
      "Epoch: 33   Validation Loss: 8.73632092475891 \n",
      "Epoch: 34   Training Loss: 8.483872573453555 \n",
      "Epoch: 34   Validation Loss: 8.73627679824829 \n",
      "Epoch: 35   Training Loss: 8.483817654209416 \n",
      "Epoch: 35   Validation Loss: 8.736252899169921 \n",
      "Epoch: 36   Training Loss: 8.48382200637905 \n",
      "Epoch: 36   Validation Loss: 8.73621298789978 \n",
      "Epoch: 37   Training Loss: 8.483702555087365 \n",
      "Epoch: 37   Validation Loss: 8.736387510299682 \n",
      "Epoch: 38   Training Loss: 8.483810203697358 \n",
      "Epoch: 38   Validation Loss: 8.73637565612793 \n",
      "Epoch: 39   Training Loss: 8.483691244481015 \n",
      "Epoch: 39   Validation Loss: 8.73629870414734 \n",
      "Epoch: 40   Training Loss: 8.483625825277123 \n",
      "Epoch: 40   Validation Loss: 8.736286306381226 \n",
      "Epoch: 41   Training Loss: 8.483538959168364 \n",
      "Epoch: 41   Validation Loss: 8.736277446746826 \n",
      "Epoch: 42   Training Loss: 8.483465992043785 \n",
      "Epoch: 42   Validation Loss: 8.736287622451782 \n",
      "Epoch: 43   Training Loss: 8.483504201220349 \n",
      "Epoch: 43   Validation Loss: 8.736175136566162 \n",
      "Epoch: 44   Training Loss: 8.483303411658271 \n",
      "Epoch: 44   Validation Loss: 8.736146697998047 \n",
      "Epoch: 45   Training Loss: 8.483247817166848 \n",
      "Epoch: 45   Validation Loss: 8.736123094558716 \n",
      "Epoch: 46   Training Loss: 8.483153778985143 \n",
      "Epoch: 46   Validation Loss: 8.736084461212158 \n",
      "Epoch: 47   Training Loss: 8.4829734717909 \n",
      "Epoch: 47   Validation Loss: 8.736059217453002 \n",
      "Epoch: 48   Training Loss: 8.482721398707362 \n",
      "Epoch: 48   Validation Loss: 8.736065301895142 \n",
      "Epoch: 49   Training Loss: 8.482455062561785 \n",
      "Epoch: 49   Validation Loss: 8.73665771484375 \n",
      "Epoch: 50   Training Loss: 8.481992532144067 \n",
      "Epoch: 50   Validation Loss: 8.736440868377686 \n",
      "Epoch: 51   Training Loss: 8.48204809540417 \n",
      "Epoch: 51   Validation Loss: 8.736683826446534 \n",
      "Epoch: 52   Training Loss: 8.48148376461235 \n",
      "Epoch: 52   Validation Loss: 8.736409196853637 \n",
      "Epoch: 53   Training Loss: 8.480164175938116 \n",
      "Epoch: 53   Validation Loss: 8.737828874588013 \n",
      "Epoch: 54   Training Loss: 8.478408625768498 \n",
      "Epoch: 54   Validation Loss: 8.739282836914063 \n",
      "Epoch: 55   Training Loss: 8.473145269891248 \n",
      "Epoch: 55   Validation Loss: 8.740925779342652 \n",
      "Epoch: 56   Training Loss: 8.47090186689049 \n",
      "Epoch: 56   Validation Loss: 8.757914562225341 \n",
      "Epoch: 57   Training Loss: 8.466774701057002 \n",
      "Epoch: 57   Validation Loss: 8.771633558273315 \n",
      "Epoch: 58   Training Loss: 8.462276710011064 \n",
      "Epoch: 58   Validation Loss: 8.785384321212769 \n",
      "Epoch: 59   Training Loss: 8.46594102051109 \n",
      "Epoch: 59   Validation Loss: 8.772353038787841 \n",
      "Epoch: 60   Training Loss: 8.469573385901748 \n",
      "Epoch: 60   Validation Loss: 8.779291791915893 \n",
      "Epoch: 61   Training Loss: 8.48264256902039 \n",
      "Epoch: 61   Validation Loss: 8.749605407714844 \n",
      "Epoch: 62   Training Loss: 8.485944624170662 \n",
      "Epoch: 62   Validation Loss: 8.752305288314819 \n",
      "Epoch: 63   Training Loss: 8.489236194454133 \n",
      "Epoch: 63   Validation Loss: 8.736149911880494 \n",
      "Epoch: 64   Training Loss: 8.48427182007581 \n",
      "Epoch: 64   Validation Loss: 8.743550033569337 \n",
      "Epoch: 65   Training Loss: 8.483118590861558 \n",
      "Epoch: 65   Validation Loss: 8.73692099571228 \n",
      "Epoch: 66   Training Loss: 8.483756073638796 \n",
      "Epoch: 66   Validation Loss: 8.737679691314698 \n",
      "Epoch: 67   Training Loss: 8.478969225892797 \n",
      "Epoch: 67   Validation Loss: 8.737719774246216 \n",
      "Epoch: 68   Training Loss: 8.470231364071369 \n",
      "Epoch: 68   Validation Loss: 8.74015139579773 \n",
      "Epoch: 69   Training Loss: 8.459352711047977 \n",
      "Epoch: 69   Validation Loss: 8.746769037246704 \n",
      "Epoch: 70   Training Loss: 8.449007444120944 \n",
      "Epoch: 70   Validation Loss: 8.754633293151855 \n",
      "Epoch: 71   Training Loss: 8.436516378186644 \n",
      "Epoch: 71   Validation Loss: 8.757874460220338 \n",
      "Epoch: 72   Training Loss: 8.42117361817509 \n",
      "Epoch: 72   Validation Loss: 8.771407470703124 \n",
      "Epoch: 73   Training Loss: 8.445226402431727 \n",
      "Epoch: 73   Validation Loss: 8.775272541046142 \n",
      "Epoch: 74   Training Loss: 8.501473858356476 \n",
      "Epoch: 74   Validation Loss: 8.758769388198852 \n",
      "Epoch: 75   Training Loss: 8.504000060558319 \n",
      "Epoch: 75   Validation Loss: 8.766438961029053 \n",
      "Epoch: 76   Training Loss: 8.494208114147186 \n",
      "Epoch: 76   Validation Loss: 8.763191452026367 \n",
      "Epoch: 77   Training Loss: 8.497623212337494 \n",
      "Epoch: 77   Validation Loss: 8.760308456420898 \n",
      "Epoch: 78   Training Loss: 8.487797112464905 \n",
      "Epoch: 78   Validation Loss: 8.772686567306518 \n",
      "Epoch: 79   Training Loss: 8.48477635383606 \n",
      "Epoch: 79   Validation Loss: 8.780243997573853 \n",
      "Epoch: 80   Training Loss: 8.481988861560822 \n",
      "Epoch: 80   Validation Loss: 8.761872482299804 \n",
      "Epoch: 81   Training Loss: 8.472515730857848 \n",
      "Epoch: 81   Validation Loss: 8.757125902175904 \n",
      "Epoch: 82   Training Loss: 8.46898862361908 \n",
      "Epoch: 82   Validation Loss: 8.742687654495239 \n",
      "Epoch: 83   Training Loss: 8.467002329826355 \n",
      "Epoch: 83   Validation Loss: 8.754338893890381 \n",
      "Epoch: 84   Training Loss: 8.462755827903747 \n",
      "Epoch: 84   Validation Loss: 8.811338109970093 \n",
      "Epoch: 85   Training Loss: 8.460235450267792 \n",
      "Epoch: 85   Validation Loss: 8.86013518333435 \n",
      "Epoch: 86   Training Loss: 8.501267936229706 \n",
      "Epoch: 86   Validation Loss: 8.970511159896851 \n",
      "Epoch: 87   Training Loss: 8.47097470998764 \n",
      "Epoch: 87   Validation Loss: 9.009923458099365 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88   Training Loss: 8.46782990604639 \n",
      "Epoch: 88   Validation Loss: 8.858855066299439 \n",
      "Epoch: 89   Training Loss: 8.449937668144702 \n",
      "Epoch: 89   Validation Loss: 8.874694061279296 \n",
      "Epoch: 90   Training Loss: 8.445013651177288 \n",
      "Epoch: 90   Validation Loss: 8.790194654464722 \n",
      "Epoch: 91   Training Loss: 8.437083059251309 \n",
      "Epoch: 91   Validation Loss: 8.780454216003418 \n",
      "Epoch: 92   Training Loss: 8.426953628957271 \n",
      "Epoch: 92   Validation Loss: 8.763729438781738 \n",
      "Epoch: 93   Training Loss: 8.419134220927953 \n",
      "Epoch: 93   Validation Loss: 8.747516841888428 \n",
      "Epoch: 94   Training Loss: 8.421218005120755 \n",
      "Epoch: 94   Validation Loss: 8.738847455978394 \n",
      "Epoch: 95   Training Loss: 8.41088564403355 \n",
      "Epoch: 95   Validation Loss: 8.755626983642578 \n",
      "Epoch: 96   Training Loss: 8.416103798672557 \n",
      "Epoch: 96   Validation Loss: 8.750886497497559 \n",
      "Epoch: 97   Training Loss: 8.400067842714488 \n",
      "Epoch: 97   Validation Loss: 8.75445424079895 \n",
      "Epoch: 98   Training Loss: 8.385491701811553 \n",
      "Epoch: 98   Validation Loss: 8.759950866699219 \n",
      "Epoch: 99   Training Loss: 8.468175596594811 \n",
      "Epoch: 99   Validation Loss: 8.776069126129151 \n",
      "Epoch: 100   Training Loss: 8.41835089325905 \n",
      "Epoch: 100   Validation Loss: 8.757581663131713 \n",
      "Epoch: 101   Training Loss: 8.512292728424072 \n",
      "Epoch: 101   Validation Loss: 8.776016883850097 \n",
      "Epoch: 102   Training Loss: 8.436115090847016 \n",
      "Epoch: 102   Validation Loss: 8.787005672454834 \n",
      "Epoch: 103   Training Loss: 8.40510653913021 \n",
      "Epoch: 103   Validation Loss: 8.824874076843262 \n",
      "Epoch: 104   Training Loss: 8.404094880670309 \n",
      "Epoch: 104   Validation Loss: 8.800997982025146 \n",
      "Epoch: 105   Training Loss: 8.391808127015828 \n",
      "Epoch: 105   Validation Loss: 8.82814549446106 \n",
      "Epoch: 106   Training Loss: 8.390744313299656 \n",
      "Epoch: 106   Validation Loss: 8.842763509750366 \n",
      "Epoch: 107   Training Loss: 8.399855218678713 \n",
      "Epoch: 107   Validation Loss: 8.787711992263795 \n",
      "Epoch: 108   Training Loss: 8.405419149398803 \n",
      "Epoch: 108   Validation Loss: 8.792164487838745 \n",
      "Epoch: 109   Training Loss: 8.396466532871127 \n",
      "Epoch: 109   Validation Loss: 8.793076772689819 \n",
      "Epoch: 110   Training Loss: 8.399684394150972 \n",
      "Epoch: 110   Validation Loss: 8.800663919448853 \n",
      "Epoch: 111   Training Loss: 8.397239306233823 \n",
      "Epoch: 111   Validation Loss: 8.814180727005004 \n",
      "Epoch: 112   Training Loss: 8.38107488784939 \n",
      "Epoch: 112   Validation Loss: 8.80812994003296 \n",
      "Epoch: 113   Training Loss: 8.39064860828221 \n",
      "Epoch: 113   Validation Loss: 8.785786476135254 \n",
      "Epoch: 114   Training Loss: 8.384694915600122 \n",
      "Epoch: 114   Validation Loss: 8.793521966934204 \n",
      "Epoch: 115   Training Loss: 8.370252338573337 \n",
      "Epoch: 115   Validation Loss: 8.831416501998902 \n",
      "Epoch: 116   Training Loss: 8.372693783417343 \n",
      "Epoch: 116   Validation Loss: 8.79621280670166 \n",
      "Epoch: 117   Training Loss: 8.356721135042608 \n",
      "Epoch: 117   Validation Loss: 8.790581598281861 \n",
      "Epoch: 118   Training Loss: 8.371125117056073 \n",
      "Epoch: 118   Validation Loss: 8.78428518295288 \n",
      "Epoch: 119   Training Loss: 8.367553071528674 \n",
      "Epoch: 119   Validation Loss: 8.785849142074586 \n",
      "Epoch: 120   Training Loss: 8.365031555779279 \n",
      "Epoch: 120   Validation Loss: 8.783283052444459 \n",
      "Epoch: 121   Training Loss: 8.347749418560415 \n",
      "Epoch: 121   Validation Loss: 8.81224564552307 \n",
      "Epoch: 122   Training Loss: 8.338292019320653 \n",
      "Epoch: 122   Validation Loss: 8.804694137573243 \n",
      "Epoch: 123   Training Loss: 8.328697058223188 \n",
      "Epoch: 123   Validation Loss: 8.808211832046508 \n",
      "Epoch: 124   Training Loss: 8.323833116665483 \n",
      "Epoch: 124   Validation Loss: 8.841748790740967 \n",
      "Epoch: 125   Training Loss: 8.305844789221883 \n",
      "Epoch: 125   Validation Loss: 8.86456784248352 \n",
      "Epoch: 126   Training Loss: 8.340718714743852 \n",
      "Epoch: 126   Validation Loss: 8.821067094802856 \n",
      "Epoch: 127   Training Loss: 8.318088330700993 \n",
      "Epoch: 127   Validation Loss: 8.852237176895141 \n",
      "Epoch: 128   Training Loss: 8.303240221850574 \n",
      "Epoch: 128   Validation Loss: 8.846438398361206 \n",
      "Epoch: 129   Training Loss: 8.316512477584183 \n",
      "Epoch: 129   Validation Loss: 8.829569053649902 \n",
      "Epoch: 130   Training Loss: 8.291635374501347 \n",
      "Epoch: 130   Validation Loss: 8.849342136383056 \n",
      "Epoch: 131   Training Loss: 8.301634236723185 \n",
      "Epoch: 131   Validation Loss: 8.867228393554688 \n",
      "Epoch: 132   Training Loss: 8.286604375466704 \n",
      "Epoch: 132   Validation Loss: 8.885414590835572 \n",
      "Epoch: 133   Training Loss: 8.289918599575758 \n",
      "Epoch: 133   Validation Loss: 8.877841529846192 \n",
      "Epoch: 134   Training Loss: 8.283598798215388 \n",
      "Epoch: 134   Validation Loss: 8.866294288635254 \n",
      "Epoch: 135   Training Loss: 8.288591602295638 \n",
      "Epoch: 135   Validation Loss: 8.860707960128785 \n",
      "Epoch: 136   Training Loss: 8.288722822591662 \n",
      "Epoch: 136   Validation Loss: 8.846248950958252 \n",
      "Epoch: 137   Training Loss: 8.312507354468107 \n",
      "Epoch: 137   Validation Loss: 8.825146522521973 \n",
      "Epoch: 138   Training Loss: 8.289629220962524 \n",
      "Epoch: 138   Validation Loss: 8.827587089538575 \n",
      "Epoch: 139   Training Loss: 8.282574613541364 \n",
      "Epoch: 139   Validation Loss: 8.831581401824952 \n",
      "Epoch: 140   Training Loss: 8.26028442338109 \n",
      "Epoch: 140   Validation Loss: 8.863914337158203 \n",
      "Epoch: 141   Training Loss: 8.257279339917003 \n",
      "Epoch: 141   Validation Loss: 8.83805094718933 \n",
      "Epoch: 142   Training Loss: 8.239629292711616 \n",
      "Epoch: 142   Validation Loss: 8.848646259307861 \n",
      "Epoch: 143   Training Loss: 8.223155215159059 \n",
      "Epoch: 143   Validation Loss: 8.860407657623291 \n",
      "Epoch: 144   Training Loss: 8.210313187241555 \n",
      "Epoch: 144   Validation Loss: 8.869549188613892 \n",
      "Epoch: 145   Training Loss: 8.18557142019272 \n",
      "Epoch: 145   Validation Loss: 8.90427188873291 \n",
      "Epoch: 146   Training Loss: 8.170456617698074 \n",
      "Epoch: 146   Validation Loss: 8.911001529693603 \n",
      "Epoch: 147   Training Loss: 8.190306261703372 \n",
      "Epoch: 147   Validation Loss: 8.893453464508056 \n",
      "Epoch: 148   Training Loss: 8.169533396959304 \n",
      "Epoch: 148   Validation Loss: 8.913889169692993 \n",
      "Epoch: 149   Training Loss: 8.22029718875885 \n",
      "Epoch: 149   Validation Loss: 8.867797327041625 \n",
      "Epoch: 150   Training Loss: 8.289361808300018 \n",
      "Epoch: 150   Validation Loss: 8.891050863265992 \n",
      "Epoch: 151   Training Loss: 8.29918275117874 \n",
      "Epoch: 151   Validation Loss: 8.92403450012207 \n",
      "Epoch: 152   Training Loss: 8.218296159505844 \n",
      "Epoch: 152   Validation Loss: 8.930251941680908 \n",
      "Epoch: 153   Training Loss: 8.191947491168976 \n",
      "Epoch: 153   Validation Loss: 8.973448295593261 \n",
      "Epoch: 154   Training Loss: 8.182491804212331 \n",
      "Epoch: 154   Validation Loss: 8.928544273376465 \n",
      "Epoch: 155   Training Loss: 8.158899782784284 \n",
      "Epoch: 155   Validation Loss: 8.884256439208984 \n",
      "Epoch: 156   Training Loss: 8.124369024708868 \n",
      "Epoch: 156   Validation Loss: 8.929676895141602 \n",
      "Epoch: 157   Training Loss: 8.129255338720977 \n",
      "Epoch: 157   Validation Loss: 8.946822996139526 \n",
      "Epoch: 158   Training Loss: 8.11684523332864 \n",
      "Epoch: 158   Validation Loss: 8.89931040763855 \n",
      "Epoch: 159   Training Loss: 8.097785277627409 \n",
      "Epoch: 159   Validation Loss: 8.92182188987732 \n",
      "Epoch: 160   Training Loss: 8.068349714577199 \n",
      "Epoch: 160   Validation Loss: 9.031673822402954 \n",
      "Epoch: 161   Training Loss: 8.080270161777735 \n",
      "Epoch: 161   Validation Loss: 8.897964363098145 \n",
      "Epoch: 162   Training Loss: 8.068398484140635 \n",
      "Epoch: 162   Validation Loss: 9.058660535812377 \n",
      "Epoch: 163   Training Loss: 8.078208501338958 \n",
      "Epoch: 163   Validation Loss: 9.000128631591798 \n",
      "Epoch: 164   Training Loss: 8.054147105664015 \n",
      "Epoch: 164   Validation Loss: 9.080419998168946 \n",
      "Epoch: 165   Training Loss: 8.060504961311818 \n",
      "Epoch: 165   Validation Loss: 9.099065217971802 \n",
      "Epoch: 166   Training Loss: 8.067823897376657 \n",
      "Epoch: 166   Validation Loss: 9.142232818603516 \n",
      "Epoch: 167   Training Loss: 8.03637010589242 \n",
      "Epoch: 167   Validation Loss: 9.209110012054444 \n",
      "Epoch: 168   Training Loss: 8.005336697846651 \n",
      "Epoch: 168   Validation Loss: 9.171970300674438 \n",
      "Epoch: 169   Training Loss: 7.959208079949021 \n",
      "Epoch: 169   Validation Loss: 9.261851568222045 \n",
      "Epoch: 170   Training Loss: 7.951842878311872 \n",
      "Epoch: 170   Validation Loss: 9.144661779403686 \n",
      "Epoch: 171   Training Loss: 8.03067759230733 \n",
      "Epoch: 171   Validation Loss: 9.217641429901123 \n",
      "Epoch: 172   Training Loss: 7.924817672073841 \n",
      "Epoch: 172   Validation Loss: 9.289379425048828 \n",
      "Epoch: 173   Training Loss: 7.859439696371555 \n",
      "Epoch: 173   Validation Loss: 9.337112722396851 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 174   Training Loss: 7.883467222601175 \n",
      "Epoch: 174   Validation Loss: 9.252546548843384 \n",
      "Epoch: 175   Training Loss: 7.933497635424137 \n",
      "Epoch: 175   Validation Loss: 9.208754997253418 \n",
      "Epoch: 176   Training Loss: 7.898731876313686 \n",
      "Epoch: 176   Validation Loss: 9.146307563781738 \n",
      "Epoch: 177   Training Loss: 7.844026287645102 \n",
      "Epoch: 177   Validation Loss: 9.191887817382813 \n",
      "Epoch: 178   Training Loss: 7.813667964413762 \n",
      "Epoch: 178   Validation Loss: 9.169699277877807 \n",
      "Epoch: 179   Training Loss: 7.809930724501609 \n",
      "Epoch: 179   Validation Loss: 9.198466663360596 \n",
      "Epoch: 180   Training Loss: 7.819246979728341 \n",
      "Epoch: 180   Validation Loss: 9.142744302749634 \n",
      "Epoch: 181   Training Loss: 7.805179300904274 \n",
      "Epoch: 181   Validation Loss: 9.101352910995484 \n",
      "Epoch: 182   Training Loss: 7.734932668954134 \n",
      "Epoch: 182   Validation Loss: 9.145942058563232 \n",
      "Epoch: 183   Training Loss: 7.705524940788746 \n",
      "Epoch: 183   Validation Loss: 9.15460651397705 \n",
      "Epoch: 184   Training Loss: 7.712793701887131 \n",
      "Epoch: 184   Validation Loss: 9.121778450012208 \n",
      "Epoch: 185   Training Loss: 7.735592028200626 \n",
      "Epoch: 185   Validation Loss: 9.112182664871217 \n",
      "Epoch: 186   Training Loss: 7.834897173047065 \n",
      "Epoch: 186   Validation Loss: 9.138232383728027 \n",
      "Epoch: 187   Training Loss: 7.712031798064709 \n",
      "Epoch: 187   Validation Loss: 9.062253189086913 \n",
      "Epoch: 188   Training Loss: 7.6179824033379555 \n",
      "Epoch: 188   Validation Loss: 9.190080060958863 \n",
      "Epoch: 189   Training Loss: 7.644102267026901 \n",
      "Epoch: 189   Validation Loss: 8.972627639770508 \n",
      "Epoch: 190   Training Loss: 7.690236579626799 \n",
      "Epoch: 190   Validation Loss: 9.07439637184143 \n",
      "Epoch: 191   Training Loss: 7.627784374803305 \n",
      "Epoch: 191   Validation Loss: 9.104562892913819 \n",
      "Epoch: 192   Training Loss: 7.564905636012554 \n",
      "Epoch: 192   Validation Loss: 9.13998908996582 \n",
      "Epoch: 193   Training Loss: 7.45903057679534 \n",
      "Epoch: 193   Validation Loss: 9.12816388130188 \n",
      "Epoch: 194   Training Loss: 7.47027745783329 \n",
      "Epoch: 194   Validation Loss: 9.099993324279785 \n",
      "Epoch: 195   Training Loss: 7.433399866521358 \n",
      "Epoch: 195   Validation Loss: 9.385893812179566 \n",
      "Epoch: 196   Training Loss: 7.456918129622936 \n",
      "Epoch: 196   Validation Loss: 9.3279962348938 \n",
      "Epoch: 197   Training Loss: 7.345322432518006 \n",
      "Epoch: 197   Validation Loss: 9.496591053009034 \n",
      "Epoch: 198   Training Loss: 7.4171739211678505 \n",
      "Epoch: 198   Validation Loss: 9.50367875099182 \n",
      "Epoch: 199   Training Loss: 7.350124181210995 \n",
      "Epoch: 199   Validation Loss: 9.393713665008544 \n",
      "Epoch: 200   Training Loss: 7.333595881462097 \n",
      "Epoch: 200   Validation Loss: 9.398368625640869 \n",
      "Epoch: 201   Training Loss: 7.3881527709960935 \n",
      "Epoch: 201   Validation Loss: 9.28110463142395 \n",
      "Epoch: 202   Training Loss: 7.1666830787062645 \n",
      "Epoch: 202   Validation Loss: 9.405767917633057 \n",
      "Epoch: 203   Training Loss: 7.223698004186153 \n",
      "Epoch: 203   Validation Loss: 9.447949123382568 \n",
      "Epoch: 204   Training Loss: 7.3039140902459625 \n",
      "Epoch: 204   Validation Loss: 9.296687107086182 \n",
      "Epoch: 205   Training Loss: 7.354471078515052 \n",
      "Epoch: 205   Validation Loss: 9.568350925445557 \n",
      "Epoch: 206   Training Loss: 7.321462300419808 \n",
      "Epoch: 206   Validation Loss: 9.189311656951904 \n",
      "Epoch: 207   Training Loss: 7.392333776950836 \n",
      "Epoch: 207   Validation Loss: 9.387963027954102 \n",
      "Epoch: 208   Training Loss: 7.247597830295563 \n",
      "Epoch: 208   Validation Loss: 9.568609447479249 \n",
      "Epoch: 209   Training Loss: 7.192465290129185 \n",
      "Epoch: 209   Validation Loss: 9.358354778289796 \n",
      "Epoch: 210   Training Loss: 7.210427152514458 \n",
      "Epoch: 210   Validation Loss: 9.436176471710205 \n",
      "Epoch: 211   Training Loss: 7.229611144065857 \n",
      "Epoch: 211   Validation Loss: 9.555753622055054 \n",
      "Epoch: 212   Training Loss: 7.0073660388588905 \n",
      "Epoch: 212   Validation Loss: 9.702638902664184 \n",
      "Epoch: 213   Training Loss: 6.949171186089516 \n",
      "Epoch: 213   Validation Loss: 9.648703851699828 \n",
      "Epoch: 214   Training Loss: 6.879620463848114 \n",
      "Epoch: 214   Validation Loss: 9.57380145072937 \n",
      "Epoch: 215   Training Loss: 6.752600086182356 \n",
      "Epoch: 215   Validation Loss: 9.702924718856812 \n",
      "Epoch: 216   Training Loss: 6.862256543636322 \n",
      "Epoch: 216   Validation Loss: 9.971997861862183 \n",
      "Epoch: 217   Training Loss: 6.779010165035725 \n",
      "Epoch: 217   Validation Loss: 9.920424642562866 \n",
      "Epoch: 218   Training Loss: 6.917102657854557 \n",
      "Epoch: 218   Validation Loss: 10.047005548477173 \n",
      "Epoch: 219   Training Loss: 6.843038249015808 \n",
      "Epoch: 219   Validation Loss: 10.127423973083497 \n",
      "Epoch: 220   Training Loss: 6.742192811965943 \n",
      "Epoch: 220   Validation Loss: 9.990432205200195 \n",
      "Epoch: 221   Training Loss: 6.690135148912669 \n",
      "Epoch: 221   Validation Loss: 9.946334009170533 \n",
      "Epoch: 222   Training Loss: 6.737390628159046 \n",
      "Epoch: 222   Validation Loss: 10.122861490249633 \n",
      "Epoch: 223   Training Loss: 6.558504461795092 \n",
      "Epoch: 223   Validation Loss: 10.017096900939942 \n",
      "Epoch: 224   Training Loss: 6.633723692595959 \n",
      "Epoch: 224   Validation Loss: 10.149743156433106 \n",
      "Epoch: 225   Training Loss: 6.482425337135791 \n",
      "Epoch: 225   Validation Loss: 9.747160320281983 \n",
      "Epoch: 226   Training Loss: 6.797620553076268 \n",
      "Epoch: 226   Validation Loss: 10.083479223251343 \n",
      "Epoch: 227   Training Loss: 6.621109005212784 \n",
      "Epoch: 227   Validation Loss: 10.103611221313477 \n",
      "Epoch: 228   Training Loss: 6.626382484734059 \n",
      "Epoch: 228   Validation Loss: 10.13721360206604 \n",
      "Epoch: 229   Training Loss: 6.438259480595589 \n",
      "Epoch: 229   Validation Loss: 10.202580099105836 \n",
      "Epoch: 230   Training Loss: 6.6128127789497375 \n",
      "Epoch: 230   Validation Loss: 9.843952846527099 \n",
      "Epoch: 231   Training Loss: 6.547985185682774 \n",
      "Epoch: 231   Validation Loss: 10.042760848999023 \n",
      "Epoch: 232   Training Loss: 6.5390690901875494 \n",
      "Epoch: 232   Validation Loss: 9.89359848022461 \n",
      "Epoch: 233   Training Loss: 6.443627897500992 \n",
      "Epoch: 233   Validation Loss: 10.247184801101685 \n",
      "Epoch: 234   Training Loss: 6.323143739104271 \n",
      "Epoch: 234   Validation Loss: 10.20799518585205 \n",
      "Epoch: 235   Training Loss: 6.270604966580867 \n",
      "Epoch: 235   Validation Loss: 10.179840726852417 \n",
      "Epoch: 236   Training Loss: 6.25102227896452 \n",
      "Epoch: 236   Validation Loss: 9.862135257720947 \n",
      "Epoch: 237   Training Loss: 6.026283023059368 \n",
      "Epoch: 237   Validation Loss: 10.161673946380615 \n",
      "Epoch: 238   Training Loss: 6.212216430604458 \n",
      "Epoch: 238   Validation Loss: 9.857446546554565 \n",
      "Epoch: 239   Training Loss: 6.4176319482922555 \n",
      "Epoch: 239   Validation Loss: 10.286402254104614 \n",
      "Epoch: 240   Training Loss: 6.0066375683248046 \n",
      "Epoch: 240   Validation Loss: 10.468514680862427 \n",
      "Epoch: 241   Training Loss: 5.9384698252379895 \n",
      "Epoch: 241   Validation Loss: 10.291092138290406 \n",
      "Epoch: 242   Training Loss: 5.583237238228321 \n",
      "Epoch: 242   Validation Loss: 10.625640687942505 \n",
      "Epoch: 243   Training Loss: 5.4144115760922435 \n",
      "Epoch: 243   Validation Loss: 10.652989740371703 \n",
      "Epoch: 244   Training Loss: 5.449925103485584 \n",
      "Epoch: 244   Validation Loss: 10.645065298080445 \n",
      "Epoch: 245   Training Loss: 5.647157284319401 \n",
      "Epoch: 245   Validation Loss: 10.498985776901245 \n",
      "Epoch: 246   Training Loss: 5.540897817909718 \n",
      "Epoch: 246   Validation Loss: 10.601727619171143 \n",
      "Epoch: 247   Training Loss: 5.642303368449211 \n",
      "Epoch: 247   Validation Loss: 10.087900896072387 \n",
      "Epoch: 248   Training Loss: 5.764647034108639 \n",
      "Epoch: 248   Validation Loss: 10.774446754455566 \n",
      "Epoch: 249   Training Loss: 5.239088229238987 \n",
      "Epoch: 249   Validation Loss: 10.777367162704468 \n",
      "Epoch: 250   Training Loss: 5.026637407243252 \n",
      "Epoch: 250   Validation Loss: 10.8023384475708 \n",
      "Epoch: 251   Training Loss: 4.873252348899841 \n",
      "Epoch: 251   Validation Loss: 10.742659072875977 \n",
      "Epoch: 252   Training Loss: 5.009702874720096 \n",
      "Epoch: 252   Validation Loss: 10.702509498596191 \n",
      "Epoch: 253   Training Loss: 4.934190970659256 \n",
      "Epoch: 253   Validation Loss: 10.976671123504639 \n",
      "Epoch: 254   Training Loss: 5.087642734646797 \n",
      "Epoch: 254   Validation Loss: 10.910531215667724 \n",
      "Epoch: 255   Training Loss: 5.138409589529037 \n",
      "Epoch: 255   Validation Loss: 10.718941516876221 \n",
      "Epoch: 256   Training Loss: 5.023282598853111 \n",
      "Epoch: 256   Validation Loss: 10.864779691696167 \n",
      "Epoch: 257   Training Loss: 4.865723013430834 \n",
      "Epoch: 257   Validation Loss: 10.715290470123291 \n",
      "Epoch: 258   Training Loss: 4.855053709745407 \n",
      "Epoch: 258   Validation Loss: 11.117820949554444 \n",
      "Epoch: 259   Training Loss: 4.745627456903458 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 259   Validation Loss: 11.329025077819825 \n",
      "Epoch: 260   Training Loss: 4.570484665036202 \n",
      "Epoch: 260   Validation Loss: 11.138452339172364 \n",
      "Epoch: 261   Training Loss: 4.479416221678257 \n",
      "Epoch: 261   Validation Loss: 11.055455074310302 \n",
      "Epoch: 262   Training Loss: 4.50043534964323 \n",
      "Epoch: 262   Validation Loss: 11.096573181152344 \n",
      "Epoch: 263   Training Loss: 4.244444440603257 \n",
      "Epoch: 263   Validation Loss: 10.962744045257569 \n",
      "Epoch: 264   Training Loss: 4.444811956882477 \n",
      "Epoch: 264   Validation Loss: 11.06393310546875 \n",
      "Epoch: 265   Training Loss: 4.151600767970085 \n",
      "Epoch: 265   Validation Loss: 11.214322090148926 \n",
      "Epoch: 266   Training Loss: 4.004123290777207 \n",
      "Epoch: 266   Validation Loss: 11.150833435058594 \n",
      "Epoch: 267   Training Loss: 4.125936575531959 \n",
      "Epoch: 267   Validation Loss: 11.319279556274415 \n",
      "Epoch: 268   Training Loss: 3.7784598967432976 \n",
      "Epoch: 268   Validation Loss: 11.658071212768554 \n",
      "Epoch: 269   Training Loss: 3.8922485002875327 \n",
      "Epoch: 269   Validation Loss: 11.888335723876953 \n",
      "Epoch: 270   Training Loss: 3.7143446359038355 \n",
      "Epoch: 270   Validation Loss: 12.092546272277833 \n",
      "Epoch: 271   Training Loss: 3.4952313324809072 \n",
      "Epoch: 271   Validation Loss: 12.040752410888672 \n",
      "Epoch: 272   Training Loss: 3.3281919410824776 \n",
      "Epoch: 272   Validation Loss: 11.922865905761718 \n",
      "Epoch: 273   Training Loss: 3.1901836556196215 \n",
      "Epoch: 273   Validation Loss: 12.413699378967285 \n",
      "Epoch: 274   Training Loss: 3.1781594863533975 \n",
      "Epoch: 274   Validation Loss: 11.48137336730957 \n",
      "Epoch: 275   Training Loss: 3.1397185054421426 \n",
      "Epoch: 275   Validation Loss: 11.852532863616943 \n",
      "Epoch: 276   Training Loss: 3.2092212530970574 \n",
      "Epoch: 276   Validation Loss: 12.209631576538087 \n",
      "Epoch: 277   Training Loss: 2.988604208827019 \n",
      "Epoch: 277   Validation Loss: 11.940191459655761 \n",
      "Epoch: 278   Training Loss: 2.6581914073228834 \n",
      "Epoch: 278   Validation Loss: 12.05429084777832 \n",
      "Epoch: 279   Training Loss: 2.468708312809467 \n",
      "Epoch: 279   Validation Loss: 11.71886360168457 \n",
      "Epoch: 280   Training Loss: 2.389005695730448 \n",
      "Epoch: 280   Validation Loss: 11.496532688140869 \n",
      "Epoch: 281   Training Loss: 2.315585045814514 \n",
      "Epoch: 281   Validation Loss: 11.360607185363769 \n",
      "Epoch: 282   Training Loss: 2.315631548166275 \n",
      "Epoch: 282   Validation Loss: 11.913507461547852 \n",
      "Epoch: 283   Training Loss: 2.264470830261707 \n",
      "Epoch: 283   Validation Loss: 12.258944702148437 \n",
      "Epoch: 284   Training Loss: 2.1765472170710565 \n",
      "Epoch: 284   Validation Loss: 12.197515316009522 \n",
      "Epoch: 285   Training Loss: 2.133119527101517 \n",
      "Epoch: 285   Validation Loss: 12.559165821075439 \n",
      "Epoch: 286   Training Loss: 2.1762694084644316 \n",
      "Epoch: 286   Validation Loss: 12.382972774505616 \n",
      "Epoch: 287   Training Loss: 2.0364331007003784 \n",
      "Epoch: 287   Validation Loss: 12.598233623504639 \n",
      "Epoch: 288   Training Loss: 2.0165707229077814 \n",
      "Epoch: 288   Validation Loss: 12.717239627838135 \n",
      "Epoch: 289   Training Loss: 1.8774133624136449 \n",
      "Epoch: 289   Validation Loss: 12.732840118408204 \n",
      "Epoch: 290   Training Loss: 1.7834532496333122 \n",
      "Epoch: 290   Validation Loss: 12.308268356323243 \n",
      "Epoch: 291   Training Loss: 1.650608928054571 \n",
      "Epoch: 291   Validation Loss: 12.784120483398437 \n",
      "Epoch: 292   Training Loss: 1.464185142070055 \n",
      "Epoch: 292   Validation Loss: 12.504629611968994 \n",
      "Epoch: 293   Training Loss: 1.2655625522136689 \n",
      "Epoch: 293   Validation Loss: 12.846351318359375 \n",
      "Epoch: 294   Training Loss: 1.294068044871092 \n",
      "Epoch: 294   Validation Loss: 12.988740425109864 \n",
      "Epoch: 295   Training Loss: 1.2563777054846286 \n",
      "Epoch: 295   Validation Loss: 12.800129089355469 \n",
      "Epoch: 296   Training Loss: 1.6327417817711831 \n",
      "Epoch: 296   Validation Loss: 13.057836399078369 \n",
      "Epoch: 297   Training Loss: 1.2771623726189136 \n",
      "Epoch: 297   Validation Loss: 12.73526912689209 \n",
      "Epoch: 298   Training Loss: 1.2181182011961937 \n",
      "Epoch: 298   Validation Loss: 12.955740585327149 \n",
      "Epoch: 299   Training Loss: 1.0644964073598384 \n",
      "Epoch: 299   Validation Loss: 12.89338544845581 \n",
      "Epoch: 300   Training Loss: 0.9245576966553927 \n",
      "Epoch: 300   Validation Loss: 12.354677333831788 \n",
      "Epoch: 301   Training Loss: 0.9312947917729616 \n",
      "Epoch: 301   Validation Loss: 12.380990295410156 \n",
      "Epoch: 302   Training Loss: 0.9514601119607687 \n",
      "Epoch: 302   Validation Loss: 12.542759799957276 \n",
      "Epoch: 303   Training Loss: 0.9232106297463178 \n",
      "Epoch: 303   Validation Loss: 12.32331735610962 \n",
      "Epoch: 304   Training Loss: 1.009696018397808 \n",
      "Epoch: 304   Validation Loss: 12.10626745223999 \n",
      "Epoch: 305   Training Loss: 1.1147463100403547 \n",
      "Epoch: 305   Validation Loss: 12.414883937835693 \n",
      "Epoch: 306   Training Loss: 1.0494360307604074 \n",
      "Epoch: 306   Validation Loss: 11.97722490310669 \n",
      "Epoch: 307   Training Loss: 0.7820057861506939 \n",
      "Epoch: 307   Validation Loss: 12.289982433319091 \n",
      "Epoch: 308   Training Loss: 0.6639844363927841 \n",
      "Epoch: 308   Validation Loss: 12.286580200195312 \n",
      "Epoch: 309   Training Loss: 0.5268681428208947 \n",
      "Epoch: 309   Validation Loss: 12.082977714538574 \n",
      "Epoch: 310   Training Loss: 0.44396137837320565 \n",
      "Epoch: 310   Validation Loss: 12.249852771759032 \n",
      "Epoch: 311   Training Loss: 0.6338246103003621 \n",
      "Epoch: 311   Validation Loss: 12.281773777008057 \n",
      "Epoch: 312   Training Loss: 0.7416135681420565 \n",
      "Epoch: 312   Validation Loss: 12.043081283569336 \n",
      "Epoch: 313   Training Loss: 0.4247068898379803 \n",
      "Epoch: 313   Validation Loss: 12.17598066329956 \n",
      "Epoch: 314   Training Loss: 0.3028272655233741 \n",
      "Epoch: 314   Validation Loss: 11.858341455459595 \n",
      "Epoch: 315   Training Loss: 0.2924918534234166 \n",
      "Epoch: 315   Validation Loss: 11.94011344909668 \n",
      "Epoch: 316   Training Loss: 0.35861146468669175 \n",
      "Epoch: 316   Validation Loss: 11.43459831237793 \n",
      "Epoch: 317   Training Loss: 0.5571459268778562 \n",
      "Epoch: 317   Validation Loss: 11.774785995483398 \n",
      "Epoch: 318   Training Loss: 0.3510802353546023 \n",
      "Epoch: 318   Validation Loss: 11.787607040405273 \n",
      "Epoch: 319   Training Loss: 0.2586232388950884 \n",
      "Epoch: 319   Validation Loss: 11.29520700454712 \n",
      "Epoch: 320   Training Loss: 0.333906957115978 \n",
      "Epoch: 320   Validation Loss: 11.44035011291504 \n",
      "Epoch: 321   Training Loss: 0.35291628766804933 \n",
      "Epoch: 321   Validation Loss: 11.526096286773681 \n",
      "Epoch: 322   Training Loss: 0.4257013411074877 \n",
      "Epoch: 322   Validation Loss: 11.918661241531373 \n",
      "Epoch: 323   Training Loss: 0.5215036030858755 \n",
      "Epoch: 323   Validation Loss: 11.640996685028076 \n",
      "Epoch: 324   Training Loss: 0.29954702105373143 \n",
      "Epoch: 324   Validation Loss: 11.516510257720947 \n",
      "Epoch: 325   Training Loss: 0.20197844771668316 \n",
      "Epoch: 325   Validation Loss: 11.419821071624757 \n",
      "Epoch: 326   Training Loss: 0.15318849673494697 \n",
      "Epoch: 326   Validation Loss: 11.291015653610229 \n",
      "Epoch: 327   Training Loss: 0.08977519873529673 \n",
      "Epoch: 327   Validation Loss: 11.200008907318114 \n",
      "Epoch: 328   Training Loss: 0.06294882327318191 \n",
      "Epoch: 328   Validation Loss: 11.227215566635131 \n",
      "Epoch: 329   Training Loss: 0.049295391784980894 \n",
      "Epoch: 329   Validation Loss: 11.341818780899048 \n",
      "Epoch: 330   Training Loss: 0.041947468016296625 \n",
      "Epoch: 330   Validation Loss: 11.352310466766358 \n",
      "Epoch: 331   Training Loss: 0.034861666448414326 \n",
      "Epoch: 331   Validation Loss: 11.379394121170044 \n",
      "Epoch: 332   Training Loss: 0.03001409800257534 \n",
      "Epoch: 332   Validation Loss: 11.436930646896363 \n",
      "Epoch: 333   Training Loss: 0.025917951944284142 \n",
      "Epoch: 333   Validation Loss: 11.436274690628052 \n",
      "Epoch: 334   Training Loss: 0.023223963845521212 \n",
      "Epoch: 334   Validation Loss: 11.36275526046753 \n",
      "Epoch: 335   Training Loss: 0.021066053165122868 \n",
      "Epoch: 335   Validation Loss: 11.339413919448852 \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, truths)\n\u001b[0;32m---> 39\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss / Train\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, epoch) \u001b[38;5;66;03m# adds training loss scalar\u001b[39;00m\n\u001b[1;32m     40\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     41\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:388\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaffe2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m workspace\n\u001b[1;32m    386\u001b[0m     scalar_value \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mFetchBlob(scalar_value)\n\u001b[0;32m--> 388\u001b[0m summary \u001b[38;5;241m=\u001b[39m scalar(\n\u001b[1;32m    389\u001b[0m     tag, scalar_value, new_style\u001b[38;5;241m=\u001b[39mnew_style, double_precision\u001b[38;5;241m=\u001b[39mdouble_precision\n\u001b[1;32m    390\u001b[0m )\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(summary, global_step, walltime)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/tensorboard/summary.py:280\u001b[0m, in \u001b[0;36mscalar\u001b[0;34m(name, tensor, collections, new_style, double_precision)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscalar\u001b[39m(name, tensor, collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, new_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, double_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;124;03m\"\"\"Outputs a `Summary` protocol buffer containing a single scalar value.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    The generated Summary has a Tensor.proto containing the input Tensor.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m      ValueError: If tensor has the wrong shape or type.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m make_np(tensor)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    282\u001b[0m         tensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    283\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor should contain one element (0 dimensions). Was given size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# python float is double precision in numpy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/tensorboard/_convert_np.py:23\u001b[0m, in \u001b[0;36mmake_np\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([x])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _prepare_pytorch(x)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, but numpy array, torch tensor, or caffe2 blob name are expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mtype\u001b[39m(x)\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/tensorboard/_convert_np.py:32\u001b[0m, in \u001b[0;36m_prepare_pytorch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_pytorch\u001b[39m(x):\n\u001b[0;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Create writer and profiler to analyze loss over each epoch\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Set device to CUDA if available, initialize model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "net = generate_model(50)\n",
    "net.to(device)\n",
    "\n",
    "# Set up optimizer and loss function, set number of epochs\n",
    "optimizer = optim.SGD(net.parameters(), lr = 5e-2, momentum=0.9, weight_decay = 0)\n",
    "criterion = nn.MSELoss(reduction = 'mean')\n",
    "criterion.to(device)\n",
    "num_epochs = 500\n",
    "\n",
    "# Iniitializing variables to show statistics\n",
    "iteration = 0\n",
    "test_iteration = 0\n",
    "loss_list = []\n",
    "test_loss_list = []\n",
    "epoch_loss_averages = []\n",
    "test_epoch_loss_averages = []\n",
    "\n",
    "# Iterates over dataset multiple times\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    test_epoch_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(trainset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(data[0]).to(device)), torch.from_numpy(data[1]).to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs).to(device)\n",
    "        loss = criterion(outputs, truths)\n",
    "        writer.add_scalar(\"Loss / Train\", loss, epoch) # adds training loss scalar\n",
    "        loss_list.append(loss.cpu().detach().numpy())\n",
    "        epoch_loss += loss.cpu().detach().numpy()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration += 1\n",
    "        if iteration % trainset.shape[0] == 0:\n",
    "            epoch_loss_averages.append(epoch_loss / trainset.shape[0])\n",
    "            print('Epoch: {}   Training Loss: {} '.format(epoch, epoch_loss / trainset.shape[0]))\n",
    "            \n",
    "    for i, test_data in enumerate(testset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(test_data[0]).to(device)), torch.from_numpy(test_data[1]).to(device).float()\n",
    "        outputs = net(inputs).to(device)\n",
    "        test_loss = criterion(outputs, truths)\n",
    "        \n",
    "        writer.add_scalar(\"Loss / Test\", test_loss, epoch) # adds testing loss scalar\n",
    "        test_loss_list.append(test_loss.cpu().detach().numpy())\n",
    "        test_epoch_loss += test_loss.cpu().detach().numpy()\n",
    "        \n",
    "        test_iteration +=1\n",
    "        if test_iteration % testset.shape[0] == 0:\n",
    "            test_epoch_loss_averages.append(test_epoch_loss / testset.shape[0])\n",
    "            print('Epoch: {}   Validation Loss: {} '.format(epoch, test_epoch_loss / testset.shape[0]))\n",
    "            \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot epoch loss to test for convergence\n",
    "plt.plot(epoch_loss_averages)\n",
    "plt.plot(test_epoch_loss_averages)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716136ea",
   "metadata": {},
   "source": [
    "As we can see, a small dataset shows convergence which means this network MAY be able to be a solution. The next step is to test on a significantly larger dataset to see if this can deal with the overfitting problem. If the large set has convergence in the training data with a larger set then the overfitting must be dealt with in another way"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
