{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1586819",
   "metadata": {},
   "source": [
    "# 3D Convolutional Neural Network for Tomographic Alignment\n",
    "\n",
    "## Regular CNN Expanding the Dataset\n",
    "\n",
    "In order to test potential methods for performing automatic tomographic alignment using neural networks, we start with a standard model using a three dimensional convolution. The main problem with tomographic alignment is that a stack of two dimensional projections have to be processed simultameously for optimal results. While two dimensional convolutions can be used with channels corresponding with each projection angle, this is likely similar in computational complexity to a three dimensional neural network. Therefore the approach for this test is similar to video classification, where each frame in a video is is instead each projection angle. Now the dataset will be expanded to be larger in order to determine if this can be generalized on the phantom data.\n",
    "\n",
    "In order to test if this method can provide a convergence, phantoms will be artificially misaligned to create a training and testing set. But first packages for tomography, image transformations, and neural networks have to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1cdf4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential packages\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import tomography and imaging packages\n",
    "import tomopy\n",
    "from skimage.transform import rotate, AffineTransform\n",
    "from skimage import transform as tf\n",
    "\n",
    "# Import neural net packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.profiler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d73afe",
   "metadata": {},
   "source": [
    "Since the model will be a computationally complex CNN, we must ensure that the GPU is being used for calculations or else computation will be far too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0b669c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Environment: pytorch\n",
      "Cuda Version: 11.8\n",
      "Cuda Availability: True\n"
     ]
    }
   ],
   "source": [
    "# Checking to ensure environment and cuda are correct\n",
    "print(\"Working Environment: {}\".format(os.environ['CONDA_DEFAULT_ENV']))\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"Cuda Version: {}\".format(torch.version.cuda))\n",
    "print(\"Cuda Availability: {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73daf4d6",
   "metadata": {},
   "source": [
    "Now that the packages have been imported and CUDA is set up correctly, the next step is to create the dataset to be used for training and testing the neural network. The misalignment function is created to perform different random misalignments on the phantom set of tomographic scans. The shape of all of the data is then checked for errors and the data is split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d7e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for artificial misalignment\n",
    "def misalign(prj, mis_axis, ang_tilt = False, noise = False, background = False):\n",
    "    num_prj, col, row = prj.shape\n",
    "    dx = mis_axis[:, 0]\n",
    "    dy = mis_axis[:, 1]\n",
    "    prj_tmp = tomopy.shift_images(prj, dx, dy)\n",
    "    \n",
    "    for i in range(num_prj):\n",
    "        d_row, d_col, d_ang = mis_axis[i]\n",
    "        \n",
    "        if ang_tilt == True:\n",
    "            prj_tmp[i, :, :] = rotate(prj[i,:,:], d_ang)\n",
    "        else:\n",
    "            prj_tmp[i, :, :] = prj[i,:,:]\n",
    "        \n",
    "        if noise == True:\n",
    "            prj_tmp[i, :, :] = random_noise(prj_tmp[i, :, :], mode = 'gaussian')\n",
    "            \n",
    "        if background == True:\n",
    "            prj_tmp[i, :, :] = prj_tmp[i, :, :]+np.random.random()/5\n",
    "            prj_tmp[i, :, :] = prj_tmp[i, :, :]/prj_tmp[i, :, :].max()  \n",
    "            \n",
    "    return prj_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3b066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating ground truth tomography\n",
    "data = tomopy.shepp3d(128)\n",
    "ang = tomopy.angles(180)\n",
    "proj = tomopy.project(data, ang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a796e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset to store misaligned projections and \n",
    "training_entries = 100\n",
    "entries = int(np.ceil(training_entries * 5 / 4))\n",
    "dataset = np.zeros((entries, 2), dtype = object)\n",
    "\n",
    "for i in range(entries):\n",
    "    # Randomly determined misalignment axis\n",
    "    mis_axis = np.random.normal(0, 1, (200, 3))\n",
    "    mis_axis[:, :1] = mis_axis[:, :1]*4\n",
    "    mis_axis = np.round(mis_axis).astype(int)\n",
    "    mis_axis_in = np.expand_dims(mis_axis, axis = 0)\n",
    "    \n",
    "    proj_mis = misalign(proj.copy(), mis_axis, ang_tilt = True)\n",
    "    proj_mis = np.expand_dims(proj_mis, axis = 0)\n",
    "    proj_mis = np.expand_dims(proj_mis, axis = 0)\n",
    "    \n",
    "    dataset[i, 0] = proj_mis\n",
    "    dataset[i, 1] = np.concatenate((mis_axis_in[:, :180, 0], mis_axis_in[:, :180, 1]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd11e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 2)\n",
      "(2,)\n",
      "(1, 1, 180, 128, 184)\n",
      "(1, 360)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of dataset\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb36ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(25, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of training and testing splits\n",
    "trainset, testset = np.split(dataset, [training_entries])\n",
    "print(trainset.shape)\n",
    "print(testset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b55a9d1",
   "metadata": {},
   "source": [
    "Now that the data has been set up, the CUDA cache should be cleared and the model will be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dfd20fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared Cache.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared Cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7319a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def norm(proj):\n",
    "    proj = (proj - torch.min(proj)) / (torch.max(proj) - torch.min(proj))\n",
    "    return proj\n",
    "\n",
    "# 3D CNN to determine shift parameters\n",
    "\n",
    "class CNN_3D_aligner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_3D_aligner, self).__init__()\n",
    "\n",
    "        self.group1 = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group2 = nn.Sequential(\n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group3 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.Conv3d(64, 64, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group4 = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.Conv3d(128, 128, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group5 = nn.Sequential(\n",
    "            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=1), \n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)) \n",
    "        )\n",
    "        \n",
    "        self.group6 = nn.Sequential(\n",
    "            nn.Conv3d(256, 16, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.BatchNorm3d(16),\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1600, 512),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 360)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = norm(x)\n",
    "        \n",
    "        x = self.group1(x)\n",
    "        x = self.group2(x)\n",
    "        x = self.group3(x)\n",
    "        x = self.group4(x)\n",
    "        x = self.group5(x)\n",
    "        x = self.group6(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f078f4",
   "metadata": {},
   "source": [
    "In order to ensure the network works and understand its structure before training data on it, use the summary function in order to get an understanding of the network and fix any linear algebra errors in creating the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78be5986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN_3D_aligner                           [1, 360]                  --\n",
       "├─Sequential: 1-1                        [1, 16, 90, 64, 92]       --\n",
       "│    └─Conv3d: 2-1                       [1, 16, 180, 128, 184]    448\n",
       "│    └─BatchNorm3d: 2-2                  [1, 16, 180, 128, 184]    32\n",
       "│    └─MaxPool3d: 2-3                    [1, 16, 90, 64, 92]       --\n",
       "├─Sequential: 1-2                        [1, 32, 45, 32, 46]       --\n",
       "│    └─Conv3d: 2-4                       [1, 32, 90, 64, 92]       13,856\n",
       "│    └─BatchNorm3d: 2-5                  [1, 32, 90, 64, 92]       64\n",
       "│    └─MaxPool3d: 2-6                    [1, 32, 45, 32, 46]       --\n",
       "├─Sequential: 1-3                        [1, 64, 22, 16, 23]       --\n",
       "│    └─Conv3d: 2-7                       [1, 64, 45, 32, 46]       55,360\n",
       "│    └─BatchNorm3d: 2-8                  [1, 64, 45, 32, 46]       128\n",
       "│    └─Conv3d: 2-9                       [1, 64, 45, 32, 46]       110,656\n",
       "│    └─BatchNorm3d: 2-10                 [1, 64, 45, 32, 46]       128\n",
       "│    └─MaxPool3d: 2-11                   [1, 64, 22, 16, 23]       --\n",
       "├─Sequential: 1-4                        [1, 128, 11, 8, 11]       --\n",
       "│    └─Conv3d: 2-12                      [1, 128, 22, 16, 23]      221,312\n",
       "│    └─BatchNorm3d: 2-13                 [1, 128, 22, 16, 23]      256\n",
       "│    └─Conv3d: 2-14                      [1, 128, 22, 16, 23]      442,496\n",
       "│    └─BatchNorm3d: 2-15                 [1, 128, 22, 16, 23]      256\n",
       "│    └─MaxPool3d: 2-16                   [1, 128, 11, 8, 11]       --\n",
       "├─Sequential: 1-5                        [1, 256, 5, 4, 5]         --\n",
       "│    └─Conv3d: 2-17                      [1, 256, 11, 8, 11]       884,992\n",
       "│    └─BatchNorm3d: 2-18                 [1, 256, 11, 8, 11]       512\n",
       "│    └─Conv3d: 2-19                      [1, 256, 11, 8, 11]       1,769,728\n",
       "│    └─BatchNorm3d: 2-20                 [1, 256, 11, 8, 11]       512\n",
       "│    └─MaxPool3d: 2-21                   [1, 256, 5, 4, 5]         --\n",
       "├─Sequential: 1-6                        [1, 16, 5, 4, 5]          --\n",
       "│    └─Conv3d: 2-22                      [1, 16, 5, 4, 5]          110,608\n",
       "│    └─BatchNorm3d: 2-23                 [1, 16, 5, 4, 5]          32\n",
       "├─Flatten: 1-7                           [1, 1600]                 --\n",
       "├─Sequential: 1-8                        [1, 256]                  --\n",
       "│    └─Linear: 2-24                      [1, 512]                  819,712\n",
       "│    └─Dropout: 2-25                     [1, 512]                  --\n",
       "│    └─Linear: 2-26                      [1, 256]                  131,328\n",
       "├─Linear: 1-9                            [1, 360]                  92,520\n",
       "==========================================================================================\n",
       "Total params: 4,654,936\n",
       "Trainable params: 4,654,936\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 28.19\n",
       "==========================================================================================\n",
       "Input size (MB): 16.96\n",
       "Forward/backward pass size (MB): 1533.38\n",
       "Params size (MB): 18.62\n",
       "Estimated Total Size (MB): 1568.96\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model shape\n",
    "model = CNN_3D_aligner()\n",
    "summary(model, (1, 1, 180, 128, 184))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae80e4",
   "metadata": {},
   "source": [
    "Now the model can be trained, making sure to move all of the elements of the training process to the GPU to optimize computational speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7b99b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Epoch: 0   Training Loss: 8.793318123817444 \n",
      "Epoch: 0   Testing Loss: 8.325324363708496 \n",
      "Epoch: 1   Training Loss: 8.734093523025512 \n",
      "Epoch: 1   Testing Loss: 8.32858507156372 \n",
      "Epoch: 2   Training Loss: 8.69584065437317 \n",
      "Epoch: 2   Testing Loss: 8.359187068939208 \n",
      "Epoch: 3   Training Loss: 8.666551084518433 \n",
      "Epoch: 3   Testing Loss: 8.350412044525147 \n",
      "Epoch: 4   Training Loss: 8.619072303771972 \n",
      "Epoch: 4   Testing Loss: 8.363710250854492 \n",
      "Epoch: 5   Training Loss: 8.52696367263794 \n",
      "Epoch: 5   Testing Loss: 8.3968967628479 \n",
      "Epoch: 6   Training Loss: 8.356963062286377 \n",
      "Epoch: 6   Testing Loss: 8.38445592880249 \n",
      "Epoch: 7   Training Loss: 8.12868685245514 \n",
      "Epoch: 7   Testing Loss: 8.354944267272948 \n",
      "Epoch: 8   Training Loss: 7.92053183555603 \n",
      "Epoch: 8   Testing Loss: 8.345095691680909 \n",
      "Epoch: 9   Training Loss: 7.775568113327027 \n",
      "Epoch: 9   Testing Loss: 8.323350601196289 \n",
      "Epoch: 10   Training Loss: 7.679182415008545 \n",
      "Epoch: 10   Testing Loss: 8.332823944091796 \n",
      "Epoch: 11   Training Loss: 7.5542471218109135 \n",
      "Epoch: 11   Testing Loss: 8.33355136871338 \n",
      "Epoch: 12   Training Loss: 7.426547975540161 \n",
      "Epoch: 12   Testing Loss: 8.342337245941161 \n",
      "Epoch: 13   Training Loss: 7.310591349601745 \n",
      "Epoch: 13   Testing Loss: 8.342495460510253 \n",
      "Epoch: 14   Training Loss: 7.198400754928588 \n",
      "Epoch: 14   Testing Loss: 8.331716747283936 \n",
      "Epoch: 15   Training Loss: 7.08318377494812 \n",
      "Epoch: 15   Testing Loss: 8.34995475769043 \n",
      "Epoch: 16   Training Loss: 6.976008739471435 \n",
      "Epoch: 16   Testing Loss: 8.354268436431886 \n",
      "Epoch: 17   Training Loss: 6.8748026466369625 \n",
      "Epoch: 17   Testing Loss: 8.35254415512085 \n",
      "Epoch: 18   Training Loss: 6.770883269309998 \n",
      "Epoch: 18   Testing Loss: 8.349498786926269 \n",
      "Epoch: 19   Training Loss: 6.673019623756408 \n",
      "Epoch: 19   Testing Loss: 8.367233352661133 \n",
      "Epoch: 20   Training Loss: 6.568920850753784 \n",
      "Epoch: 20   Testing Loss: 8.375310897827148 \n",
      "Epoch: 21   Training Loss: 6.461934962272644 \n",
      "Epoch: 21   Testing Loss: 8.394963779449462 \n",
      "Epoch: 22   Training Loss: 6.359052610397339 \n",
      "Epoch: 22   Testing Loss: 8.418314151763916 \n",
      "Epoch: 23   Training Loss: 6.2616833257675175 \n",
      "Epoch: 23   Testing Loss: 8.398665466308593 \n",
      "Epoch: 24   Training Loss: 6.184883308410645 \n",
      "Epoch: 24   Testing Loss: 8.404971237182616 \n",
      "Epoch: 25   Training Loss: 6.050414981842041 \n",
      "Epoch: 25   Testing Loss: 8.40556999206543 \n",
      "Epoch: 26   Training Loss: 5.96661913394928 \n",
      "Epoch: 26   Testing Loss: 8.399461479187012 \n",
      "Epoch: 27   Training Loss: 5.857965188026428 \n",
      "Epoch: 27   Testing Loss: 8.435967655181885 \n",
      "Epoch: 28   Training Loss: 5.781147603988647 \n",
      "Epoch: 28   Testing Loss: 8.4253879737854 \n",
      "Epoch: 29   Training Loss: 5.656187558174134 \n",
      "Epoch: 29   Testing Loss: 8.437251319885254 \n",
      "Epoch: 30   Training Loss: 5.553956990242004 \n",
      "Epoch: 30   Testing Loss: 8.44730432510376 \n",
      "Epoch: 31   Training Loss: 5.459728555679321 \n",
      "Epoch: 31   Testing Loss: 8.423367290496826 \n",
      "Epoch: 32   Training Loss: 5.361843056678772 \n",
      "Epoch: 32   Testing Loss: 8.41973659515381 \n",
      "Epoch: 33   Training Loss: 5.251159715652466 \n",
      "Epoch: 33   Testing Loss: 8.455441360473634 \n",
      "Epoch: 34   Training Loss: 5.166071186065674 \n",
      "Epoch: 34   Testing Loss: 8.484492797851562 \n",
      "Epoch: 35   Training Loss: 5.059201021194458 \n",
      "Epoch: 35   Testing Loss: 8.427311267852783 \n",
      "Epoch: 36   Training Loss: 4.94561981678009 \n",
      "Epoch: 36   Testing Loss: 8.460693435668945 \n",
      "Epoch: 37   Training Loss: 4.8713849472999575 \n",
      "Epoch: 37   Testing Loss: 8.452931804656982 \n",
      "Epoch: 38   Training Loss: 4.780325047969818 \n",
      "Epoch: 38   Testing Loss: 8.479495582580567 \n",
      "Epoch: 39   Training Loss: 4.68877956867218 \n",
      "Epoch: 39   Testing Loss: 8.452060298919678 \n",
      "Epoch: 40   Training Loss: 4.579373028278351 \n",
      "Epoch: 40   Testing Loss: 8.44970775604248 \n",
      "Epoch: 41   Training Loss: 4.49466912984848 \n",
      "Epoch: 41   Testing Loss: 8.537133865356445 \n",
      "Epoch: 42   Training Loss: 4.394843573570252 \n",
      "Epoch: 42   Testing Loss: 8.548899898529053 \n",
      "Epoch: 43   Training Loss: 4.295682947635651 \n",
      "Epoch: 43   Testing Loss: 8.502318210601807 \n",
      "Epoch: 44   Training Loss: 4.216442067623138 \n",
      "Epoch: 44   Testing Loss: 8.578767604827881 \n",
      "Epoch: 45   Training Loss: 4.136173241138458 \n",
      "Epoch: 45   Testing Loss: 8.577309131622314 \n",
      "Epoch: 46   Training Loss: 4.076196124553681 \n",
      "Epoch: 46   Testing Loss: 8.541141471862794 \n",
      "Epoch: 47   Training Loss: 4.006234905719757 \n",
      "Epoch: 47   Testing Loss: 8.577635307312011 \n",
      "Epoch: 48   Training Loss: 3.909734098911285 \n",
      "Epoch: 48   Testing Loss: 8.614783306121826 \n",
      "Epoch: 49   Training Loss: 3.8913978290557862 \n",
      "Epoch: 49   Testing Loss: 8.545002670288087 \n",
      "Epoch: 50   Training Loss: 3.8272038006782534 \n",
      "Epoch: 50   Testing Loss: 8.567675132751464 \n",
      "Epoch: 51   Training Loss: 3.7188504934310913 \n",
      "Epoch: 51   Testing Loss: 8.557644386291503 \n",
      "Epoch: 52   Training Loss: 3.6241546726226805 \n",
      "Epoch: 52   Testing Loss: 8.683408069610596 \n",
      "Epoch: 53   Training Loss: 3.5391357374191283 \n",
      "Epoch: 53   Testing Loss: 8.6011381149292 \n",
      "Epoch: 54   Training Loss: 3.4811334729194643 \n",
      "Epoch: 54   Testing Loss: 8.560552387237548 \n",
      "Epoch: 55   Training Loss: 3.4073798990249635 \n",
      "Epoch: 55   Testing Loss: 8.726902561187744 \n",
      "Epoch: 56   Training Loss: 3.3510935497283936 \n",
      "Epoch: 56   Testing Loss: 8.646647129058838 \n",
      "Epoch: 57   Training Loss: 3.278052821159363 \n",
      "Epoch: 57   Testing Loss: 8.58439073562622 \n",
      "Epoch: 58   Training Loss: 3.210248248577118 \n",
      "Epoch: 58   Testing Loss: 8.675149784088134 \n",
      "Epoch: 59   Training Loss: 3.1760900020599365 \n",
      "Epoch: 59   Testing Loss: 8.47464153289795 \n",
      "Epoch: 60   Training Loss: 3.114510064125061 \n",
      "Epoch: 60   Testing Loss: 8.550942058563232 \n",
      "Epoch: 61   Training Loss: 3.0525191807746888 \n",
      "Epoch: 61   Testing Loss: 8.630702838897705 \n",
      "Epoch: 62   Training Loss: 3.005647883415222 \n",
      "Epoch: 62   Testing Loss: 8.929776401519776 \n",
      "Epoch: 63   Training Loss: 2.9906859517097475 \n",
      "Epoch: 63   Testing Loss: 8.776176452636719 \n",
      "Epoch: 64   Training Loss: 2.947494306564331 \n",
      "Epoch: 64   Testing Loss: 8.626161632537842 \n",
      "Epoch: 65   Training Loss: 2.869493510723114 \n",
      "Epoch: 65   Testing Loss: 8.589286270141601 \n",
      "Epoch: 66   Training Loss: 2.828857595920563 \n",
      "Epoch: 66   Testing Loss: 8.687296543121338 \n",
      "Epoch: 67   Training Loss: 2.788447265625 \n",
      "Epoch: 67   Testing Loss: 8.787301082611084 \n",
      "Epoch: 68   Training Loss: 2.748495645523071 \n",
      "Epoch: 68   Testing Loss: 8.76719051361084 \n",
      "Epoch: 69   Training Loss: 2.6943607902526856 \n",
      "Epoch: 69   Testing Loss: 8.705702724456787 \n",
      "Epoch: 70   Training Loss: 2.6535403108596802 \n",
      "Epoch: 70   Testing Loss: 8.684647483825684 \n",
      "Epoch: 71   Training Loss: 2.574784436225891 \n",
      "Epoch: 71   Testing Loss: 8.742929611206055 \n",
      "Epoch: 72   Training Loss: 2.5436961960792543 \n",
      "Epoch: 72   Testing Loss: 8.679965801239014 \n",
      "Epoch: 73   Training Loss: 2.510637080669403 \n",
      "Epoch: 73   Testing Loss: 8.77957908630371 \n",
      "Epoch: 74   Training Loss: 2.4646423292160033 \n",
      "Epoch: 74   Testing Loss: 8.77864866256714 \n",
      "Epoch: 75   Training Loss: 2.403769084215164 \n",
      "Epoch: 75   Testing Loss: 8.772509613037109 \n",
      "Epoch: 76   Training Loss: 2.4137413907051086 \n",
      "Epoch: 76   Testing Loss: 8.802531299591065 \n",
      "Epoch: 77   Training Loss: 2.3942812407016754 \n",
      "Epoch: 77   Testing Loss: 9.02314121246338 \n",
      "Epoch: 78   Training Loss: 2.356697437763214 \n",
      "Epoch: 78   Testing Loss: 8.738793468475341 \n",
      "Epoch: 79   Training Loss: 2.300900151729584 \n",
      "Epoch: 79   Testing Loss: 8.813993663787842 \n",
      "Epoch: 80   Training Loss: 2.2698584151268006 \n",
      "Epoch: 80   Testing Loss: 8.789678859710694 \n",
      "Epoch: 81   Training Loss: 2.243926373720169 \n",
      "Epoch: 81   Testing Loss: 8.739229049682617 \n",
      "Epoch: 82   Training Loss: 2.1944517946243285 \n",
      "Epoch: 82   Testing Loss: 8.868894863128663 \n",
      "Epoch: 83   Training Loss: 2.1482454347610473 \n",
      "Epoch: 83   Testing Loss: 8.715879516601563 \n",
      "Epoch: 84   Training Loss: 2.1251041173934935 \n",
      "Epoch: 84   Testing Loss: 8.922884426116944 \n",
      "Epoch: 85   Training Loss: 2.0916566216945647 \n",
      "Epoch: 85   Testing Loss: 8.95860918045044 \n",
      "Epoch: 86   Training Loss: 2.058984771966934 \n",
      "Epoch: 86   Testing Loss: 8.99965238571167 \n",
      "Epoch: 87   Training Loss: 2.013532849550247 \n",
      "Epoch: 87   Testing Loss: 8.864743461608887 \n",
      "Epoch: 88   Training Loss: 1.9889806997776032 \n",
      "Epoch: 88   Testing Loss: 8.752749233245849 \n",
      "Epoch: 89   Training Loss: 1.9667136478424072 \n",
      "Epoch: 89   Testing Loss: 8.861836929321289 \n",
      "Epoch: 90   Training Loss: 1.945115592479706 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90   Testing Loss: 8.973290195465088 \n",
      "Epoch: 91   Training Loss: 1.9232703936100006 \n",
      "Epoch: 91   Testing Loss: 8.81376953125 \n",
      "Epoch: 92   Training Loss: 1.929992107152939 \n",
      "Epoch: 92   Testing Loss: 8.850815143585205 \n",
      "Epoch: 93   Training Loss: 1.922905184030533 \n",
      "Epoch: 93   Testing Loss: 8.795391654968261 \n",
      "Epoch: 94   Training Loss: 1.8548858487606048 \n",
      "Epoch: 94   Testing Loss: 8.730340766906739 \n",
      "Epoch: 95   Training Loss: 1.820324079990387 \n",
      "Epoch: 95   Testing Loss: 8.997866382598877 \n",
      "Epoch: 96   Training Loss: 1.8157422149181366 \n",
      "Epoch: 96   Testing Loss: 8.956126861572265 \n",
      "Epoch: 97   Training Loss: 1.790838179588318 \n",
      "Epoch: 97   Testing Loss: 8.873952808380126 \n",
      "Epoch: 98   Training Loss: 1.776229695081711 \n",
      "Epoch: 98   Testing Loss: 8.896875228881836 \n",
      "Epoch: 99   Training Loss: 1.7655360436439513 \n",
      "Epoch: 99   Testing Loss: 8.910537509918212 \n",
      "Epoch: 100   Training Loss: 1.770086555480957 \n",
      "Epoch: 100   Testing Loss: 8.82753782272339 \n",
      "Epoch: 101   Training Loss: 1.7236129832267761 \n",
      "Epoch: 101   Testing Loss: 8.972825813293458 \n",
      "Epoch: 102   Training Loss: 1.7016603028774262 \n",
      "Epoch: 102   Testing Loss: 8.903764362335204 \n",
      "Epoch: 103   Training Loss: 1.6857379007339477 \n",
      "Epoch: 103   Testing Loss: 8.958155155181885 \n",
      "Epoch: 104   Training Loss: 1.6580230855941773 \n",
      "Epoch: 104   Testing Loss: 8.958003158569335 \n",
      "Epoch: 105   Training Loss: 1.6291028428077698 \n",
      "Epoch: 105   Testing Loss: 8.983681049346924 \n",
      "Epoch: 106   Training Loss: 1.6291695737838745 \n",
      "Epoch: 106   Testing Loss: 9.053028507232666 \n",
      "Epoch: 107   Training Loss: 1.6114972996711732 \n",
      "Epoch: 107   Testing Loss: 9.03133197784424 \n",
      "Epoch: 108   Training Loss: 1.624062498807907 \n",
      "Epoch: 108   Testing Loss: 9.044416313171388 \n",
      "Epoch: 109   Training Loss: 1.6032045841217042 \n",
      "Epoch: 109   Testing Loss: 8.862136402130126 \n",
      "Epoch: 110   Training Loss: 1.5781152987480163 \n",
      "Epoch: 110   Testing Loss: 8.890932292938233 \n",
      "Epoch: 111   Training Loss: 1.5383711957931518 \n",
      "Epoch: 111   Testing Loss: 8.930356979370117 \n",
      "Epoch: 112   Training Loss: 1.5286573123931886 \n",
      "Epoch: 112   Testing Loss: 8.859234771728516 \n",
      "Epoch: 113   Training Loss: 1.4889800369739532 \n",
      "Epoch: 113   Testing Loss: 9.041031799316407 \n",
      "Epoch: 114   Training Loss: 1.5044270646572113 \n",
      "Epoch: 114   Testing Loss: 8.978202018737793 \n",
      "Epoch: 115   Training Loss: 1.4976427447795868 \n",
      "Epoch: 115   Testing Loss: 8.967148036956788 \n",
      "Epoch: 116   Training Loss: 1.4683958053588868 \n",
      "Epoch: 116   Testing Loss: 8.857424697875977 \n",
      "Epoch: 117   Training Loss: 1.4407932710647584 \n",
      "Epoch: 117   Testing Loss: 8.985737323760986 \n",
      "Epoch: 118   Training Loss: 1.4327241921424865 \n",
      "Epoch: 118   Testing Loss: 8.858123569488525 \n",
      "Epoch: 119   Training Loss: 1.4532923197746277 \n",
      "Epoch: 119   Testing Loss: 8.81304105758667 \n",
      "Epoch: 120   Training Loss: 1.4336294114589692 \n",
      "Epoch: 120   Testing Loss: 9.000914154052735 \n",
      "Epoch: 121   Training Loss: 1.415714847445488 \n",
      "Epoch: 121   Testing Loss: 8.92959846496582 \n",
      "Epoch: 122   Training Loss: 1.3980761307477951 \n",
      "Epoch: 122   Testing Loss: 9.018627967834473 \n",
      "Epoch: 123   Training Loss: 1.3750137293338776 \n",
      "Epoch: 123   Testing Loss: 9.044411754608154 \n",
      "Epoch: 124   Training Loss: 1.3630753815174104 \n",
      "Epoch: 124   Testing Loss: 8.97390811920166 \n",
      "Epoch: 125   Training Loss: 1.3885602128505707 \n",
      "Epoch: 125   Testing Loss: 8.986337642669678 \n",
      "Epoch: 126   Training Loss: 1.3706324315071106 \n",
      "Epoch: 126   Testing Loss: 9.020861797332763 \n",
      "Epoch: 127   Training Loss: 1.3621624147891997 \n",
      "Epoch: 127   Testing Loss: 8.899299983978272 \n",
      "Epoch: 128   Training Loss: 1.3109607636928557 \n",
      "Epoch: 128   Testing Loss: 8.950228843688965 \n",
      "Epoch: 129   Training Loss: 1.3111424905061722 \n",
      "Epoch: 129   Testing Loss: 8.970069427490234 \n",
      "Epoch: 130   Training Loss: 1.3045440590381623 \n",
      "Epoch: 130   Testing Loss: 8.98542673110962 \n",
      "Epoch: 131   Training Loss: 1.2919990646839141 \n",
      "Epoch: 131   Testing Loss: 8.963141403198243 \n",
      "Epoch: 132   Training Loss: 1.3023923325538636 \n",
      "Epoch: 132   Testing Loss: 9.103137493133545 \n",
      "Epoch: 133   Training Loss: 1.3027207762002946 \n",
      "Epoch: 133   Testing Loss: 9.058432693481445 \n",
      "Epoch: 134   Training Loss: 1.2538112103939056 \n",
      "Epoch: 134   Testing Loss: 9.123906364440918 \n",
      "Epoch: 135   Training Loss: 1.2356821250915528 \n",
      "Epoch: 135   Testing Loss: 8.878439178466797 \n",
      "Epoch: 136   Training Loss: 1.2347090923786164 \n",
      "Epoch: 136   Testing Loss: 8.938835258483886 \n",
      "Epoch: 137   Training Loss: 1.2409285414218902 \n",
      "Epoch: 137   Testing Loss: 8.874291954040528 \n",
      "Epoch: 138   Training Loss: 1.2131610685586929 \n",
      "Epoch: 138   Testing Loss: 8.901549530029296 \n",
      "Epoch: 139   Training Loss: 1.2099634277820588 \n",
      "Epoch: 139   Testing Loss: 8.955743045806885 \n",
      "Epoch: 140   Training Loss: 1.1946783673763275 \n",
      "Epoch: 140   Testing Loss: 9.104097537994384 \n",
      "Epoch: 141   Training Loss: 1.189878597855568 \n",
      "Epoch: 141   Testing Loss: 9.146528015136719 \n",
      "Epoch: 142   Training Loss: 1.2320100021362306 \n",
      "Epoch: 142   Testing Loss: 8.979338569641113 \n",
      "Epoch: 143   Training Loss: 1.168437762260437 \n",
      "Epoch: 143   Testing Loss: 8.994618644714356 \n",
      "Epoch: 144   Training Loss: 1.1789019507169725 \n",
      "Epoch: 144   Testing Loss: 8.997200927734376 \n",
      "Epoch: 145   Training Loss: 1.1804499793052674 \n",
      "Epoch: 145   Testing Loss: 8.827397365570068 \n",
      "Epoch: 146   Training Loss: 1.1787224847078324 \n",
      "Epoch: 146   Testing Loss: 8.885452098846436 \n",
      "Epoch: 147   Training Loss: 1.1676379853487016 \n",
      "Epoch: 147   Testing Loss: 8.911033611297608 \n",
      "Epoch: 148   Training Loss: 1.1572911322116852 \n",
      "Epoch: 148   Testing Loss: 8.91849880218506 \n",
      "Epoch: 149   Training Loss: 1.1401714420318603 \n",
      "Epoch: 149   Testing Loss: 8.953980579376221 \n",
      "Epoch: 150   Training Loss: 1.1387707805633545 \n",
      "Epoch: 150   Testing Loss: 8.924423828125 \n",
      "Epoch: 151   Training Loss: 1.1070465767383575 \n",
      "Epoch: 151   Testing Loss: 8.79464822769165 \n",
      "Epoch: 152   Training Loss: 1.122658949494362 \n",
      "Epoch: 152   Testing Loss: 8.975096321105957 \n",
      "Epoch: 153   Training Loss: 1.1131792163848877 \n",
      "Epoch: 153   Testing Loss: 8.895306911468506 \n",
      "Epoch: 154   Training Loss: 1.1082408320903778 \n",
      "Epoch: 154   Testing Loss: 9.006345291137695 \n",
      "Epoch: 155   Training Loss: 1.0811361771821977 \n",
      "Epoch: 155   Testing Loss: 8.890956134796143 \n",
      "Epoch: 156   Training Loss: 1.126984662413597 \n",
      "Epoch: 156   Testing Loss: 9.067725944519044 \n",
      "Epoch: 157   Training Loss: 1.1016157013177872 \n",
      "Epoch: 157   Testing Loss: 8.987700805664062 \n",
      "Epoch: 158   Training Loss: 1.0712417727708816 \n",
      "Epoch: 158   Testing Loss: 9.012012443542481 \n",
      "Epoch: 159   Training Loss: 1.0758427661657333 \n",
      "Epoch: 159   Testing Loss: 9.004829559326172 \n",
      "Epoch: 160   Training Loss: 1.07334528028965 \n",
      "Epoch: 160   Testing Loss: 8.942690086364745 \n",
      "Epoch: 161   Training Loss: 1.0557055580615997 \n",
      "Epoch: 161   Testing Loss: 8.953910179138184 \n",
      "Epoch: 162   Training Loss: 1.0577605921030044 \n",
      "Epoch: 162   Testing Loss: 8.976468925476075 \n",
      "Epoch: 163   Training Loss: 1.0495829766988753 \n",
      "Epoch: 163   Testing Loss: 8.853931522369384 \n",
      "Epoch: 164   Training Loss: 1.0360804170370101 \n",
      "Epoch: 164   Testing Loss: 9.053099250793457 \n",
      "Epoch: 165   Training Loss: 1.038837125301361 \n",
      "Epoch: 165   Testing Loss: 8.917064247131348 \n",
      "Epoch: 166   Training Loss: 1.0368521440029144 \n",
      "Epoch: 166   Testing Loss: 9.031030254364014 \n",
      "Epoch: 167   Training Loss: 1.0327400118112564 \n",
      "Epoch: 167   Testing Loss: 8.95396827697754 \n",
      "Epoch: 168   Training Loss: 1.0304142141342163 \n",
      "Epoch: 168   Testing Loss: 8.927556171417237 \n",
      "Epoch: 169   Training Loss: 1.019530622959137 \n",
      "Epoch: 169   Testing Loss: 8.952753772735596 \n",
      "Epoch: 170   Training Loss: 1.0152287995815277 \n",
      "Epoch: 170   Testing Loss: 9.068004989624024 \n",
      "Epoch: 171   Training Loss: 1.0235714733600616 \n",
      "Epoch: 171   Testing Loss: 9.081325092315673 \n",
      "Epoch: 172   Training Loss: 1.0096402621269227 \n",
      "Epoch: 172   Testing Loss: 8.905438842773437 \n",
      "Epoch: 173   Training Loss: 0.9769364166259765 \n",
      "Epoch: 173   Testing Loss: 8.977850379943847 \n",
      "Epoch: 174   Training Loss: 0.9997357511520386 \n",
      "Epoch: 174   Testing Loss: 8.922743244171143 \n",
      "Epoch: 175   Training Loss: 0.9856722939014435 \n",
      "Epoch: 175   Testing Loss: 8.998538455963136 \n",
      "Epoch: 176   Training Loss: 0.966304178237915 \n",
      "Epoch: 176   Testing Loss: 9.038073196411133 \n",
      "Epoch: 177   Training Loss: 0.9766772830486298 \n",
      "Epoch: 177   Testing Loss: 8.959097366333008 \n",
      "Epoch: 178   Training Loss: 0.9703547030687332 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 178   Testing Loss: 8.931179294586181 \n",
      "Epoch: 179   Training Loss: 1.0089993107318878 \n",
      "Epoch: 179   Testing Loss: 8.954056377410888 \n",
      "Epoch: 180   Training Loss: 0.9581072223186493 \n",
      "Epoch: 180   Testing Loss: 8.878962173461915 \n",
      "Epoch: 181   Training Loss: 0.9873437374830246 \n",
      "Epoch: 181   Testing Loss: 9.050045375823975 \n",
      "Epoch: 182   Training Loss: 0.9681277620792389 \n",
      "Epoch: 182   Testing Loss: 9.04552583694458 \n",
      "Epoch: 183   Training Loss: 0.9791577214002609 \n",
      "Epoch: 183   Testing Loss: 8.926215343475342 \n",
      "Epoch: 184   Training Loss: 0.9481297945976257 \n",
      "Epoch: 184   Testing Loss: 8.907053203582764 \n",
      "Epoch: 185   Training Loss: 0.96627936065197 \n",
      "Epoch: 185   Testing Loss: 8.978461589813232 \n",
      "Epoch: 186   Training Loss: 0.952237007021904 \n",
      "Epoch: 186   Testing Loss: 8.997892837524414 \n",
      "Epoch: 187   Training Loss: 0.9330206120014191 \n",
      "Epoch: 187   Testing Loss: 8.856536712646484 \n",
      "Epoch: 188   Training Loss: 0.9409269905090332 \n",
      "Epoch: 188   Testing Loss: 9.013673210144043 \n",
      "Epoch: 189   Training Loss: 0.9430388003587723 \n",
      "Epoch: 189   Testing Loss: 9.03845500946045 \n",
      "Epoch: 190   Training Loss: 0.9200732386112214 \n",
      "Epoch: 190   Testing Loss: 8.9121337890625 \n",
      "Epoch: 191   Training Loss: 0.9228986632823944 \n",
      "Epoch: 191   Testing Loss: 8.938753242492675 \n",
      "Epoch: 192   Training Loss: 0.8928072506189346 \n",
      "Epoch: 192   Testing Loss: 8.920719337463378 \n",
      "Epoch: 193   Training Loss: 0.9054590129852295 \n",
      "Epoch: 193   Testing Loss: 8.933836193084717 \n",
      "Epoch: 194   Training Loss: 0.9373667734861374 \n",
      "Epoch: 194   Testing Loss: 8.989752006530761 \n",
      "Epoch: 195   Training Loss: 0.891796326637268 \n",
      "Epoch: 195   Testing Loss: 9.01508373260498 \n",
      "Epoch: 196   Training Loss: 0.9201813220977784 \n",
      "Epoch: 196   Testing Loss: 8.922795124053955 \n",
      "Epoch: 197   Training Loss: 0.9178031635284424 \n",
      "Epoch: 197   Testing Loss: 8.770242366790772 \n",
      "Epoch: 198   Training Loss: 0.8947193759679795 \n",
      "Epoch: 198   Testing Loss: 8.997896137237548 \n",
      "Epoch: 199   Training Loss: 0.87390076816082 \n",
      "Epoch: 199   Testing Loss: 9.023093204498291 \n",
      "Epoch: 200   Training Loss: 0.8702172964811326 \n",
      "Epoch: 200   Testing Loss: 8.988508987426759 \n",
      "Epoch: 201   Training Loss: 0.890792868733406 \n",
      "Epoch: 201   Testing Loss: 8.9589404296875 \n",
      "Epoch: 202   Training Loss: 0.8813595777750015 \n",
      "Epoch: 202   Testing Loss: 8.974514923095704 \n",
      "Epoch: 203   Training Loss: 0.869350466132164 \n",
      "Epoch: 203   Testing Loss: 9.022190399169922 \n",
      "Epoch: 204   Training Loss: 0.8831525111198425 \n",
      "Epoch: 204   Testing Loss: 8.889599876403809 \n",
      "Epoch: 205   Training Loss: 0.8719006061553956 \n",
      "Epoch: 205   Testing Loss: 8.845258979797363 \n",
      "Epoch: 206   Training Loss: 0.8921327191591263 \n",
      "Epoch: 206   Testing Loss: 8.96143497467041 \n",
      "Epoch: 207   Training Loss: 0.8742174232006072 \n",
      "Epoch: 207   Testing Loss: 9.073663177490234 \n",
      "Epoch: 208   Training Loss: 0.8831774473190308 \n",
      "Epoch: 208   Testing Loss: 8.96763957977295 \n",
      "Epoch: 209   Training Loss: 0.8575991213321685 \n",
      "Epoch: 209   Testing Loss: 8.92964153289795 \n",
      "Epoch: 210   Training Loss: 0.8652060788869858 \n",
      "Epoch: 210   Testing Loss: 8.976649017333985 \n",
      "Epoch: 211   Training Loss: 0.8685629808902741 \n",
      "Epoch: 211   Testing Loss: 9.003423690795898 \n",
      "Epoch: 212   Training Loss: 0.8466724663972854 \n",
      "Epoch: 212   Testing Loss: 9.000723037719727 \n",
      "Epoch: 213   Training Loss: 0.8259466433525086 \n",
      "Epoch: 213   Testing Loss: 8.811396255493165 \n",
      "Epoch: 214   Training Loss: 0.8517678731679916 \n",
      "Epoch: 214   Testing Loss: 8.862575550079345 \n",
      "Epoch: 215   Training Loss: 0.8713117074966431 \n",
      "Epoch: 215   Testing Loss: 8.897938289642333 \n",
      "Epoch: 216   Training Loss: 0.8633993929624557 \n",
      "Epoch: 216   Testing Loss: 8.994822521209716 \n",
      "Epoch: 217   Training Loss: 0.8330126351118088 \n",
      "Epoch: 217   Testing Loss: 8.958018379211426 \n",
      "Epoch: 218   Training Loss: 0.8694078540802002 \n",
      "Epoch: 218   Testing Loss: 9.075388660430908 \n",
      "Epoch: 219   Training Loss: 0.8316762208938598 \n",
      "Epoch: 219   Testing Loss: 8.9576460647583 \n",
      "Epoch: 220   Training Loss: 0.8176164323091507 \n",
      "Epoch: 220   Testing Loss: 9.008016929626464 \n",
      "Epoch: 221   Training Loss: 0.8278927862644195 \n",
      "Epoch: 221   Testing Loss: 8.920629959106446 \n",
      "Epoch: 222   Training Loss: 0.847101382613182 \n",
      "Epoch: 222   Testing Loss: 9.013657321929932 \n",
      "Epoch: 223   Training Loss: 0.8234808629751206 \n",
      "Epoch: 223   Testing Loss: 8.918138008117676 \n",
      "Epoch: 224   Training Loss: 0.8454789310693741 \n",
      "Epoch: 224   Testing Loss: 8.928551559448243 \n",
      "Epoch: 225   Training Loss: 0.8251697653532029 \n",
      "Epoch: 225   Testing Loss: 8.844456405639649 \n",
      "Epoch: 226   Training Loss: 0.8325526291131973 \n",
      "Epoch: 226   Testing Loss: 8.997381134033203 \n",
      "Epoch: 227   Training Loss: 0.8113452172279358 \n",
      "Epoch: 227   Testing Loss: 8.930944995880127 \n",
      "Epoch: 228   Training Loss: 0.8234828114509583 \n",
      "Epoch: 228   Testing Loss: 8.995513134002685 \n",
      "Epoch: 229   Training Loss: 0.8227015745639801 \n",
      "Epoch: 229   Testing Loss: 9.026672782897949 \n",
      "Epoch: 230   Training Loss: 0.8022166389226913 \n",
      "Epoch: 230   Testing Loss: 8.974802322387696 \n",
      "Epoch: 231   Training Loss: 0.7788266235589981 \n",
      "Epoch: 231   Testing Loss: 9.000516223907471 \n",
      "Epoch: 232   Training Loss: 0.7954925230145454 \n",
      "Epoch: 232   Testing Loss: 8.977615356445312 \n",
      "Epoch: 233   Training Loss: 0.8122489190101624 \n",
      "Epoch: 233   Testing Loss: 9.028328018188477 \n",
      "Epoch: 234   Training Loss: 0.7902357757091523 \n",
      "Epoch: 234   Testing Loss: 9.026757106781005 \n",
      "Epoch: 235   Training Loss: 0.8019228148460388 \n",
      "Epoch: 235   Testing Loss: 9.096710815429688 \n",
      "Epoch: 236   Training Loss: 0.813409640789032 \n",
      "Epoch: 236   Testing Loss: 8.891060943603515 \n",
      "Epoch: 237   Training Loss: 0.81428735435009 \n",
      "Epoch: 237   Testing Loss: 9.053489513397217 \n",
      "Epoch: 238   Training Loss: 0.7801080214977264 \n",
      "Epoch: 238   Testing Loss: 8.937127742767334 \n",
      "Epoch: 239   Training Loss: 0.8047786545753479 \n",
      "Epoch: 239   Testing Loss: 8.928550605773927 \n",
      "Epoch: 240   Training Loss: 0.7807988584041595 \n",
      "Epoch: 240   Testing Loss: 8.971399517059327 \n",
      "Epoch: 241   Training Loss: 0.7790645211935043 \n",
      "Epoch: 241   Testing Loss: 8.925742244720459 \n",
      "Epoch: 242   Training Loss: 0.7979520070552826 \n",
      "Epoch: 242   Testing Loss: 9.002610511779785 \n",
      "Epoch: 243   Training Loss: 0.7849382227659225 \n",
      "Epoch: 243   Testing Loss: 8.906928958892822 \n",
      "Epoch: 244   Training Loss: 0.7796473550796509 \n",
      "Epoch: 244   Testing Loss: 9.000517044067383 \n",
      "Epoch: 245   Training Loss: 0.7841045394539833 \n",
      "Epoch: 245   Testing Loss: 8.972613525390624 \n",
      "Epoch: 246   Training Loss: 0.7847678047418595 \n",
      "Epoch: 246   Testing Loss: 8.903096103668213 \n",
      "Epoch: 247   Training Loss: 0.7623574933409691 \n",
      "Epoch: 247   Testing Loss: 8.993844718933106 \n",
      "Epoch: 248   Training Loss: 0.7810529860854148 \n",
      "Epoch: 248   Testing Loss: 9.020948944091797 \n",
      "Epoch: 249   Training Loss: 0.7995188674330711 \n",
      "Epoch: 249   Testing Loss: 8.945726375579834 \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Create writer and profiler to analyze loss over each epoch\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Set device to CUDA if available, initialize model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "net = CNN_3D_aligner()\n",
    "net.to(device)\n",
    "\n",
    "# Set up optimizer and loss function, set number of epochs\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-4, momentum=0.9)\n",
    "criterion = nn.MSELoss(reduction = 'mean')\n",
    "criterion.to(device)\n",
    "num_epochs = 250\n",
    "\n",
    "# Iniitializing variables to show statistics\n",
    "iteration = 0\n",
    "test_iteration = 0\n",
    "loss_list = []\n",
    "test_loss_list = []\n",
    "epoch_loss_averages = []\n",
    "test_epoch_loss_averages = []\n",
    "\n",
    "# Iterates over dataset multiple times\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    test_epoch_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(trainset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(data[0]).to(device)), torch.from_numpy(data[1]).to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs).to(device)\n",
    "        loss = criterion(outputs, truths)\n",
    "        writer.add_scalar(\"Loss / Train\", loss, epoch) # adds training loss scalar\n",
    "        loss_list.append(loss.cpu().detach().numpy())\n",
    "        epoch_loss += loss.cpu().detach().numpy()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration += 1\n",
    "        if iteration % trainset.shape[0] == 0:\n",
    "            epoch_loss_averages.append(epoch_loss / trainset.shape[0])\n",
    "            print('Epoch: {}   Training Loss: {} '.format(epoch, epoch_loss / trainset.shape[0]))\n",
    "            \n",
    "    for i, test_data in enumerate(testset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(test_data[0]).to(device)), torch.from_numpy(test_data[1]).to(device).float()\n",
    "        outputs = net(inputs).to(device)\n",
    "        test_loss = criterion(outputs, truths)\n",
    "        \n",
    "        writer.add_scalar(\"Loss / Test\", test_loss, epoch) # adds testing loss scalar\n",
    "        test_loss_list.append(test_loss.cpu().detach().numpy())\n",
    "        test_epoch_loss += test_loss.cpu().detach().numpy()\n",
    "        \n",
    "        test_iteration +=1\n",
    "        if test_iteration % testset.shape[0] == 0:\n",
    "            test_epoch_loss_averages.append(test_epoch_loss / testset.shape[0])\n",
    "            print('Epoch: {}   Testing Loss: {} '.format(epoch, test_epoch_loss / testset.shape[0]))\n",
    "            \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9d536",
   "metadata": {},
   "source": [
    "Now in order to observe convergence or lack thereof a graph of loss per epoch are created for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec9c6ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot epoch loss to test for convergence\n",
    "plt.plot(epoch_loss_averages)\n",
    "plt.plot(test_epoch_loss_averages)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253f989",
   "metadata": {},
   "source": [
    "As seen above, the neural network is able to create convergence for the training data to the subpixel level. However, the testing loss does not converge indicating that this model has a overfitting problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
