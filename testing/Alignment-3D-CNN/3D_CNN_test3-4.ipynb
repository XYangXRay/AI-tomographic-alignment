{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b87aa476",
   "metadata": {},
   "source": [
    "# 3D Convolutional Neural Network for Tomographic Alignment\n",
    "\n",
    "## Expanding the Network: ResNet and Engineering the Data Again\n",
    "\n",
    "Since based on previous analysis it is likely that the network was too small to properly identify the features necessary for alignment, the next logical step is to expand the network. Making deeper neural networks usually results in diminishing returns, but residual neural networks have proven to successfully allow for deeper neural networks. Now this model structure will be used in order to find some form of convergence.\n",
    "\n",
    "While it seems like the ResNet will increase the performance of the model on larger training sets, testing convergence is still a major problem. Now the input data will be modified to find the difference between each projection which should help the network better isolate the pattern we want it to find. This will hopefully allow for better understanding of the specific alignment problem by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d27f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential packages\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import tomography and imaging packages\n",
    "import tomopy\n",
    "from skimage.transform import rotate, AffineTransform\n",
    "from skimage import transform as tf\n",
    "\n",
    "# Import neural net packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.profiler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5db9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Environment: pytorch\n",
      "Cuda Version: 11.8\n",
      "Cuda Availability: True\n",
      "/home/liam/Projects/Tomographic Alignment\r\n"
     ]
    }
   ],
   "source": [
    "# Checking to ensure environment and cuda are correct\n",
    "print(\"Working Environment: {}\".format(os.environ['CONDA_DEFAULT_ENV']))\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"Cuda Version: {}\".format(torch.version.cuda))\n",
    "print(\"Cuda Availability: {}\".format(torch.cuda.is_available()))\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1876ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting function that can be used for modifying the data\n",
    "def data_mean_difference(data):\n",
    "    \n",
    "    # Create copy of dataset for projections to be modified, also initializes array for mean projections\n",
    "    projections = data[:, 0].copy()\n",
    "    mean_projections = np.zeros((projections.shape[0]), dtype = object)\n",
    "    \n",
    "    # Iterate through each projection stack\n",
    "    for i in range (projections.shape[0]):\n",
    "\n",
    "        # Get rid of extra dimensions for neural networks and create a mean projection\n",
    "        projections[i] = np.squeeze(projections[i])\n",
    "        mean_projections[i] = np.mean(projections[i], axis = 0)\n",
    "\n",
    "        # Iterate through every projection in stack\n",
    "        for j in range (projections[0].shape[0]):\n",
    "\n",
    "            # Change current projection to difference between current projection and mean projection\n",
    "            projections[i][j] = projections[i][j] - mean_projections[i]\n",
    "            \n",
    "        # Expand dimensions to original form for neural networks\n",
    "        projections[i] = np.expand_dims(projections[i], axis = 0)\n",
    "        projections[i] = np.expand_dims(projections[i], axis = 0)\n",
    "\n",
    "    # Create data difference array\n",
    "    data_diff = data.copy()\n",
    "\n",
    "    # Replace all data with the new difference projections\n",
    "    for i in range (data.shape[0]):\n",
    "\n",
    "        data_diff[i][0] = projections[i]\n",
    "        \n",
    "    return data_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38471a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_difference(data):\n",
    "\n",
    "    data_diff = np.zeros((data.shape), dtype = object)\n",
    "    projections = []\n",
    "\n",
    "    for i in range (entries):\n",
    "        projections.append(np.squeeze(data[:, 0][i]))\n",
    "\n",
    "    projections = np.asarray(projections)\n",
    "\n",
    "    differences = np.zeros((250, projections[0].shape[0] - 1), dtype = object)\n",
    "\n",
    "    for i in range (entries):\n",
    "\n",
    "        for j in range (projections[0].shape[0] - 1):\n",
    "\n",
    "            differences[i, j] = (projections[i][j + 1] - projections[i][j])\n",
    "\n",
    "    for i in range (data_diff.shape[0]):\n",
    "\n",
    "        data_diff[i][0] = np.zeros((differences.shape[1], differences[0, 0].shape[0], differences[0,0].shape[1]))\n",
    "        data_diff[i][1] = data[i][1]\n",
    "\n",
    "        for j in range (projections[0].shape[0] - 1):\n",
    "\n",
    "            data_diff[i][0][j] = differences[i, j]\n",
    "\n",
    "        data_diff[i][0] = np.expand_dims(data_diff[i][0], axis = 0)\n",
    "        data_diff[i][0] = np.expand_dims(data_diff[i][0], axis = 0)\n",
    "\n",
    "    return data_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df2f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data, 25 entries of 128 resolution shepp3ds\n",
    "res = 128\n",
    "entries = 25\n",
    "data = []\n",
    "\n",
    "for i in range(entries):\n",
    "    data.append(np.load('./shepp{}-{}/shepp{}-{}_{}.npy'.format(res, entries, res, entries, i), \n",
    "                        allow_pickle = True))\n",
    "    \n",
    "data = np.asarray(data)\n",
    "\n",
    "\n",
    "# data = data_mean_difference(data)\n",
    "data = data_difference(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c8c622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Dataset: (20, 2)\n",
      "Shape of Testing Dataset: (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking shape of training and testing splits\n",
    "trainset, testset = np.split(data, [int(entries * 4 / 5)])\n",
    "print(\"Shape of Training Dataset: {}\".format(trainset.shape))\n",
    "print(\"Shape of Testing Dataset: {}\".format(testset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cb3e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def norm(proj):\n",
    "    proj = (proj - torch.min(proj)) / (torch.max(proj) - torch.min(proj))\n",
    "    return proj\n",
    "\n",
    "# Get inplanes for resnet\n",
    "def get_inplanes():\n",
    "    return [64, 128, 256, 512]\n",
    "\n",
    "\n",
    "# Preset for a 3x3x3 kernel convolution\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "# Preset for a 1x1x1 kernel convolution\n",
    "def conv1x1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=1,\n",
    "                     stride=stride,\n",
    "                     bias=False)\n",
    "\n",
    "# Basic block for resnet\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv3x3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# Bottleneck block for resnet\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv1x1x1(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = conv3x3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Resnet structure\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 block_inplanes,\n",
    "                 n_input_channels=1,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 shortcut_type='B',\n",
    "                 widen_factor=1.0,\n",
    "                 n_classes=360):\n",
    "        super().__init__()\n",
    "\n",
    "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
    "\n",
    "        self.in_planes = block_inplanes[0]\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        self.conv1 = nn.Conv3d(n_input_channels,\n",
    "                               self.in_planes,\n",
    "                               kernel_size=(conv1_t_size, 7, 7),\n",
    "                               stride=(conv1_t_stride, 2, 2),\n",
    "                               padding=(conv1_t_size // 2, 3, 3),\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n",
    "                                       shortcut_type)\n",
    "        self.layer2 = self._make_layer(block,\n",
    "                                       block_inplanes[1],\n",
    "                                       layers[1],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer3 = self._make_layer(block,\n",
    "                                       block_inplanes[2],\n",
    "                                       layers[2],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer4 = self._make_layer(block,\n",
    "                                       block_inplanes[3],\n",
    "                                       layers[3],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _downsample_basic_block(self, x, planes, stride):\n",
    "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n",
    "                                out.size(3), out.size(4))\n",
    "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "            zero_pads = zero_pads.cuda()\n",
    "\n",
    "        out = torch.cat([out.data, zero_pads], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # make layer helper function\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            \n",
    "                downsample = nn.Sequential(\n",
    "                    conv1x1x1(self.in_planes, planes * block.expansion, stride),\n",
    "                    nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(in_planes=self.in_planes,\n",
    "                  planes=planes,\n",
    "                  stride=stride,\n",
    "                  downsample=downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Generates form of resnet\n",
    "def generate_model(model_depth, **kwargs):\n",
    "    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
    "\n",
    "    if model_depth == 10:\n",
    "        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 18:\n",
    "        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 34:\n",
    "        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 50:\n",
    "        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 101:\n",
    "        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 152:\n",
    "        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 200:\n",
    "        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e9f8c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 360]                  --\n",
       "├─Conv3d: 1-1                            [1, 64, 180, 64, 92]      21,952\n",
       "├─BatchNorm3d: 1-2                       [1, 64, 180, 64, 92]      128\n",
       "├─ReLU: 1-3                              [1, 64, 180, 64, 92]      --\n",
       "├─MaxPool3d: 1-4                         [1, 64, 90, 32, 46]       --\n",
       "├─Sequential: 1-5                        [1, 256, 90, 32, 46]      --\n",
       "│    └─Bottleneck: 2-1                   [1, 256, 90, 32, 46]      --\n",
       "│    │    └─Conv3d: 3-1                  [1, 64, 90, 32, 46]       4,096\n",
       "│    │    └─BatchNorm3d: 3-2             [1, 64, 90, 32, 46]       128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 90, 32, 46]       --\n",
       "│    │    └─Conv3d: 3-4                  [1, 64, 90, 32, 46]       110,592\n",
       "│    │    └─BatchNorm3d: 3-5             [1, 64, 90, 32, 46]       128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 90, 32, 46]       --\n",
       "│    │    └─Conv3d: 3-7                  [1, 256, 90, 32, 46]      16,384\n",
       "│    │    └─BatchNorm3d: 3-8             [1, 256, 90, 32, 46]      512\n",
       "│    │    └─Sequential: 3-9              [1, 256, 90, 32, 46]      16,896\n",
       "│    │    └─ReLU: 3-10                   [1, 256, 90, 32, 46]      --\n",
       "│    └─Bottleneck: 2-2                   [1, 256, 90, 32, 46]      --\n",
       "│    │    └─Conv3d: 3-11                 [1, 64, 90, 32, 46]       16,384\n",
       "│    │    └─BatchNorm3d: 3-12            [1, 64, 90, 32, 46]       128\n",
       "│    │    └─ReLU: 3-13                   [1, 64, 90, 32, 46]       --\n",
       "│    │    └─Conv3d: 3-14                 [1, 64, 90, 32, 46]       110,592\n",
       "│    │    └─BatchNorm3d: 3-15            [1, 64, 90, 32, 46]       128\n",
       "│    │    └─ReLU: 3-16                   [1, 64, 90, 32, 46]       --\n",
       "│    │    └─Conv3d: 3-17                 [1, 256, 90, 32, 46]      16,384\n",
       "│    │    └─BatchNorm3d: 3-18            [1, 256, 90, 32, 46]      512\n",
       "│    │    └─ReLU: 3-19                   [1, 256, 90, 32, 46]      --\n",
       "│    └─Bottleneck: 2-3                   [1, 256, 90, 32, 46]      --\n",
       "│    │    └─Conv3d: 3-20                 [1, 64, 90, 32, 46]       16,384\n",
       "│    │    └─BatchNorm3d: 3-21            [1, 64, 90, 32, 46]       128\n",
       "│    │    └─ReLU: 3-22                   [1, 64, 90, 32, 46]       --\n",
       "│    │    └─Conv3d: 3-23                 [1, 64, 90, 32, 46]       110,592\n",
       "│    │    └─BatchNorm3d: 3-24            [1, 64, 90, 32, 46]       128\n",
       "│    │    └─ReLU: 3-25                   [1, 64, 90, 32, 46]       --\n",
       "│    │    └─Conv3d: 3-26                 [1, 256, 90, 32, 46]      16,384\n",
       "│    │    └─BatchNorm3d: 3-27            [1, 256, 90, 32, 46]      512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 90, 32, 46]      --\n",
       "├─Sequential: 1-6                        [1, 512, 45, 16, 23]      --\n",
       "│    └─Bottleneck: 2-4                   [1, 512, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-29                 [1, 128, 90, 32, 46]      32,768\n",
       "│    │    └─BatchNorm3d: 3-30            [1, 128, 90, 32, 46]      256\n",
       "│    │    └─ReLU: 3-31                   [1, 128, 90, 32, 46]      --\n",
       "│    │    └─Conv3d: 3-32                 [1, 128, 45, 16, 23]      442,368\n",
       "│    │    └─BatchNorm3d: 3-33            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-34                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-35                 [1, 512, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-36            [1, 512, 45, 16, 23]      1,024\n",
       "│    │    └─Sequential: 3-37             [1, 512, 45, 16, 23]      132,096\n",
       "│    │    └─ReLU: 3-38                   [1, 512, 45, 16, 23]      --\n",
       "│    └─Bottleneck: 2-5                   [1, 512, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-39                 [1, 128, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-40            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-41                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-42                 [1, 128, 45, 16, 23]      442,368\n",
       "│    │    └─BatchNorm3d: 3-43            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-44                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-45                 [1, 512, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-46            [1, 512, 45, 16, 23]      1,024\n",
       "│    │    └─ReLU: 3-47                   [1, 512, 45, 16, 23]      --\n",
       "│    └─Bottleneck: 2-6                   [1, 512, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-48                 [1, 128, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-49            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-50                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-51                 [1, 128, 45, 16, 23]      442,368\n",
       "│    │    └─BatchNorm3d: 3-52            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-53                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-54                 [1, 512, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-55            [1, 512, 45, 16, 23]      1,024\n",
       "│    │    └─ReLU: 3-56                   [1, 512, 45, 16, 23]      --\n",
       "│    └─Bottleneck: 2-7                   [1, 512, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-57                 [1, 128, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-58            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-59                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-60                 [1, 128, 45, 16, 23]      442,368\n",
       "│    │    └─BatchNorm3d: 3-61            [1, 128, 45, 16, 23]      256\n",
       "│    │    └─ReLU: 3-62                   [1, 128, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-63                 [1, 512, 45, 16, 23]      65,536\n",
       "│    │    └─BatchNorm3d: 3-64            [1, 512, 45, 16, 23]      1,024\n",
       "│    │    └─ReLU: 3-65                   [1, 512, 45, 16, 23]      --\n",
       "├─Sequential: 1-7                        [1, 1024, 23, 8, 12]      --\n",
       "│    └─Bottleneck: 2-8                   [1, 1024, 23, 8, 12]      --\n",
       "│    │    └─Conv3d: 3-66                 [1, 256, 45, 16, 23]      131,072\n",
       "│    │    └─BatchNorm3d: 3-67            [1, 256, 45, 16, 23]      512\n",
       "│    │    └─ReLU: 3-68                   [1, 256, 45, 16, 23]      --\n",
       "│    │    └─Conv3d: 3-69                 [1, 256, 23, 8, 12]       1,769,472\n",
       "│    │    └─BatchNorm3d: 3-70            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-71                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-72                 [1, 1024, 23, 8, 12]      262,144\n",
       "│    │    └─BatchNorm3d: 3-73            [1, 1024, 23, 8, 12]      2,048\n",
       "│    │    └─Sequential: 3-74             [1, 1024, 23, 8, 12]      526,336\n",
       "│    │    └─ReLU: 3-75                   [1, 1024, 23, 8, 12]      --\n",
       "│    └─Bottleneck: 2-9                   [1, 1024, 23, 8, 12]      --\n",
       "│    │    └─Conv3d: 3-76                 [1, 256, 23, 8, 12]       262,144\n",
       "│    │    └─BatchNorm3d: 3-77            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-78                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-79                 [1, 256, 23, 8, 12]       1,769,472\n",
       "│    │    └─BatchNorm3d: 3-80            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-81                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-82                 [1, 1024, 23, 8, 12]      262,144\n",
       "│    │    └─BatchNorm3d: 3-83            [1, 1024, 23, 8, 12]      2,048\n",
       "│    │    └─ReLU: 3-84                   [1, 1024, 23, 8, 12]      --\n",
       "│    └─Bottleneck: 2-10                  [1, 1024, 23, 8, 12]      --\n",
       "│    │    └─Conv3d: 3-85                 [1, 256, 23, 8, 12]       262,144\n",
       "│    │    └─BatchNorm3d: 3-86            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-87                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-88                 [1, 256, 23, 8, 12]       1,769,472\n",
       "│    │    └─BatchNorm3d: 3-89            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-90                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-91                 [1, 1024, 23, 8, 12]      262,144\n",
       "│    │    └─BatchNorm3d: 3-92            [1, 1024, 23, 8, 12]      2,048\n",
       "│    │    └─ReLU: 3-93                   [1, 1024, 23, 8, 12]      --\n",
       "│    └─Bottleneck: 2-11                  [1, 1024, 23, 8, 12]      --\n",
       "│    │    └─Conv3d: 3-94                 [1, 256, 23, 8, 12]       262,144\n",
       "│    │    └─BatchNorm3d: 3-95            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-96                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-97                 [1, 256, 23, 8, 12]       1,769,472\n",
       "│    │    └─BatchNorm3d: 3-98            [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-99                   [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-100                [1, 1024, 23, 8, 12]      262,144\n",
       "│    │    └─BatchNorm3d: 3-101           [1, 1024, 23, 8, 12]      2,048\n",
       "│    │    └─ReLU: 3-102                  [1, 1024, 23, 8, 12]      --\n",
       "│    └─Bottleneck: 2-12                  [1, 1024, 23, 8, 12]      --\n",
       "│    │    └─Conv3d: 3-103                [1, 256, 23, 8, 12]       262,144\n",
       "│    │    └─BatchNorm3d: 3-104           [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-105                  [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-106                [1, 256, 23, 8, 12]       1,769,472\n",
       "│    │    └─BatchNorm3d: 3-107           [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-108                  [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-109                [1, 1024, 23, 8, 12]      262,144\n",
       "│    │    └─BatchNorm3d: 3-110           [1, 1024, 23, 8, 12]      2,048\n",
       "│    │    └─ReLU: 3-111                  [1, 1024, 23, 8, 12]      --\n",
       "│    └─Bottleneck: 2-13                  [1, 1024, 23, 8, 12]      --\n",
       "│    │    └─Conv3d: 3-112                [1, 256, 23, 8, 12]       262,144\n",
       "│    │    └─BatchNorm3d: 3-113           [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-114                  [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-115                [1, 256, 23, 8, 12]       1,769,472\n",
       "│    │    └─BatchNorm3d: 3-116           [1, 256, 23, 8, 12]       512\n",
       "│    │    └─ReLU: 3-117                  [1, 256, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-118                [1, 1024, 23, 8, 12]      262,144\n",
       "│    │    └─BatchNorm3d: 3-119           [1, 1024, 23, 8, 12]      2,048\n",
       "│    │    └─ReLU: 3-120                  [1, 1024, 23, 8, 12]      --\n",
       "├─Sequential: 1-8                        [1, 2048, 12, 4, 6]       --\n",
       "│    └─Bottleneck: 2-14                  [1, 2048, 12, 4, 6]       --\n",
       "│    │    └─Conv3d: 3-121                [1, 512, 23, 8, 12]       524,288\n",
       "│    │    └─BatchNorm3d: 3-122           [1, 512, 23, 8, 12]       1,024\n",
       "│    │    └─ReLU: 3-123                  [1, 512, 23, 8, 12]       --\n",
       "│    │    └─Conv3d: 3-124                [1, 512, 12, 4, 6]        7,077,888\n",
       "│    │    └─BatchNorm3d: 3-125           [1, 512, 12, 4, 6]        1,024\n",
       "│    │    └─ReLU: 3-126                  [1, 512, 12, 4, 6]        --\n",
       "│    │    └─Conv3d: 3-127                [1, 2048, 12, 4, 6]       1,048,576\n",
       "│    │    └─BatchNorm3d: 3-128           [1, 2048, 12, 4, 6]       4,096\n",
       "│    │    └─Sequential: 3-129            [1, 2048, 12, 4, 6]       2,101,248\n",
       "│    │    └─ReLU: 3-130                  [1, 2048, 12, 4, 6]       --\n",
       "│    └─Bottleneck: 2-15                  [1, 2048, 12, 4, 6]       --\n",
       "│    │    └─Conv3d: 3-131                [1, 512, 12, 4, 6]        1,048,576\n",
       "│    │    └─BatchNorm3d: 3-132           [1, 512, 12, 4, 6]        1,024\n",
       "│    │    └─ReLU: 3-133                  [1, 512, 12, 4, 6]        --\n",
       "│    │    └─Conv3d: 3-134                [1, 512, 12, 4, 6]        7,077,888\n",
       "│    │    └─BatchNorm3d: 3-135           [1, 512, 12, 4, 6]        1,024\n",
       "│    │    └─ReLU: 3-136                  [1, 512, 12, 4, 6]        --\n",
       "│    │    └─Conv3d: 3-137                [1, 2048, 12, 4, 6]       1,048,576\n",
       "│    │    └─BatchNorm3d: 3-138           [1, 2048, 12, 4, 6]       4,096\n",
       "│    │    └─ReLU: 3-139                  [1, 2048, 12, 4, 6]       --\n",
       "│    └─Bottleneck: 2-16                  [1, 2048, 12, 4, 6]       --\n",
       "│    │    └─Conv3d: 3-140                [1, 512, 12, 4, 6]        1,048,576\n",
       "│    │    └─BatchNorm3d: 3-141           [1, 512, 12, 4, 6]        1,024\n",
       "│    │    └─ReLU: 3-142                  [1, 512, 12, 4, 6]        --\n",
       "│    │    └─Conv3d: 3-143                [1, 512, 12, 4, 6]        7,077,888\n",
       "│    │    └─BatchNorm3d: 3-144           [1, 512, 12, 4, 6]        1,024\n",
       "│    │    └─ReLU: 3-145                  [1, 512, 12, 4, 6]        --\n",
       "│    │    └─Conv3d: 3-146                [1, 2048, 12, 4, 6]       1,048,576\n",
       "│    │    └─BatchNorm3d: 3-147           [1, 2048, 12, 4, 6]       4,096\n",
       "│    │    └─ReLU: 3-148                  [1, 2048, 12, 4, 6]       --\n",
       "├─AdaptiveAvgPool3d: 1-9                 [1, 2048, 1, 1, 1]        --\n",
       "├─Linear: 1-10                           [1, 360]                  737,640\n",
       "==========================================================================================\n",
       "Total params: 46,892,712\n",
       "Trainable params: 46,892,712\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 166.72\n",
       "==========================================================================================\n",
       "Input size (MB): 16.96\n",
       "Forward/backward pass size (MB): 5744.99\n",
       "Params size (MB): 187.57\n",
       "Estimated Total Size (MB): 5949.52\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = generate_model(50)\n",
    "summary(model, (1, 1, 180, 128, 184))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60436868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Epoch: 0   Training Loss: 12.569259285926819 \n",
      "Epoch: 0   Validation Loss: 8.960946464538575 \n",
      "Epoch: 1   Training Loss: 9.144684958457947 \n",
      "Epoch: 1   Validation Loss: 11.09910831451416 \n",
      "Epoch: 2   Training Loss: 9.174124169349671 \n",
      "Epoch: 2   Validation Loss: 8.002226161956788 \n",
      "Epoch: 3   Training Loss: 8.506996178627015 \n",
      "Epoch: 3   Validation Loss: 8.024258136749268 \n",
      "Epoch: 4   Training Loss: 8.348653173446655 \n",
      "Epoch: 4   Validation Loss: 8.162216758728027 \n",
      "Epoch: 5   Training Loss: 8.322318530082702 \n",
      "Epoch: 5   Validation Loss: 8.21976203918457 \n",
      "Epoch: 6   Training Loss: 8.319579100608825 \n",
      "Epoch: 6   Validation Loss: 8.215788269042969 \n",
      "Epoch: 7   Training Loss: 8.307487392425537 \n",
      "Epoch: 7   Validation Loss: 8.21584186553955 \n",
      "Epoch: 8   Training Loss: 8.299998760223389 \n",
      "Epoch: 8   Validation Loss: 8.220346450805664 \n",
      "Epoch: 9   Training Loss: 8.29520661830902 \n",
      "Epoch: 9   Validation Loss: 8.22338171005249 \n",
      "Epoch: 10   Training Loss: 8.291370749473572 \n",
      "Epoch: 10   Validation Loss: 8.225130081176758 \n",
      "Epoch: 11   Training Loss: 8.288002467155456 \n",
      "Epoch: 11   Validation Loss: 8.226882839202881 \n",
      "Epoch: 12   Training Loss: 8.285101675987244 \n",
      "Epoch: 12   Validation Loss: 8.228129482269287 \n",
      "Epoch: 13   Training Loss: 8.28250834941864 \n",
      "Epoch: 13   Validation Loss: 8.22967987060547 \n",
      "Epoch: 14   Training Loss: 8.280320024490356 \n",
      "Epoch: 14   Validation Loss: 8.230864143371582 \n",
      "Epoch: 15   Training Loss: 8.278316330909728 \n",
      "Epoch: 15   Validation Loss: 8.232066345214843 \n",
      "Epoch: 16   Training Loss: 8.276530742645264 \n",
      "Epoch: 16   Validation Loss: 8.233020687103272 \n",
      "Epoch: 17   Training Loss: 8.274893546104432 \n",
      "Epoch: 17   Validation Loss: 8.23418607711792 \n",
      "Epoch: 18   Training Loss: 8.273458123207092 \n",
      "Epoch: 18   Validation Loss: 8.234726524353027 \n",
      "Epoch: 19   Training Loss: 8.272007131576538 \n",
      "Epoch: 19   Validation Loss: 8.23551778793335 \n",
      "Epoch: 20   Training Loss: 8.270700240135193 \n",
      "Epoch: 20   Validation Loss: 8.236085414886475 \n",
      "Epoch: 21   Training Loss: 8.269404816627503 \n",
      "Epoch: 21   Validation Loss: 8.236694431304931 \n",
      "Epoch: 22   Training Loss: 8.268208360671997 \n",
      "Epoch: 22   Validation Loss: 8.237314319610595 \n",
      "Epoch: 23   Training Loss: 8.267008686065674 \n",
      "Epoch: 23   Validation Loss: 8.237965774536132 \n",
      "Epoch: 24   Training Loss: 8.265846800804137 \n",
      "Epoch: 24   Validation Loss: 8.238686847686768 \n",
      "Epoch: 25   Training Loss: 8.264649939537048 \n",
      "Epoch: 25   Validation Loss: 8.23951072692871 \n",
      "Epoch: 26   Training Loss: 8.263462018966674 \n",
      "Epoch: 26   Validation Loss: 8.240066719055175 \n",
      "Epoch: 27   Training Loss: 8.26209065914154 \n",
      "Epoch: 27   Validation Loss: 8.241213321685791 \n",
      "Epoch: 28   Training Loss: 8.260521149635315 \n",
      "Epoch: 28   Validation Loss: 8.242427921295166 \n",
      "Epoch: 29   Training Loss: 8.258668899536133 \n",
      "Epoch: 29   Validation Loss: 8.243810749053955 \n",
      "Epoch: 30   Training Loss: 8.256048393249511 \n",
      "Epoch: 30   Validation Loss: 8.245987224578858 \n",
      "Epoch: 31   Training Loss: 8.252746534347533 \n",
      "Epoch: 31   Validation Loss: 8.246644592285156 \n",
      "Epoch: 32   Training Loss: 8.247673988342285 \n",
      "Epoch: 32   Validation Loss: 8.24710111618042 \n",
      "Epoch: 33   Training Loss: 8.241325879096985 \n",
      "Epoch: 33   Validation Loss: 8.247975254058838 \n",
      "Epoch: 34   Training Loss: 8.231696915626525 \n",
      "Epoch: 34   Validation Loss: 8.249698543548584 \n",
      "Epoch: 35   Training Loss: 8.222462797164917 \n",
      "Epoch: 35   Validation Loss: 8.254601097106933 \n",
      "Epoch: 36   Training Loss: 8.201406598091125 \n",
      "Epoch: 36   Validation Loss: 8.264135551452636 \n",
      "Epoch: 37   Training Loss: 8.205577898025513 \n",
      "Epoch: 37   Validation Loss: 8.263870239257812 \n",
      "Epoch: 38   Training Loss: 8.24581446647644 \n",
      "Epoch: 38   Validation Loss: 8.398278045654298 \n",
      "Epoch: 39   Training Loss: 8.301710319519042 \n",
      "Epoch: 39   Validation Loss: 8.200216293334961 \n",
      "Epoch: 40   Training Loss: 8.269198632240295 \n",
      "Epoch: 40   Validation Loss: 8.216341018676758 \n",
      "Epoch: 41   Training Loss: 8.266459703445435 \n",
      "Epoch: 41   Validation Loss: 8.228315162658692 \n",
      "Epoch: 42   Training Loss: 8.26404640674591 \n",
      "Epoch: 42   Validation Loss: 8.230917835235596 \n",
      "Epoch: 43   Training Loss: 8.261629128456116 \n",
      "Epoch: 43   Validation Loss: 8.23295555114746 \n",
      "Epoch: 44   Training Loss: 8.258384728431702 \n",
      "Epoch: 44   Validation Loss: 8.23547134399414 \n",
      "Epoch: 45   Training Loss: 8.253055477142334 \n",
      "Epoch: 45   Validation Loss: 8.24000425338745 \n",
      "Epoch: 46   Training Loss: 8.238231897354126 \n",
      "Epoch: 46   Validation Loss: 8.256689739227294 \n",
      "Epoch: 47   Training Loss: 8.215245652198792 \n",
      "Epoch: 47   Validation Loss: 8.24393835067749 \n",
      "Epoch: 48   Training Loss: 8.144895792007446 \n",
      "Epoch: 48   Validation Loss: 8.302748966217042 \n",
      "Epoch: 49   Training Loss: 8.327897763252258 \n",
      "Epoch: 49   Validation Loss: 8.266508388519288 \n",
      "Epoch: 50   Training Loss: 8.282850313186646 \n",
      "Epoch: 50   Validation Loss: 8.204364585876466 \n",
      "Epoch: 51   Training Loss: 8.269948124885559 \n",
      "Epoch: 51   Validation Loss: 8.22279987335205 \n",
      "Epoch: 52   Training Loss: 8.267311716079712 \n",
      "Epoch: 52   Validation Loss: 8.230652809143066 \n",
      "Epoch: 53   Training Loss: 8.265125370025634 \n",
      "Epoch: 53   Validation Loss: 8.233383274078369 \n",
      "Epoch: 54   Training Loss: 8.263143754005432 \n",
      "Epoch: 54   Validation Loss: 8.234927082061768 \n",
      "Epoch: 55   Training Loss: 8.261507320404053 \n",
      "Epoch: 55   Validation Loss: 8.23624267578125 \n",
      "Epoch: 56   Training Loss: 8.260026931762695 \n",
      "Epoch: 56   Validation Loss: 8.237048530578614 \n",
      "Epoch: 57   Training Loss: 8.258540034294128 \n",
      "Epoch: 57   Validation Loss: 8.23778419494629 \n",
      "Epoch: 58   Training Loss: 8.256896543502808 \n",
      "Epoch: 58   Validation Loss: 8.238294124603271 \n",
      "Epoch: 59   Training Loss: 8.254646396636963 \n",
      "Epoch: 59   Validation Loss: 8.238875675201417 \n",
      "Epoch: 60   Training Loss: 8.25032298564911 \n",
      "Epoch: 60   Validation Loss: 8.239799308776856 \n",
      "Epoch: 61   Training Loss: 8.243730759620666 \n",
      "Epoch: 61   Validation Loss: 8.238669204711915 \n",
      "Epoch: 62   Training Loss: 8.256778645515443 \n",
      "Epoch: 62   Validation Loss: 8.235391902923585 \n",
      "Epoch: 63   Training Loss: 8.255570149421692 \n",
      "Epoch: 63   Validation Loss: 8.239183139801025 \n",
      "Epoch: 64   Training Loss: 8.252078485488891 \n",
      "Epoch: 64   Validation Loss: 8.24211540222168 \n",
      "Epoch: 65   Training Loss: 8.246801948547363 \n",
      "Epoch: 65   Validation Loss: 8.244727993011475 \n",
      "Epoch: 66   Training Loss: 8.231576871871948 \n",
      "Epoch: 66   Validation Loss: 8.255409431457519 \n",
      "Epoch: 67   Training Loss: 8.197417974472046 \n",
      "Epoch: 67   Validation Loss: 8.269457244873047 \n",
      "Epoch: 68   Training Loss: 8.148795223236084 \n",
      "Epoch: 68   Validation Loss: 8.291508769989013 \n",
      "Epoch: 69   Training Loss: 8.14779498577118 \n",
      "Epoch: 69   Validation Loss: 8.30288610458374 \n",
      "Epoch: 70   Training Loss: 8.046146655082703 \n",
      "Epoch: 70   Validation Loss: 8.440785598754882 \n",
      "Epoch: 71   Training Loss: 8.012871384620667 \n",
      "Epoch: 71   Validation Loss: 8.37808599472046 \n",
      "Epoch: 72   Training Loss: 7.964110231399536 \n",
      "Epoch: 72   Validation Loss: 8.462828826904296 \n",
      "Epoch: 73   Training Loss: 7.927628993988037 \n",
      "Epoch: 73   Validation Loss: 8.565154075622559 \n",
      "Epoch: 74   Training Loss: 7.9074419975280765 \n",
      "Epoch: 74   Validation Loss: 9.243195533752441 \n",
      "Epoch: 75   Training Loss: 8.618191885948182 \n",
      "Epoch: 75   Validation Loss: 8.184207248687745 \n",
      "Epoch: 76   Training Loss: 8.343628931045533 \n",
      "Epoch: 76   Validation Loss: 8.171741104125976 \n",
      "Epoch: 77   Training Loss: 8.29257550239563 \n",
      "Epoch: 77   Validation Loss: 8.205452632904052 \n",
      "Epoch: 78   Training Loss: 8.285939788818359 \n",
      "Epoch: 78   Validation Loss: 8.21761999130249 \n",
      "Epoch: 79   Training Loss: 8.280396914482116 \n",
      "Epoch: 79   Validation Loss: 8.22000789642334 \n",
      "Epoch: 80   Training Loss: 8.276700568199157 \n",
      "Epoch: 80   Validation Loss: 8.22521162033081 \n",
      "Epoch: 81   Training Loss: 8.274077343940736 \n",
      "Epoch: 81   Validation Loss: 8.226165294647217 \n",
      "Epoch: 82   Training Loss: 8.27171494960785 \n",
      "Epoch: 82   Validation Loss: 8.228614044189452 \n",
      "Epoch: 83   Training Loss: 8.269754409790039 \n",
      "Epoch: 83   Validation Loss: 8.230140590667725 \n",
      "Epoch: 84   Training Loss: 8.268107986450195 \n",
      "Epoch: 84   Validation Loss: 8.230712985992431 \n",
      "Epoch: 85   Training Loss: 8.266528010368347 \n",
      "Epoch: 85   Validation Loss: 8.232077407836915 \n",
      "Epoch: 86   Training Loss: 8.26516330242157 \n",
      "Epoch: 86   Validation Loss: 8.233094501495362 \n",
      "Epoch: 87   Training Loss: 8.264028215408326 \n",
      "Epoch: 87   Validation Loss: 8.234344959259033 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88   Training Loss: 8.262976837158202 \n",
      "Epoch: 88   Validation Loss: 8.234258079528809 \n",
      "Epoch: 89   Training Loss: 8.26187481880188 \n",
      "Epoch: 89   Validation Loss: 8.235258674621582 \n",
      "Epoch: 90   Training Loss: 8.260921311378478 \n",
      "Epoch: 90   Validation Loss: 8.235381984710694 \n",
      "Epoch: 91   Training Loss: 8.260008430480957 \n",
      "Epoch: 91   Validation Loss: 8.236366653442383 \n",
      "Epoch: 92   Training Loss: 8.259218955039978 \n",
      "Epoch: 92   Validation Loss: 8.237056350708007 \n",
      "Epoch: 93   Training Loss: 8.258497333526611 \n",
      "Epoch: 93   Validation Loss: 8.237780284881591 \n",
      "Epoch: 94   Training Loss: 8.257835793495179 \n",
      "Epoch: 94   Validation Loss: 8.23843011856079 \n",
      "Epoch: 95   Training Loss: 8.257223463058471 \n",
      "Epoch: 95   Validation Loss: 8.23863344192505 \n",
      "Epoch: 96   Training Loss: 8.256591057777404 \n",
      "Epoch: 96   Validation Loss: 8.23928918838501 \n",
      "Epoch: 97   Training Loss: 8.256066584587098 \n",
      "Epoch: 97   Validation Loss: 8.239816761016845 \n",
      "Epoch: 98   Training Loss: 8.255568671226502 \n",
      "Epoch: 98   Validation Loss: 8.24021406173706 \n",
      "Epoch: 99   Training Loss: 8.255075383186341 \n",
      "Epoch: 99   Validation Loss: 8.24061622619629 \n",
      "Epoch: 100   Training Loss: 8.2545640707016 \n",
      "Epoch: 100   Validation Loss: 8.241025733947755 \n",
      "Epoch: 101   Training Loss: 8.254106831550597 \n",
      "Epoch: 101   Validation Loss: 8.241479301452637 \n",
      "Epoch: 102   Training Loss: 8.253714561462402 \n",
      "Epoch: 102   Validation Loss: 8.242056369781494 \n",
      "Epoch: 103   Training Loss: 8.253350043296814 \n",
      "Epoch: 103   Validation Loss: 8.242275619506836 \n",
      "Epoch: 104   Training Loss: 8.252962160110474 \n",
      "Epoch: 104   Validation Loss: 8.242727470397949 \n",
      "Epoch: 105   Training Loss: 8.252611017227172 \n",
      "Epoch: 105   Validation Loss: 8.242944622039795 \n",
      "Epoch: 106   Training Loss: 8.252236247062683 \n",
      "Epoch: 106   Validation Loss: 8.243356513977051 \n",
      "Epoch: 107   Training Loss: 8.251904082298278 \n",
      "Epoch: 107   Validation Loss: 8.243509578704835 \n",
      "Epoch: 108   Training Loss: 8.25155749320984 \n",
      "Epoch: 108   Validation Loss: 8.243789577484131 \n",
      "Epoch: 109   Training Loss: 8.251236939430237 \n",
      "Epoch: 109   Validation Loss: 8.243999767303468 \n",
      "Epoch: 110   Training Loss: 8.250933051109314 \n",
      "Epoch: 110   Validation Loss: 8.244421195983886 \n",
      "Epoch: 111   Training Loss: 8.25066819190979 \n",
      "Epoch: 111   Validation Loss: 8.244679069519043 \n",
      "Epoch: 112   Training Loss: 8.250412106513977 \n",
      "Epoch: 112   Validation Loss: 8.244921970367432 \n",
      "Epoch: 113   Training Loss: 8.250145101547242 \n",
      "Epoch: 113   Validation Loss: 8.245037078857422 \n",
      "Epoch: 114   Training Loss: 8.249886059761048 \n",
      "Epoch: 114   Validation Loss: 8.245222663879394 \n",
      "Epoch: 115   Training Loss: 8.249634265899658 \n",
      "Epoch: 115   Validation Loss: 8.245421314239502 \n",
      "Epoch: 116   Training Loss: 8.24939067363739 \n",
      "Epoch: 116   Validation Loss: 8.245447254180908 \n",
      "Epoch: 117   Training Loss: 8.249138927459716 \n",
      "Epoch: 117   Validation Loss: 8.245540332794189 \n",
      "Epoch: 118   Training Loss: 8.248896074295043 \n",
      "Epoch: 118   Validation Loss: 8.245777320861816 \n",
      "Epoch: 119   Training Loss: 8.24869089126587 \n",
      "Epoch: 119   Validation Loss: 8.245998096466064 \n",
      "Epoch: 120   Training Loss: 8.248473501205444 \n",
      "Epoch: 120   Validation Loss: 8.246149444580078 \n",
      "Epoch: 121   Training Loss: 8.24826967716217 \n",
      "Epoch: 121   Validation Loss: 8.246141624450683 \n",
      "Epoch: 122   Training Loss: 8.248071646690368 \n",
      "Epoch: 122   Validation Loss: 8.246402835845947 \n",
      "Epoch: 123   Training Loss: 8.247871994972229 \n",
      "Epoch: 123   Validation Loss: 8.246539306640624 \n",
      "Epoch: 124   Training Loss: 8.247673988342285 \n",
      "Epoch: 124   Validation Loss: 8.246602249145507 \n",
      "Epoch: 125   Training Loss: 8.247479557991028 \n",
      "Epoch: 125   Validation Loss: 8.24662389755249 \n",
      "Epoch: 126   Training Loss: 8.247289347648621 \n",
      "Epoch: 126   Validation Loss: 8.246721935272216 \n",
      "Epoch: 127   Training Loss: 8.24709768295288 \n",
      "Epoch: 127   Validation Loss: 8.246772766113281 \n",
      "Epoch: 128   Training Loss: 8.24690821170807 \n",
      "Epoch: 128   Validation Loss: 8.246893882751465 \n",
      "Epoch: 129   Training Loss: 8.246734762191773 \n",
      "Epoch: 129   Validation Loss: 8.247011470794678 \n",
      "Epoch: 130   Training Loss: 8.246557140350342 \n",
      "Epoch: 130   Validation Loss: 8.246982669830322 \n",
      "Epoch: 131   Training Loss: 8.246378993988037 \n",
      "Epoch: 131   Validation Loss: 8.247135353088378 \n",
      "Epoch: 132   Training Loss: 8.24621603488922 \n",
      "Epoch: 132   Validation Loss: 8.247277164459229 \n",
      "Epoch: 133   Training Loss: 8.24605996608734 \n",
      "Epoch: 133   Validation Loss: 8.247294998168945 \n",
      "Epoch: 134   Training Loss: 8.245894718170167 \n",
      "Epoch: 134   Validation Loss: 8.247373008728028 \n",
      "Epoch: 135   Training Loss: 8.245730304718018 \n",
      "Epoch: 135   Validation Loss: 8.247524642944336 \n",
      "Epoch: 136   Training Loss: 8.24558916091919 \n",
      "Epoch: 136   Validation Loss: 8.247592449188232 \n",
      "Epoch: 137   Training Loss: 8.245441794395447 \n",
      "Epoch: 137   Validation Loss: 8.247686576843261 \n",
      "Epoch: 138   Training Loss: 8.245299816131592 \n",
      "Epoch: 138   Validation Loss: 8.247768211364747 \n",
      "Epoch: 139   Training Loss: 8.24515917301178 \n",
      "Epoch: 139   Validation Loss: 8.247753715515136 \n",
      "Epoch: 140   Training Loss: 8.245011830329895 \n",
      "Epoch: 140   Validation Loss: 8.24784164428711 \n",
      "Epoch: 141   Training Loss: 8.244868612289428 \n",
      "Epoch: 141   Validation Loss: 8.247897243499756 \n",
      "Epoch: 142   Training Loss: 8.244735097885131 \n",
      "Epoch: 142   Validation Loss: 8.247943115234374 \n",
      "Epoch: 143   Training Loss: 8.244596099853515 \n",
      "Epoch: 143   Validation Loss: 8.247993183135986 \n",
      "Epoch: 144   Training Loss: 8.244469141960144 \n",
      "Epoch: 144   Validation Loss: 8.24809398651123 \n",
      "Epoch: 145   Training Loss: 8.24434244632721 \n",
      "Epoch: 145   Validation Loss: 8.248137187957763 \n",
      "Epoch: 146   Training Loss: 8.24422640800476 \n",
      "Epoch: 146   Validation Loss: 8.248144054412842 \n",
      "Epoch: 147   Training Loss: 8.244101858139038 \n",
      "Epoch: 147   Validation Loss: 8.248217105865479 \n",
      "Epoch: 148   Training Loss: 8.243991923332214 \n",
      "Epoch: 148   Validation Loss: 8.248279476165772 \n",
      "Epoch: 149   Training Loss: 8.243865489959717 \n",
      "Epoch: 149   Validation Loss: 8.248328495025635 \n",
      "Epoch: 150   Training Loss: 8.24375228881836 \n",
      "Epoch: 150   Validation Loss: 8.248289966583252 \n",
      "Epoch: 151   Training Loss: 8.243613290786744 \n",
      "Epoch: 151   Validation Loss: 8.248223209381104 \n",
      "Epoch: 152   Training Loss: 8.243488335609436 \n",
      "Epoch: 152   Validation Loss: 8.248362922668457 \n",
      "Epoch: 153   Training Loss: 8.243380284309387 \n",
      "Epoch: 153   Validation Loss: 8.2484055519104 \n",
      "Epoch: 154   Training Loss: 8.243254995346069 \n",
      "Epoch: 154   Validation Loss: 8.248448467254638 \n",
      "Epoch: 155   Training Loss: 8.243133115768433 \n",
      "Epoch: 155   Validation Loss: 8.248513984680176 \n",
      "Epoch: 156   Training Loss: 8.243026399612427 \n",
      "Epoch: 156   Validation Loss: 8.248584175109864 \n",
      "Epoch: 157   Training Loss: 8.242908501625061 \n",
      "Epoch: 157   Validation Loss: 8.248587131500244 \n",
      "Epoch: 158   Training Loss: 8.242794346809386 \n",
      "Epoch: 158   Validation Loss: 8.24853687286377 \n",
      "Epoch: 159   Training Loss: 8.242676615715027 \n",
      "Epoch: 159   Validation Loss: 8.248679161071777 \n",
      "Epoch: 160   Training Loss: 8.24258062839508 \n",
      "Epoch: 160   Validation Loss: 8.248659420013428 \n",
      "Epoch: 161   Training Loss: 8.24247124195099 \n",
      "Epoch: 161   Validation Loss: 8.248671436309815 \n",
      "Epoch: 162   Training Loss: 8.242355179786681 \n",
      "Epoch: 162   Validation Loss: 8.248591232299805 \n",
      "Epoch: 163   Training Loss: 8.24224078655243 \n",
      "Epoch: 163   Validation Loss: 8.248616981506348 \n",
      "Epoch: 164   Training Loss: 8.242124152183532 \n",
      "Epoch: 164   Validation Loss: 8.248644065856933 \n",
      "Epoch: 165   Training Loss: 8.242013597488404 \n",
      "Epoch: 165   Validation Loss: 8.24866304397583 \n",
      "Epoch: 166   Training Loss: 8.241906571388245 \n",
      "Epoch: 166   Validation Loss: 8.248814487457276 \n",
      "Epoch: 167   Training Loss: 8.241820526123046 \n",
      "Epoch: 167   Validation Loss: 8.248897361755372 \n",
      "Epoch: 168   Training Loss: 8.241729593276977 \n",
      "Epoch: 168   Validation Loss: 8.248953723907471 \n",
      "Epoch: 169   Training Loss: 8.241645312309265 \n",
      "Epoch: 169   Validation Loss: 8.248988819122314 \n",
      "Epoch: 170   Training Loss: 8.241559672355653 \n",
      "Epoch: 170   Validation Loss: 8.249022483825684 \n",
      "Epoch: 171   Training Loss: 8.241472482681274 \n",
      "Epoch: 171   Validation Loss: 8.249010276794433 \n",
      "Epoch: 172   Training Loss: 8.241376638412476 \n",
      "Epoch: 172   Validation Loss: 8.249021339416505 \n",
      "Epoch: 173   Training Loss: 8.241278982162475 \n",
      "Epoch: 173   Validation Loss: 8.249056148529053 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 174   Training Loss: 8.24122200012207 \n",
      "Epoch: 174   Validation Loss: 8.249147605895995 \n",
      "Epoch: 175   Training Loss: 8.241117453575134 \n",
      "Epoch: 175   Validation Loss: 8.24915018081665 \n",
      "Epoch: 176   Training Loss: 8.241029214859008 \n",
      "Epoch: 176   Validation Loss: 8.249224662780762 \n",
      "Epoch: 177   Training Loss: 8.240936422348023 \n",
      "Epoch: 177   Validation Loss: 8.249260139465331 \n",
      "Epoch: 178   Training Loss: 8.240850639343261 \n",
      "Epoch: 178   Validation Loss: 8.249312114715575 \n",
      "Epoch: 179   Training Loss: 8.240770077705383 \n",
      "Epoch: 179   Validation Loss: 8.249272060394286 \n",
      "Epoch: 180   Training Loss: 8.240680241584778 \n",
      "Epoch: 180   Validation Loss: 8.249236583709717 \n",
      "Epoch: 181   Training Loss: 8.240581250190735 \n",
      "Epoch: 181   Validation Loss: 8.249327564239502 \n",
      "Epoch: 182   Training Loss: 8.240504598617553 \n",
      "Epoch: 182   Validation Loss: 8.249422073364258 \n",
      "Epoch: 183   Training Loss: 8.240433287620544 \n",
      "Epoch: 183   Validation Loss: 8.249446678161622 \n",
      "Epoch: 184   Training Loss: 8.240350985527039 \n",
      "Epoch: 184   Validation Loss: 8.24948673248291 \n",
      "Epoch: 185   Training Loss: 8.240274596214295 \n",
      "Epoch: 185   Validation Loss: 8.249458599090577 \n",
      "Epoch: 186   Training Loss: 8.240188097953796 \n",
      "Epoch: 186   Validation Loss: 8.249539375305176 \n",
      "Epoch: 187   Training Loss: 8.240117573738098 \n",
      "Epoch: 187   Validation Loss: 8.249575996398926 \n",
      "Epoch: 188   Training Loss: 8.24003837108612 \n",
      "Epoch: 188   Validation Loss: 8.249542617797852 \n",
      "Epoch: 189   Training Loss: 8.239956212043761 \n",
      "Epoch: 189   Validation Loss: 8.249559593200683 \n",
      "Epoch: 190   Training Loss: 8.239879083633422 \n",
      "Epoch: 190   Validation Loss: 8.249548435211182 \n",
      "Epoch: 191   Training Loss: 8.239798712730408 \n",
      "Epoch: 191   Validation Loss: 8.249638175964355 \n",
      "Epoch: 192   Training Loss: 8.239728975296021 \n",
      "Epoch: 192   Validation Loss: 8.249634933471679 \n",
      "Epoch: 193   Training Loss: 8.239707493782044 \n",
      "Epoch: 193   Validation Loss: 8.249717807769775 \n",
      "Epoch: 194   Training Loss: 8.239627122879028 \n",
      "Epoch: 194   Validation Loss: 8.249749660491943 \n",
      "Epoch: 195   Training Loss: 8.239533281326294 \n",
      "Epoch: 195   Validation Loss: 8.249965858459472 \n",
      "Epoch: 196   Training Loss: 8.23993968963623 \n",
      "Epoch: 196   Validation Loss: 8.250040531158447 \n",
      "Epoch: 197   Training Loss: 8.23970456123352 \n",
      "Epoch: 197   Validation Loss: 8.249909687042237 \n",
      "Epoch: 198   Training Loss: 8.239624428749085 \n",
      "Epoch: 198   Validation Loss: 8.249820137023926 \n",
      "Epoch: 199   Training Loss: 8.239494228363037 \n",
      "Epoch: 199   Validation Loss: 8.249835681915282 \n",
      "Epoch: 200   Training Loss: 8.239385414123536 \n",
      "Epoch: 200   Validation Loss: 8.249841117858887 \n",
      "Epoch: 201   Training Loss: 8.23928861618042 \n",
      "Epoch: 201   Validation Loss: 8.249812507629395 \n",
      "Epoch: 202   Training Loss: 8.239198994636535 \n",
      "Epoch: 202   Validation Loss: 8.249909114837646 \n",
      "Epoch: 203   Training Loss: 8.239116740226745 \n",
      "Epoch: 203   Validation Loss: 8.249901008605956 \n",
      "Epoch: 204   Training Loss: 8.239038491249085 \n",
      "Epoch: 204   Validation Loss: 8.249936294555663 \n",
      "Epoch: 205   Training Loss: 8.238964819908142 \n",
      "Epoch: 205   Validation Loss: 8.249944972991944 \n",
      "Epoch: 206   Training Loss: 8.238872814178468 \n",
      "Epoch: 206   Validation Loss: 8.25002613067627 \n",
      "Epoch: 207   Training Loss: 8.238790273666382 \n",
      "Epoch: 207   Validation Loss: 8.2500638961792 \n",
      "Epoch: 208   Training Loss: 8.23878300189972 \n",
      "Epoch: 208   Validation Loss: 8.249981689453126 \n",
      "Epoch: 209   Training Loss: 8.238710689544678 \n",
      "Epoch: 209   Validation Loss: 8.250021362304688 \n",
      "Epoch: 210   Training Loss: 8.238652086257934 \n",
      "Epoch: 210   Validation Loss: 8.249986934661866 \n",
      "Epoch: 211   Training Loss: 8.238597130775451 \n",
      "Epoch: 211   Validation Loss: 8.249937438964844 \n",
      "Epoch: 212   Training Loss: 8.238521575927734 \n",
      "Epoch: 212   Validation Loss: 8.250068855285644 \n",
      "Epoch: 213   Training Loss: 8.238449001312256 \n",
      "Epoch: 213   Validation Loss: 8.250173950195313 \n",
      "Epoch: 214   Training Loss: 8.238392519950867 \n",
      "Epoch: 214   Validation Loss: 8.25016975402832 \n",
      "Epoch: 215   Training Loss: 8.23829789161682 \n",
      "Epoch: 215   Validation Loss: 8.250199222564698 \n",
      "Epoch: 216   Training Loss: 8.238207507133485 \n",
      "Epoch: 216   Validation Loss: 8.250279426574707 \n",
      "Epoch: 217   Training Loss: 8.238127303123473 \n",
      "Epoch: 217   Validation Loss: 8.250280475616455 \n",
      "Epoch: 218   Training Loss: 8.238053369522095 \n",
      "Epoch: 218   Validation Loss: 8.250283145904541 \n",
      "Epoch: 219   Training Loss: 8.238015174865723 \n",
      "Epoch: 219   Validation Loss: 8.250243663787842 \n",
      "Epoch: 220   Training Loss: 8.23793921470642 \n",
      "Epoch: 220   Validation Loss: 8.250274276733398 \n",
      "Epoch: 221   Training Loss: 8.23789849281311 \n",
      "Epoch: 221   Validation Loss: 8.250323390960693 \n",
      "Epoch: 222   Training Loss: 8.237777209281921 \n",
      "Epoch: 222   Validation Loss: 8.250588130950927 \n",
      "Epoch: 223   Training Loss: 8.237706184387207 \n",
      "Epoch: 223   Validation Loss: 8.250455093383788 \n",
      "Epoch: 224   Training Loss: 8.23763816356659 \n",
      "Epoch: 224   Validation Loss: 8.250546550750732 \n",
      "Epoch: 225   Training Loss: 8.23753297328949 \n",
      "Epoch: 225   Validation Loss: 8.250686836242675 \n",
      "Epoch: 226   Training Loss: 8.237526082992554 \n",
      "Epoch: 226   Validation Loss: 8.250502586364746 \n",
      "Epoch: 227   Training Loss: 8.237463760375977 \n",
      "Epoch: 227   Validation Loss: 8.25068712234497 \n",
      "Epoch: 228   Training Loss: 8.237355589866638 \n",
      "Epoch: 228   Validation Loss: 8.250656127929688 \n",
      "Epoch: 229   Training Loss: 8.237288784980773 \n",
      "Epoch: 229   Validation Loss: 8.250460338592529 \n",
      "Epoch: 230   Training Loss: 8.237271690368653 \n",
      "Epoch: 230   Validation Loss: 8.250537204742432 \n",
      "Epoch: 231   Training Loss: 8.237241554260255 \n",
      "Epoch: 231   Validation Loss: 8.2505934715271 \n",
      "Epoch: 232   Training Loss: 8.237108397483826 \n",
      "Epoch: 232   Validation Loss: 8.250791454315186 \n",
      "Epoch: 233   Training Loss: 8.237013959884644 \n",
      "Epoch: 233   Validation Loss: 8.250763988494873 \n",
      "Epoch: 234   Training Loss: 8.237008261680604 \n",
      "Epoch: 234   Validation Loss: 8.250832748413085 \n",
      "Epoch: 235   Training Loss: 8.237268733978272 \n",
      "Epoch: 235   Validation Loss: 8.250296115875244 \n",
      "Epoch: 236   Training Loss: 8.236954379081727 \n",
      "Epoch: 236   Validation Loss: 8.250817966461181 \n",
      "Epoch: 237   Training Loss: 8.236782646179199 \n",
      "Epoch: 237   Validation Loss: 8.250718879699708 \n",
      "Epoch: 238   Training Loss: 8.236704182624816 \n",
      "Epoch: 238   Validation Loss: 8.250878238677979 \n",
      "Epoch: 239   Training Loss: 8.23660717010498 \n",
      "Epoch: 239   Validation Loss: 8.250912761688232 \n",
      "Epoch: 240   Training Loss: 8.236506485939026 \n",
      "Epoch: 240   Validation Loss: 8.250837802886963 \n",
      "Epoch: 241   Training Loss: 8.23655071258545 \n",
      "Epoch: 241   Validation Loss: 8.250638103485107 \n",
      "Epoch: 242   Training Loss: 8.236330938339233 \n",
      "Epoch: 242   Validation Loss: 8.250799942016602 \n",
      "Epoch: 243   Training Loss: 8.236109399795533 \n",
      "Epoch: 243   Validation Loss: 8.251186180114747 \n",
      "Epoch: 244   Training Loss: 8.236045646667481 \n",
      "Epoch: 244   Validation Loss: 8.251011943817138 \n",
      "Epoch: 245   Training Loss: 8.236014270782471 \n",
      "Epoch: 245   Validation Loss: 8.25038251876831 \n",
      "Epoch: 246   Training Loss: 8.235822892189026 \n",
      "Epoch: 246   Validation Loss: 8.25102252960205 \n",
      "Epoch: 247   Training Loss: 8.235578489303588 \n",
      "Epoch: 247   Validation Loss: 8.251080894470215 \n",
      "Epoch: 248   Training Loss: 8.23547441959381 \n",
      "Epoch: 248   Validation Loss: 8.251023387908935 \n",
      "Epoch: 249   Training Loss: 8.235445308685303 \n",
      "Epoch: 249   Validation Loss: 8.250950050354003 \n",
      "Epoch: 250   Training Loss: 8.234960007667542 \n",
      "Epoch: 250   Validation Loss: 8.251719188690185 \n",
      "Epoch: 251   Training Loss: 8.236030459403992 \n",
      "Epoch: 251   Validation Loss: 8.25097713470459 \n",
      "Epoch: 252   Training Loss: 8.235271430015564 \n",
      "Epoch: 252   Validation Loss: 8.250961685180664 \n",
      "Epoch: 253   Training Loss: 8.234765791893006 \n",
      "Epoch: 253   Validation Loss: 8.25118646621704 \n",
      "Epoch: 254   Training Loss: 8.232939338684082 \n",
      "Epoch: 254   Validation Loss: 8.252190113067627 \n",
      "Epoch: 255   Training Loss: 8.23063313961029 \n",
      "Epoch: 255   Validation Loss: 8.253674221038818 \n",
      "Epoch: 256   Training Loss: 8.230454659461975 \n",
      "Epoch: 256   Validation Loss: 8.251105785369873 \n",
      "Epoch: 257   Training Loss: 8.223711228370666 \n",
      "Epoch: 257   Validation Loss: 8.247761154174805 \n",
      "Epoch: 258   Training Loss: 8.208021855354309 \n",
      "Epoch: 258   Validation Loss: 8.252717685699462 \n",
      "Epoch: 259   Training Loss: 8.202849674224854 \n",
      "Epoch: 259   Validation Loss: 8.244390106201172 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 260   Training Loss: 8.172235941886902 \n",
      "Epoch: 260   Validation Loss: 8.268188190460204 \n",
      "Epoch: 261   Training Loss: 8.190795016288757 \n",
      "Epoch: 261   Validation Loss: 8.244248962402343 \n",
      "Epoch: 262   Training Loss: 8.250867915153503 \n",
      "Epoch: 262   Validation Loss: 8.244610023498534 \n",
      "Epoch: 263   Training Loss: 8.24248435497284 \n",
      "Epoch: 263   Validation Loss: 8.246143627166749 \n",
      "Epoch: 264   Training Loss: 8.24123773574829 \n",
      "Epoch: 264   Validation Loss: 8.247049903869629 \n",
      "Epoch: 265   Training Loss: 8.240451407432555 \n",
      "Epoch: 265   Validation Loss: 8.247718620300294 \n",
      "Epoch: 266   Training Loss: 8.239970445632935 \n",
      "Epoch: 266   Validation Loss: 8.248249626159668 \n",
      "Epoch: 267   Training Loss: 8.239641118049622 \n",
      "Epoch: 267   Validation Loss: 8.248834896087647 \n",
      "Epoch: 268   Training Loss: 8.23938274383545 \n",
      "Epoch: 268   Validation Loss: 8.249234008789063 \n",
      "Epoch: 269   Training Loss: 8.239156579971313 \n",
      "Epoch: 269   Validation Loss: 8.249497222900391 \n",
      "Epoch: 270   Training Loss: 8.238975858688354 \n",
      "Epoch: 270   Validation Loss: 8.249646377563476 \n",
      "Epoch: 271   Training Loss: 8.238781571388245 \n",
      "Epoch: 271   Validation Loss: 8.249779605865479 \n",
      "Epoch: 272   Training Loss: 8.238616061210632 \n",
      "Epoch: 272   Validation Loss: 8.250004768371582 \n",
      "Epoch: 273   Training Loss: 8.238470935821534 \n",
      "Epoch: 273   Validation Loss: 8.25013780593872 \n",
      "Epoch: 274   Training Loss: 8.238322281837464 \n",
      "Epoch: 274   Validation Loss: 8.250240516662597 \n",
      "Epoch: 275   Training Loss: 8.238184356689453 \n",
      "Epoch: 275   Validation Loss: 8.25037202835083 \n",
      "Epoch: 276   Training Loss: 8.238050699234009 \n",
      "Epoch: 276   Validation Loss: 8.250454902648926 \n",
      "Epoch: 277   Training Loss: 8.237933421134949 \n",
      "Epoch: 277   Validation Loss: 8.250599575042724 \n",
      "Epoch: 278   Training Loss: 8.237824320793152 \n",
      "Epoch: 278   Validation Loss: 8.250636100769043 \n",
      "Epoch: 279   Training Loss: 8.237722086906434 \n",
      "Epoch: 279   Validation Loss: 8.250650978088379 \n",
      "Epoch: 280   Training Loss: 8.237611341476441 \n",
      "Epoch: 280   Validation Loss: 8.250776481628417 \n",
      "Epoch: 281   Training Loss: 8.237502336502075 \n",
      "Epoch: 281   Validation Loss: 8.250882625579834 \n",
      "Epoch: 282   Training Loss: 8.237409806251526 \n",
      "Epoch: 282   Validation Loss: 8.250894451141358 \n",
      "Epoch: 283   Training Loss: 8.237321853637695 \n",
      "Epoch: 283   Validation Loss: 8.25096321105957 \n",
      "Epoch: 284   Training Loss: 8.237231564521789 \n",
      "Epoch: 284   Validation Loss: 8.250923538208008 \n",
      "Epoch: 285   Training Loss: 8.237136626243592 \n",
      "Epoch: 285   Validation Loss: 8.250940322875977 \n",
      "Epoch: 286   Training Loss: 8.23704481124878 \n",
      "Epoch: 286   Validation Loss: 8.251054668426514 \n",
      "Epoch: 287   Training Loss: 8.236959886550903 \n",
      "Epoch: 287   Validation Loss: 8.250993919372558 \n",
      "Epoch: 288   Training Loss: 8.236865615844726 \n",
      "Epoch: 288   Validation Loss: 8.251053524017333 \n",
      "Epoch: 289   Training Loss: 8.236785101890565 \n",
      "Epoch: 289   Validation Loss: 8.251080513000488 \n",
      "Epoch: 290   Training Loss: 8.236693596839904 \n",
      "Epoch: 290   Validation Loss: 8.251095008850097 \n",
      "Epoch: 291   Training Loss: 8.236607456207276 \n",
      "Epoch: 291   Validation Loss: 8.25114688873291 \n",
      "Epoch: 292   Training Loss: 8.236528086662293 \n",
      "Epoch: 292   Validation Loss: 8.251165008544922 \n",
      "Epoch: 293   Training Loss: 8.236455249786378 \n",
      "Epoch: 293   Validation Loss: 8.251261806488037 \n",
      "Epoch: 294   Training Loss: 8.236384391784668 \n",
      "Epoch: 294   Validation Loss: 8.251258945465088 \n",
      "Epoch: 295   Training Loss: 8.23629915714264 \n",
      "Epoch: 295   Validation Loss: 8.251301574707032 \n",
      "Epoch: 296   Training Loss: 8.236225342750549 \n",
      "Epoch: 296   Validation Loss: 8.251265144348144 \n",
      "Epoch: 297   Training Loss: 8.236136603355408 \n",
      "Epoch: 297   Validation Loss: 8.251323318481445 \n",
      "Epoch: 298   Training Loss: 8.236052513122559 \n",
      "Epoch: 298   Validation Loss: 8.251382064819335 \n",
      "Epoch: 299   Training Loss: 8.235967063903809 \n",
      "Epoch: 299   Validation Loss: 8.251454067230224 \n",
      "Epoch: 300   Training Loss: 8.23587737083435 \n",
      "Epoch: 300   Validation Loss: 8.251613807678222 \n",
      "Epoch: 301   Training Loss: 8.235783815383911 \n",
      "Epoch: 301   Validation Loss: 8.251655006408692 \n",
      "Epoch: 302   Training Loss: 8.235700988769532 \n",
      "Epoch: 302   Validation Loss: 8.251427173614502 \n",
      "Epoch: 303   Training Loss: 8.235572218894958 \n",
      "Epoch: 303   Validation Loss: 8.251607990264892 \n",
      "Epoch: 304   Training Loss: 8.23549382686615 \n",
      "Epoch: 304   Validation Loss: 8.252250480651856 \n",
      "Epoch: 305   Training Loss: 8.235490322113037 \n",
      "Epoch: 305   Validation Loss: 8.251581001281739 \n",
      "Epoch: 306   Training Loss: 8.235270190238953 \n",
      "Epoch: 306   Validation Loss: 8.251715660095215 \n",
      "Epoch: 307   Training Loss: 8.235012865066528 \n",
      "Epoch: 307   Validation Loss: 8.25281867980957 \n",
      "Epoch: 308   Training Loss: 8.234499859809876 \n",
      "Epoch: 308   Validation Loss: 8.252849292755126 \n",
      "Epoch: 309   Training Loss: 8.23412823677063 \n",
      "Epoch: 309   Validation Loss: 8.253289985656739 \n",
      "Epoch: 310   Training Loss: 8.23025839328766 \n",
      "Epoch: 310   Validation Loss: 8.254028701782227 \n",
      "Epoch: 311   Training Loss: 8.194200921058655 \n",
      "Epoch: 311   Validation Loss: 8.249628162384033 \n",
      "Epoch: 312   Training Loss: 8.14540102481842 \n",
      "Epoch: 312   Validation Loss: 8.239295482635498 \n",
      "Epoch: 313   Training Loss: 8.05408980846405 \n",
      "Epoch: 313   Validation Loss: 8.28555736541748 \n",
      "Epoch: 314   Training Loss: 8.095745301246643 \n",
      "Epoch: 314   Validation Loss: 8.417997455596923 \n",
      "Epoch: 315   Training Loss: 8.116481566429139 \n",
      "Epoch: 315   Validation Loss: 8.249642372131348 \n",
      "Epoch: 316   Training Loss: 8.033198261260987 \n",
      "Epoch: 316   Validation Loss: 8.3596697807312 \n",
      "Epoch: 317   Training Loss: 8.030678606033325 \n",
      "Epoch: 317   Validation Loss: 8.274091529846192 \n",
      "Epoch: 318   Training Loss: 7.895802116394043 \n",
      "Epoch: 318   Validation Loss: 8.634436225891113 \n",
      "Epoch: 319   Training Loss: 7.911146235466004 \n",
      "Epoch: 319   Validation Loss: 8.398648452758788 \n",
      "Epoch: 320   Training Loss: 7.935560607910157 \n",
      "Epoch: 320   Validation Loss: 8.477189826965333 \n",
      "Epoch: 321   Training Loss: 8.041194367408753 \n",
      "Epoch: 321   Validation Loss: 8.389892292022704 \n",
      "Epoch: 322   Training Loss: 8.00668556690216 \n",
      "Epoch: 322   Validation Loss: 8.39420919418335 \n",
      "Epoch: 323   Training Loss: 7.890347933769226 \n",
      "Epoch: 323   Validation Loss: 8.432086944580078 \n",
      "Epoch: 324   Training Loss: 7.799348592758179 \n",
      "Epoch: 324   Validation Loss: 8.509422302246094 \n",
      "Epoch: 325   Training Loss: 7.913636636734009 \n",
      "Epoch: 325   Validation Loss: 8.603507995605469 \n",
      "Epoch: 326   Training Loss: 7.893243622779846 \n",
      "Epoch: 326   Validation Loss: 8.410375690460205 \n",
      "Epoch: 327   Training Loss: 7.836035037040711 \n",
      "Epoch: 327   Validation Loss: 8.382213687896728 \n",
      "Epoch: 328   Training Loss: 7.774561882019043 \n",
      "Epoch: 328   Validation Loss: 8.402729511260986 \n",
      "Epoch: 329   Training Loss: 7.76248722076416 \n",
      "Epoch: 329   Validation Loss: 8.352894687652588 \n",
      "Epoch: 330   Training Loss: 7.753455758094788 \n",
      "Epoch: 330   Validation Loss: 8.368269443511963 \n",
      "Epoch: 331   Training Loss: 7.744676613807679 \n",
      "Epoch: 331   Validation Loss: 8.356502056121826 \n",
      "Epoch: 332   Training Loss: 7.74027156829834 \n",
      "Epoch: 332   Validation Loss: 8.409270763397217 \n",
      "Epoch: 333   Training Loss: 7.733187675476074 \n",
      "Epoch: 333   Validation Loss: 8.366674041748047 \n",
      "Epoch: 334   Training Loss: 7.726117205619812 \n",
      "Epoch: 334   Validation Loss: 8.375535202026366 \n",
      "Epoch: 335   Training Loss: 7.7179340600967405 \n",
      "Epoch: 335   Validation Loss: 8.354006671905518 \n",
      "Epoch: 336   Training Loss: 7.697715306282044 \n",
      "Epoch: 336   Validation Loss: 8.333173942565917 \n",
      "Epoch: 337   Training Loss: 7.681230616569519 \n",
      "Epoch: 337   Validation Loss: 8.417136955261231 \n",
      "Epoch: 338   Training Loss: 7.710549449920654 \n",
      "Epoch: 338   Validation Loss: 8.397043704986572 \n",
      "Epoch: 339   Training Loss: 7.711461138725281 \n",
      "Epoch: 339   Validation Loss: 8.407495975494385 \n",
      "Epoch: 340   Training Loss: 7.677913522720337 \n",
      "Epoch: 340   Validation Loss: 8.321925067901612 \n",
      "Epoch: 341   Training Loss: 7.639196038246155 \n",
      "Epoch: 341   Validation Loss: 8.352043151855469 \n",
      "Epoch: 342   Training Loss: 7.613124990463257 \n",
      "Epoch: 342   Validation Loss: 8.492494487762452 \n",
      "Epoch: 343   Training Loss: 7.677736067771912 \n",
      "Epoch: 343   Validation Loss: 8.389768123626709 \n",
      "Epoch: 344   Training Loss: 7.586529493331909 \n",
      "Epoch: 344   Validation Loss: 8.340742874145509 \n",
      "Epoch: 345   Training Loss: 7.516404986381531 \n",
      "Epoch: 345   Validation Loss: 8.360781860351562 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 346   Training Loss: 7.522854995727539 \n",
      "Epoch: 346   Validation Loss: 8.386234188079834 \n",
      "Epoch: 347   Training Loss: 7.492452907562256 \n",
      "Epoch: 347   Validation Loss: 8.4160888671875 \n",
      "Epoch: 348   Training Loss: 7.4549973726272585 \n",
      "Epoch: 348   Validation Loss: 8.453057861328125 \n",
      "Epoch: 349   Training Loss: 7.45195574760437 \n",
      "Epoch: 349   Validation Loss: 8.477999591827393 \n",
      "Epoch: 350   Training Loss: 7.397576546669006 \n",
      "Epoch: 350   Validation Loss: 8.642270278930663 \n",
      "Epoch: 351   Training Loss: 7.537117981910706 \n",
      "Epoch: 351   Validation Loss: 8.457696628570556 \n",
      "Epoch: 352   Training Loss: 7.88538556098938 \n",
      "Epoch: 352   Validation Loss: 8.347609615325927 \n",
      "Epoch: 353   Training Loss: 7.687903523445129 \n",
      "Epoch: 353   Validation Loss: 8.59263505935669 \n",
      "Epoch: 354   Training Loss: 7.82047164440155 \n",
      "Epoch: 354   Validation Loss: 8.385050678253174 \n",
      "Epoch: 355   Training Loss: 7.540089440345764 \n",
      "Epoch: 355   Validation Loss: 8.570156478881836 \n",
      "Epoch: 356   Training Loss: 7.4302351713180546 \n",
      "Epoch: 356   Validation Loss: 8.442010116577148 \n",
      "Epoch: 357   Training Loss: 7.362640023231506 \n",
      "Epoch: 357   Validation Loss: 8.698660182952882 \n",
      "Epoch: 358   Training Loss: 7.459052920341492 \n",
      "Epoch: 358   Validation Loss: 8.347852230072021 \n",
      "Epoch: 359   Training Loss: 7.706854343414307 \n",
      "Epoch: 359   Validation Loss: 8.658108520507813 \n",
      "Epoch: 360   Training Loss: 7.74043881893158 \n",
      "Epoch: 360   Validation Loss: 8.54360933303833 \n",
      "Epoch: 361   Training Loss: 7.506836485862732 \n",
      "Epoch: 361   Validation Loss: 8.535765743255615 \n",
      "Epoch: 362   Training Loss: 7.466583204269409 \n",
      "Epoch: 362   Validation Loss: 8.859587001800538 \n",
      "Epoch: 363   Training Loss: 7.454214692115784 \n",
      "Epoch: 363   Validation Loss: 8.537092399597167 \n",
      "Epoch: 364   Training Loss: 7.34192841053009 \n",
      "Epoch: 364   Validation Loss: 8.796806621551514 \n",
      "Epoch: 365   Training Loss: 7.270238375663757 \n",
      "Epoch: 365   Validation Loss: 8.470861530303955 \n",
      "Epoch: 366   Training Loss: 7.313028454780579 \n",
      "Epoch: 366   Validation Loss: 8.614354991912842 \n",
      "Epoch: 367   Training Loss: 7.100616240501404 \n",
      "Epoch: 367   Validation Loss: 8.635909938812256 \n",
      "Epoch: 368   Training Loss: 7.07342369556427 \n",
      "Epoch: 368   Validation Loss: 8.572671508789062 \n",
      "Epoch: 369   Training Loss: 6.993089365959167 \n",
      "Epoch: 369   Validation Loss: 8.960909748077393 \n",
      "Epoch: 370   Training Loss: 7.111630940437317 \n",
      "Epoch: 370   Validation Loss: 8.531932735443116 \n",
      "Epoch: 371   Training Loss: 6.999439525604248 \n",
      "Epoch: 371   Validation Loss: 8.74498176574707 \n",
      "Epoch: 372   Training Loss: 6.992296528816223 \n",
      "Epoch: 372   Validation Loss: 8.531001949310303 \n",
      "Epoch: 373   Training Loss: 6.889491939544678 \n",
      "Epoch: 373   Validation Loss: 8.84260368347168 \n",
      "Epoch: 374   Training Loss: 6.9789509057998655 \n",
      "Epoch: 374   Validation Loss: 8.941610145568848 \n",
      "Epoch: 375   Training Loss: 7.569448256492615 \n",
      "Epoch: 375   Validation Loss: 8.439052963256836 \n",
      "Epoch: 376   Training Loss: 7.29298050403595 \n",
      "Epoch: 376   Validation Loss: 8.547834300994873 \n",
      "Epoch: 377   Training Loss: 7.021979808807373 \n",
      "Epoch: 377   Validation Loss: 8.451659774780273 \n",
      "Epoch: 378   Training Loss: 6.865644240379334 \n",
      "Epoch: 378   Validation Loss: 8.820907878875733 \n",
      "Epoch: 379   Training Loss: 6.87498562335968 \n",
      "Epoch: 379   Validation Loss: 8.773958587646485 \n",
      "Epoch: 380   Training Loss: 6.752890276908874 \n",
      "Epoch: 380   Validation Loss: 8.604429721832275 \n",
      "Epoch: 381   Training Loss: 7.202123856544494 \n",
      "Epoch: 381   Validation Loss: 8.489591979980469 \n",
      "Epoch: 382   Training Loss: 7.378836560249328 \n",
      "Epoch: 382   Validation Loss: 8.483606052398681 \n",
      "Epoch: 383   Training Loss: 7.218751263618469 \n",
      "Epoch: 383   Validation Loss: 8.588689517974853 \n",
      "Epoch: 384   Training Loss: 7.377587080001831 \n",
      "Epoch: 384   Validation Loss: 8.696685218811036 \n",
      "Epoch: 385   Training Loss: 6.982647395133972 \n",
      "Epoch: 385   Validation Loss: 8.565645980834962 \n",
      "Epoch: 386   Training Loss: 6.770531713962555 \n",
      "Epoch: 386   Validation Loss: 8.715692806243897 \n",
      "Epoch: 387   Training Loss: 6.7741428971290585 \n",
      "Epoch: 387   Validation Loss: 8.87169589996338 \n",
      "Epoch: 388   Training Loss: 6.535826969146728 \n",
      "Epoch: 388   Validation Loss: 8.821207237243652 \n",
      "Epoch: 389   Training Loss: 6.6006028890609745 \n",
      "Epoch: 389   Validation Loss: 8.523418521881103 \n",
      "Epoch: 390   Training Loss: 6.442629545927048 \n",
      "Epoch: 390   Validation Loss: 8.897735691070556 \n",
      "Epoch: 391   Training Loss: 6.452032673358917 \n",
      "Epoch: 391   Validation Loss: 8.786815643310547 \n",
      "Epoch: 392   Training Loss: 6.4392858147621155 \n",
      "Epoch: 392   Validation Loss: 9.03703670501709 \n",
      "Epoch: 393   Training Loss: 6.409264010190964 \n",
      "Epoch: 393   Validation Loss: 8.748741340637206 \n",
      "Epoch: 394   Training Loss: 6.228782826662064 \n",
      "Epoch: 394   Validation Loss: 8.80762758255005 \n",
      "Epoch: 395   Training Loss: 6.28230789899826 \n",
      "Epoch: 395   Validation Loss: 9.045098876953125 \n",
      "Epoch: 396   Training Loss: 6.249738454818726 \n",
      "Epoch: 396   Validation Loss: 8.876813983917236 \n",
      "Epoch: 397   Training Loss: 6.09483442902565 \n",
      "Epoch: 397   Validation Loss: 8.954648113250732 \n",
      "Epoch: 398   Training Loss: 6.139055681228638 \n",
      "Epoch: 398   Validation Loss: 8.868599224090577 \n",
      "Epoch: 399   Training Loss: 5.9205443322658535 \n",
      "Epoch: 399   Validation Loss: 8.826911640167236 \n",
      "Epoch: 400   Training Loss: 5.881352740526199 \n",
      "Epoch: 400   Validation Loss: 8.760520648956298 \n",
      "Epoch: 401   Training Loss: 5.814363932609558 \n",
      "Epoch: 401   Validation Loss: 9.12134828567505 \n",
      "Epoch: 402   Training Loss: 5.791938376426697 \n",
      "Epoch: 402   Validation Loss: 8.768943881988525 \n",
      "Epoch: 403   Training Loss: 5.855011087656021 \n",
      "Epoch: 403   Validation Loss: 8.95692253112793 \n",
      "Epoch: 404   Training Loss: 6.693652629852295 \n",
      "Epoch: 404   Validation Loss: 9.185741901397705 \n",
      "Epoch: 405   Training Loss: 8.28748095035553 \n",
      "Epoch: 405   Validation Loss: 8.83258934020996 \n",
      "Epoch: 406   Training Loss: 7.441301465034485 \n",
      "Epoch: 406   Validation Loss: 8.433477210998536 \n",
      "Epoch: 407   Training Loss: 6.983292043209076 \n",
      "Epoch: 407   Validation Loss: 8.513232326507568 \n",
      "Epoch: 408   Training Loss: 7.471896529197693 \n",
      "Epoch: 408   Validation Loss: 8.523207569122315 \n",
      "Epoch: 409   Training Loss: 6.787983793020248 \n",
      "Epoch: 409   Validation Loss: 8.471594715118409 \n",
      "Epoch: 410   Training Loss: 6.516604793071747 \n",
      "Epoch: 410   Validation Loss: 8.81313247680664 \n",
      "Epoch: 411   Training Loss: 6.409554123878479 \n",
      "Epoch: 411   Validation Loss: 8.851826667785645 \n",
      "Epoch: 412   Training Loss: 6.017367625236512 \n",
      "Epoch: 412   Validation Loss: 8.95815019607544 \n",
      "Epoch: 413   Training Loss: 5.804965904355049 \n",
      "Epoch: 413   Validation Loss: 9.307529067993164 \n",
      "Epoch: 414   Training Loss: 5.765342503786087 \n",
      "Epoch: 414   Validation Loss: 9.017684078216552 \n",
      "Epoch: 415   Training Loss: 5.985556408762932 \n",
      "Epoch: 415   Validation Loss: 9.212211990356446 \n",
      "Epoch: 416   Training Loss: 5.915378081798553 \n",
      "Epoch: 416   Validation Loss: 9.638126373291016 \n",
      "Epoch: 417   Training Loss: 5.8126623898744585 \n",
      "Epoch: 417   Validation Loss: 9.417651462554932 \n",
      "Epoch: 418   Training Loss: 5.709952199459076 \n",
      "Epoch: 418   Validation Loss: 9.44435510635376 \n",
      "Epoch: 419   Training Loss: 6.008818912506103 \n",
      "Epoch: 419   Validation Loss: 9.026925563812256 \n",
      "Epoch: 420   Training Loss: 7.109434628486634 \n",
      "Epoch: 420   Validation Loss: 9.376594734191894 \n",
      "Epoch: 421   Training Loss: 6.775024175643921 \n",
      "Epoch: 421   Validation Loss: 8.89973726272583 \n",
      "Epoch: 422   Training Loss: 6.119687497615814 \n",
      "Epoch: 422   Validation Loss: 9.373146629333496 \n",
      "Epoch: 423   Training Loss: 5.753573071956635 \n",
      "Epoch: 423   Validation Loss: 9.484217071533203 \n",
      "Epoch: 424   Training Loss: 5.559574770927429 \n",
      "Epoch: 424   Validation Loss: 9.278435134887696 \n",
      "Epoch: 425   Training Loss: 5.345161962509155 \n",
      "Epoch: 425   Validation Loss: 9.456378078460693 \n",
      "Epoch: 426   Training Loss: 5.288966715335846 \n",
      "Epoch: 426   Validation Loss: 9.475262260437011 \n",
      "Epoch: 427   Training Loss: 5.108295956254006 \n",
      "Epoch: 427   Validation Loss: 9.583313083648681 \n",
      "Epoch: 428   Training Loss: 5.269105790555477 \n",
      "Epoch: 428   Validation Loss: 9.264036750793457 \n",
      "Epoch: 429   Training Loss: 5.29250890314579 \n",
      "Epoch: 429   Validation Loss: 9.539182949066163 \n",
      "Epoch: 430   Training Loss: 5.133243232965469 \n",
      "Epoch: 430   Validation Loss: 9.779071140289307 \n",
      "Epoch: 431   Training Loss: 5.0965441226959225 \n",
      "Epoch: 431   Validation Loss: 9.510396003723145 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 432   Training Loss: 5.1696601316332815 \n",
      "Epoch: 432   Validation Loss: 9.4443528175354 \n",
      "Epoch: 433   Training Loss: 5.047721242904663 \n",
      "Epoch: 433   Validation Loss: 9.414839744567871 \n",
      "Epoch: 434   Training Loss: 4.916485255956649 \n",
      "Epoch: 434   Validation Loss: 9.726765632629395 \n",
      "Epoch: 435   Training Loss: 4.719050472974777 \n",
      "Epoch: 435   Validation Loss: 9.691676425933839 \n",
      "Epoch: 436   Training Loss: 4.582377475500107 \n",
      "Epoch: 436   Validation Loss: 9.58796796798706 \n",
      "Epoch: 437   Training Loss: 4.3469049200415615 \n",
      "Epoch: 437   Validation Loss: 9.624526023864746 \n",
      "Epoch: 438   Training Loss: 4.1928060978651045 \n",
      "Epoch: 438   Validation Loss: 9.66693344116211 \n",
      "Epoch: 439   Training Loss: 4.086426547169685 \n",
      "Epoch: 439   Validation Loss: 9.7801983833313 \n",
      "Epoch: 440   Training Loss: 4.018745368719101 \n",
      "Epoch: 440   Validation Loss: 9.994874572753906 \n",
      "Epoch: 441   Training Loss: 4.112797950208187 \n",
      "Epoch: 441   Validation Loss: 9.592630386352539 \n",
      "Epoch: 442   Training Loss: 4.168190905451775 \n",
      "Epoch: 442   Validation Loss: 9.748792171478271 \n",
      "Epoch: 443   Training Loss: 4.182975059747696 \n",
      "Epoch: 443   Validation Loss: 9.919865131378174 \n",
      "Epoch: 444   Training Loss: 4.012995982170105 \n",
      "Epoch: 444   Validation Loss: 10.03146333694458 \n",
      "Epoch: 445   Training Loss: 3.9434518456459045 \n",
      "Epoch: 445   Validation Loss: 9.647977733612061 \n",
      "Epoch: 446   Training Loss: 3.965266191959381 \n",
      "Epoch: 446   Validation Loss: 9.722343921661377 \n",
      "Epoch: 447   Training Loss: 3.612741769850254 \n",
      "Epoch: 447   Validation Loss: 9.892031860351562 \n",
      "Epoch: 448   Training Loss: 3.4996666312217712 \n",
      "Epoch: 448   Validation Loss: 9.673189163208008 \n",
      "Epoch: 449   Training Loss: 3.467730167508125 \n",
      "Epoch: 449   Validation Loss: 10.194916725158691 \n",
      "Epoch: 450   Training Loss: 3.739628252387047 \n",
      "Epoch: 450   Validation Loss: 10.129705238342286 \n",
      "Epoch: 451   Training Loss: 4.132382392883301 \n",
      "Epoch: 451   Validation Loss: 9.929920291900634 \n",
      "Epoch: 452   Training Loss: 4.274518072605133 \n",
      "Epoch: 452   Validation Loss: 10.02285270690918 \n",
      "Epoch: 453   Training Loss: 4.048422807455063 \n",
      "Epoch: 453   Validation Loss: 9.756140613555909 \n",
      "Epoch: 454   Training Loss: 3.8746710240840914 \n",
      "Epoch: 454   Validation Loss: 9.638769435882569 \n",
      "Epoch: 455   Training Loss: 3.479566776752472 \n",
      "Epoch: 455   Validation Loss: 10.049889183044433 \n",
      "Epoch: 456   Training Loss: 3.0559134662151335 \n",
      "Epoch: 456   Validation Loss: 9.853580665588378 \n",
      "Epoch: 457   Training Loss: 2.791625739634037 \n",
      "Epoch: 457   Validation Loss: 9.856692028045654 \n",
      "Epoch: 458   Training Loss: 2.5108977153897287 \n",
      "Epoch: 458   Validation Loss: 10.03242826461792 \n",
      "Epoch: 459   Training Loss: 2.334492306411266 \n",
      "Epoch: 459   Validation Loss: 10.333879852294922 \n",
      "Epoch: 460   Training Loss: 2.2581973344087602 \n",
      "Epoch: 460   Validation Loss: 10.144376277923584 \n",
      "Epoch: 461   Training Loss: 2.1419979229569437 \n",
      "Epoch: 461   Validation Loss: 10.006222343444824 \n",
      "Epoch: 462   Training Loss: 2.0928235232830046 \n",
      "Epoch: 462   Validation Loss: 9.95728645324707 \n",
      "Epoch: 463   Training Loss: 1.99042799025774 \n",
      "Epoch: 463   Validation Loss: 10.36842222213745 \n",
      "Epoch: 464   Training Loss: 2.0265536651015283 \n",
      "Epoch: 464   Validation Loss: 10.668013954162598 \n",
      "Epoch: 465   Training Loss: 1.886856283247471 \n",
      "Epoch: 465   Validation Loss: 10.509910392761231 \n",
      "Epoch: 466   Training Loss: 1.8650031000375749 \n",
      "Epoch: 466   Validation Loss: 10.414361000061035 \n",
      "Epoch: 467   Training Loss: 1.9114924624562264 \n",
      "Epoch: 467   Validation Loss: 10.043083381652831 \n",
      "Epoch: 468   Training Loss: 1.7526646077632904 \n",
      "Epoch: 468   Validation Loss: 10.425781631469727 \n",
      "Epoch: 469   Training Loss: 1.670730111002922 \n",
      "Epoch: 469   Validation Loss: 10.276669979095459 \n",
      "Epoch: 470   Training Loss: 1.4906818270683289 \n",
      "Epoch: 470   Validation Loss: 10.33303518295288 \n",
      "Epoch: 471   Training Loss: 1.4078910067677497 \n",
      "Epoch: 471   Validation Loss: 10.654623985290527 \n",
      "Epoch: 472   Training Loss: 1.3155209213495254 \n",
      "Epoch: 472   Validation Loss: 10.791650390625 \n",
      "Epoch: 473   Training Loss: 1.2650128602981567 \n",
      "Epoch: 473   Validation Loss: 10.195767021179199 \n",
      "Epoch: 474   Training Loss: 1.1227037586271762 \n",
      "Epoch: 474   Validation Loss: 10.71466703414917 \n",
      "Epoch: 475   Training Loss: 1.0354651495814324 \n",
      "Epoch: 475   Validation Loss: 10.52638874053955 \n",
      "Epoch: 476   Training Loss: 1.0693826079368591 \n",
      "Epoch: 476   Validation Loss: 10.549609756469726 \n",
      "Epoch: 477   Training Loss: 0.9268328435719013 \n",
      "Epoch: 477   Validation Loss: 10.5491229057312 \n",
      "Epoch: 478   Training Loss: 0.7748678382486105 \n",
      "Epoch: 478   Validation Loss: 10.550515079498291 \n",
      "Epoch: 479   Training Loss: 0.6847125455737114 \n",
      "Epoch: 479   Validation Loss: 10.56740665435791 \n",
      "Epoch: 480   Training Loss: 0.5869276635348797 \n",
      "Epoch: 480   Validation Loss: 10.821599388122559 \n",
      "Epoch: 481   Training Loss: 0.562780424579978 \n",
      "Epoch: 481   Validation Loss: 11.173319625854493 \n",
      "Epoch: 482   Training Loss: 0.6035874538123608 \n",
      "Epoch: 482   Validation Loss: 11.124450874328613 \n",
      "Epoch: 483   Training Loss: 0.5801663119345903 \n",
      "Epoch: 483   Validation Loss: 10.775861358642578 \n",
      "Epoch: 484   Training Loss: 0.5131873741745949 \n",
      "Epoch: 484   Validation Loss: 10.826604461669922 \n",
      "Epoch: 485   Training Loss: 0.4590178780257702 \n",
      "Epoch: 485   Validation Loss: 11.080891227722168 \n",
      "Epoch: 486   Training Loss: 0.4273431092500687 \n",
      "Epoch: 486   Validation Loss: 11.600283432006837 \n",
      "Epoch: 487   Training Loss: 0.4539856791496277 \n",
      "Epoch: 487   Validation Loss: 10.422486495971679 \n",
      "Epoch: 488   Training Loss: 0.3755983538925648 \n",
      "Epoch: 488   Validation Loss: 10.9705020904541 \n",
      "Epoch: 489   Training Loss: 0.39478215724229815 \n",
      "Epoch: 489   Validation Loss: 10.926790618896485 \n",
      "Epoch: 490   Training Loss: 0.3734783438965678 \n",
      "Epoch: 490   Validation Loss: 11.216358757019043 \n",
      "Epoch: 491   Training Loss: 0.3448662903159857 \n",
      "Epoch: 491   Validation Loss: 10.61502513885498 \n",
      "Epoch: 492   Training Loss: 0.22241089977324008 \n",
      "Epoch: 492   Validation Loss: 10.951133823394775 \n",
      "Epoch: 493   Training Loss: 0.18111299239099027 \n",
      "Epoch: 493   Validation Loss: 10.864044761657714 \n",
      "Epoch: 494   Training Loss: 0.13250133562833072 \n",
      "Epoch: 494   Validation Loss: 10.95267391204834 \n",
      "Epoch: 495   Training Loss: 0.12070336192846298 \n",
      "Epoch: 495   Validation Loss: 10.888012886047363 \n",
      "Epoch: 496   Training Loss: 0.09655723851174117 \n",
      "Epoch: 496   Validation Loss: 10.94243564605713 \n",
      "Epoch: 497   Training Loss: 0.11634461209177971 \n",
      "Epoch: 497   Validation Loss: 10.872670936584473 \n",
      "Epoch: 498   Training Loss: 0.0986400630325079 \n",
      "Epoch: 498   Validation Loss: 10.858714485168457 \n",
      "Epoch: 499   Training Loss: 0.10748959891498089 \n",
      "Epoch: 499   Validation Loss: 10.926411437988282 \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Create writer and profiler to analyze loss over each epoch\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Set device to CUDA if available, initialize model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "net = generate_model(50)\n",
    "net.to(device)\n",
    "\n",
    "# Set up optimizer and loss function, set number of epochs\n",
    "optimizer = optim.SGD(net.parameters(), lr = 5e-2, momentum=0.9, weight_decay = 0)\n",
    "criterion = nn.MSELoss(reduction = 'mean')\n",
    "criterion.to(device)\n",
    "num_epochs = 500\n",
    "\n",
    "# Iniitializing variables to show statistics\n",
    "iteration = 0\n",
    "test_iteration = 0\n",
    "loss_list = []\n",
    "test_loss_list = []\n",
    "epoch_loss_averages = []\n",
    "test_epoch_loss_averages = []\n",
    "\n",
    "# Iterates over dataset multiple times\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    test_epoch_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(trainset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(data[0]).to(device).float()), torch.from_numpy(data[1]).to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs).to(device)\n",
    "        loss = criterion(outputs, truths)\n",
    "        writer.add_scalar(\"Loss / Train\", loss, epoch) # adds training loss scalar\n",
    "        loss_list.append(loss.cpu().detach().numpy())\n",
    "        epoch_loss += loss.cpu().detach().numpy()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration += 1\n",
    "        if iteration % trainset.shape[0] == 0:\n",
    "            epoch_loss_averages.append(epoch_loss / trainset.shape[0])\n",
    "            print('Epoch: {}   Training Loss: {} '.format(epoch, epoch_loss / trainset.shape[0]))\n",
    "            \n",
    "    for i, test_data in enumerate(testset, 0):\n",
    "        \n",
    "        inputs, truths = norm(torch.from_numpy(test_data[0]).to(device).float()), torch.from_numpy(test_data[1]).to(device).float()\n",
    "        outputs = net(inputs).to(device)\n",
    "        test_loss = criterion(outputs, truths)\n",
    "        \n",
    "        writer.add_scalar(\"Loss / Test\", test_loss, epoch) # adds testing loss scalar\n",
    "        test_loss_list.append(test_loss.cpu().detach().numpy())\n",
    "        test_epoch_loss += test_loss.cpu().detach().numpy()\n",
    "        \n",
    "        test_iteration +=1\n",
    "        if test_iteration % testset.shape[0] == 0:\n",
    "            test_epoch_loss_averages.append(test_epoch_loss / testset.shape[0])\n",
    "            print('Epoch: {}   Validation Loss: {} '.format(epoch, test_epoch_loss / testset.shape[0]))\n",
    "            \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "541b594f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot epoch loss to test for convergence\n",
    "plt.plot(epoch_loss_averages)\n",
    "plt.plot(test_epoch_loss_averages)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716136ea",
   "metadata": {},
   "source": [
    "After inputting the stack of projection differences into the neural network it still shows convergence with a small number of samples, now to test for a larger dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
